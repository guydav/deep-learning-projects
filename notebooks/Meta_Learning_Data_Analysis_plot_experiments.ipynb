{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending key for api.wandb.ai to your netrc file: /Users/guydavidson/.netrc\n",
      "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!wandb login 9676e3cc95066e4865586082971f2653245f09b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.special import factorial\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from matplotlib import path as mpath\n",
    "\n",
    "import pickle\n",
    "import tabulate\n",
    "import wandb\n",
    "from collections import namedtuple\n",
    "\n",
    "import meta_learning_data_analysis as analysis\n",
    "import meta_learning_analysis_plots as plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['six_replications_analyses', 'control_analyses', 'query_mod_replications', 'six_replications_updated_analyses', 'updated_control_analyses', 'query_mod_updated_analyses', 'forgetting_curves_raw_data', 'preliminary_maml_analyses', 'baseline_maml_comparison_analyses', 'maml_analyses', 'maml_alpha_0_analyses', 'maml_meta_test_analyses', 'balanced_batches_analyses', 'baseline_total_curve_analyses', 'control_total_curve_analyses', 'query_mod_total_curve_analyses', 'simultaneous_training_analyses', 'per_task_simultaneous_training_analyses'])\n"
     ]
    }
   ],
   "source": [
    "cache = analysis.refresh_cache()\n",
    "print(cache.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# Baseline analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'six_replications_analyses' in cache:\n",
    "    six_replications_analyses = cache['six_replications_analyses']\n",
    "\n",
    "else:\n",
    "    six_replications_by_dimension_runs = analysis.load_runs(60)\n",
    "    print('Loaded runs')\n",
    "\n",
    "    six_reps_dict = {dimension_name:analysis.process_multiple_runs(run_set) \n",
    "                     for run_set, dimension_name \n",
    "                     in zip(six_replications_by_dimension_runs, analysis.CONDITION_ANALYSES_FIELDS)}\n",
    "    six_replications_analyses = analysis.ConditionAnalysesSet(**six_reps_dict)\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(six_replications_analyses=six_replications_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'six_replications_updated_analyses' in cache:\n",
    "#     six_replications_updated_analyses = cache['six_replications_updated_analyses']\n",
    "\n",
    "# else:\n",
    "six_replications_by_dimension_runs = analysis.load_runs(60)\n",
    "print('Loaded runs')\n",
    "\n",
    "# note: the equal accuracy field will come in as accuracy_drops\n",
    "updated_six_reps_dict = {}\n",
    "start_index = 0\n",
    "for run_set, dimension_name in zip(six_replications_by_dimension_runs[start_index:], \n",
    "                                   analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "    updated_six_reps_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "        run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size) \n",
    "\n",
    "# combined_analysis = analysis.process_multiple_runs(\n",
    "#     six_replications_by_dimension_runs[3], \n",
    "#     parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size)\n",
    "\n",
    "six_replications_updated_analyses = analysis.ConditionAnalysesSet(**updated_six_reps_dict)\n",
    "\n",
    "cache = analysis.refresh_cache(dict(six_replications_updated_analyses=six_replications_updated_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'baseline_total_curve_analyses' in cache:\n",
    "#     baseline_total_curve_analyses = cache['baseline_total_curve_analyses']\n",
    "\n",
    "# else:\n",
    "six_replications_by_dimension_runs = analysis.load_runs(60)\n",
    "print('Loaded runs')\n",
    "\n",
    "analyses_per_dimension = {}\n",
    "\n",
    "for run_set, dimension_name in zip(six_replications_by_dimension_runs, analysis.CONDITION_ANALYSES_FIELDS):\n",
    "    print(f'Starting {dimension_name}')\n",
    "    total_curve_raw, total_curve_mean, total_curve_std, total_curve_sem = \\\n",
    "        analysis.process_multiple_runs_total_task_training_curves(run_set)\n",
    "\n",
    "    analyses_per_dimension[dimension_name] = analysis.TotalCurveResults(raw=total_curve_raw,\n",
    "                                                                        mean=total_curve_mean, \n",
    "                                                                        std=total_curve_std, \n",
    "                                                                        sem=total_curve_sem)\n",
    "\n",
    "total_curve_analyses = analysis.ConditionAnalysesSet(**analyses_per_dimension)\n",
    "cache = analysis.refresh_cache(dict(baseline_total_curve_analyses=total_curve_analyses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# Control analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'control_analyses' in cache:\n",
    "    control_analyses = cache['control_analyses']\n",
    "\n",
    "else:\n",
    "    control_runs = analysis.load_runs(150, 'meta-learning-scaling/sequential-benchmark-control', False)\n",
    "    print(f'Loaded runs')\n",
    "    control_analyses = analysis.ConditionAnalysesSet(combined=analysis.process_multiple_runs(control_runs.combined))\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(control_analyses=control_analyses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'six_replications_updated_analyses' in cache:\n",
    "#     six_replications_updated_analyses = cache['six_replications_updated_analyses']\n",
    "\n",
    "# else:\n",
    "control_runs = analysis.load_runs(150, 'meta-learning-scaling/sequential-benchmark-control', False)\n",
    "print('Loaded runs')\n",
    "\n",
    "updated_control_analyses = analysis.ConditionAnalysesSet(\n",
    "    combined=analysis.process_multiple_runs(control_runs.combined, \n",
    "                                            parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size))\n",
    "\n",
    "\n",
    "cache = analysis.refresh_cache(dict(updated_control_analyses=updated_control_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'control_total_curve_analyses' in cache:\n",
    "    control_total_curve_analyses = cache['control_total_curve_analyses']\n",
    "\n",
    "else:\n",
    "    control_runs = analysis.load_runs(150, 'meta-learning-scaling/sequential-benchmark-control', False)\n",
    "    print('Loaded runs')\n",
    "    \n",
    "    total_curve_raw, total_curve_mean, total_curve_std, total_curve_sem = \\\n",
    "            analysis.process_multiple_runs_total_task_training_curves(control_runs.combined)\n",
    "    \n",
    "    control_total_curve_analyses = analysis.ConditionAnalysesSet(\n",
    "        combined=analysis.TotalCurveResults(raw=total_curve_raw,\n",
    "                                            mean=total_curve_mean,\n",
    "                                            std=total_curve_std,\n",
    "                                            sem=total_curve_sem))\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(control_total_curve_analyses=control_total_curve_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = analysis.refresh_cache(dict(control_analyses=control_analyses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the number of examples by dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = (1000, 520000)\n",
    "\n",
    "plots.plot_processed_results(first_replication_analyses.color.examples, 'Color 10-run average', ylim)\n",
    "plots.plot_processed_results(first_replication_analyses.shape.examples, 'Shape 10-run average', ylim)\n",
    "plots.plot_processed_results(first_replication_analyses.texture.examples, 'Material 10-run average', ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = (1000, 700000)\n",
    "\n",
    "plots.plot_processed_results(six_replications_analyses.color.examples, 'Color 60-run average', ylim)\n",
    "plots.plot_processed_results(six_replications_analyses.shape.examples, 'Shape 60-run average', ylim)\n",
    "plots.plot_processed_results(six_replications_analyses.texture.examples, 'Material 60-run average', ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the log of the number of examples to criterion, in each dimension, with error bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the combined results over all 180 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = (7.75, 13.25)\n",
    "\n",
    "plots.plot_processed_results(six_replications_analyses.combined.log_examples, 'Combined 180-run average', \n",
    "                       ylim, log_x=(True, True), log_y=True, sem_n=180, shade_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the absolute accuracy after introducing a new task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = None\n",
    "\n",
    "plots.plot_processed_results(six_replications_analyses.color.accuracies, 'Color 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.shape.accuracies, 'Shape 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.texture.accuracies, 'Material 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.combined.accuracies, 'Combined 180-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=180, shade_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the accuracy drop after introducing a new task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = None\n",
    "\n",
    "plots.plot_processed_results(six_replications_analyses.color.accuracy_drops, 'Color 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.shape.accuracy_drops, 'Shape 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.texture.accuracy_drops, 'Material 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# Query-modulated analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'query_mod_replications' in cache:\n",
    "    query_mod_replications = cache['query_mod_replications']\n",
    "\n",
    "else:\n",
    "    query_mod_runs = analysis.query_modulated_runs_by_dimension(30)\n",
    "    query_mod_replications = {}\n",
    "\n",
    "    ignore_runs = [] # ('at6pkicv', )\n",
    "    for mod_level in query_mod_runs:\n",
    "        mod_level_runs = query_mod_runs[mod_level]\n",
    "\n",
    "        mod_level_dict = {dimension_name: analysis.process_multiple_runs(mod_level_runs[i], ignore_runs=ignore_runs) \n",
    "                          for i, dimension_name \n",
    "                          in enumerate(analysis.CONDITION_ANALYSES_FIELDS)}\n",
    "\n",
    "        query_mod_replications[mod_level] = analysis.ConditionAnalysesSet(**mod_level_dict)\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(query_mod_replications=query_mod_replications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = analysis.refresh_cache(dict(query_mod_updated_analyses=query_mod_replications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'six_replications_updated_analyses' in cache:\n",
    "#     six_replications_updated_analyses = cache['six_replications_updated_analyses']\n",
    "\n",
    "# else:\n",
    "query_mod_runs = analysis.query_modulated_runs_by_dimension(30)\n",
    "print('Loaded runs')\n",
    "\n",
    "# note: the equal accuracy field will come in as accuracy_drops\n",
    "query_mod_replications = {}\n",
    "mod_levels = list(query_mod_runs.keys())\n",
    "start_index = 0\n",
    "\n",
    "for mod_level in mod_levels[start_index:]:\n",
    "    print(f'Starting mod level {mod_level}')\n",
    "    mod_level_runs = query_mod_runs[mod_level]\n",
    "\n",
    "    mod_level_dict = {dimension_name: analysis.process_multiple_runs(mod_level_runs[i], \n",
    "                                                                     parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size) \n",
    "                      for i, dimension_name \n",
    "                      in enumerate(analysis.CONDITION_ANALYSES_FIELDS)}\n",
    "\n",
    "    query_mod_replications[mod_level] = analysis.ConditionAnalysesSet(**mod_level_dict)\n",
    "\n",
    "\n",
    "cache = analysis.refresh_cache(dict(query_mod_updated_analyses=query_mod_replications))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = analysis.refresh_cache(dict(query_mod_replications=query_mod_replications,\n",
    "                                    control_analyses=control_analyses,\n",
    "                                    six_replications_analyses=six_replications_analyses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'query_mod_total_curve_analyses' in cache:\n",
    "    query_mod_total_curve_analyses = cache['query_mod_total_curve_analyses']\n",
    "\n",
    "else:\n",
    "    query_mod_runs = analysis.query_modulated_runs_by_dimension(30)\n",
    "    print('Loaded runs')\n",
    "    \n",
    "    query_mod_total_curve_analyses = {}\n",
    "    mod_levels = list(query_mod_runs.keys())\n",
    "    start_index = 0\n",
    "    \n",
    "    for mod_level in mod_levels[start_index:]:\n",
    "        print(f'Starting mod level {mod_level}')\n",
    "        mod_level_runs = query_mod_runs[mod_level]\n",
    "\n",
    "        analyses_per_dimension = {}\n",
    "        \n",
    "        for run_set, dimension_name in zip(mod_level_runs, analysis.CONDITION_ANALYSES_FIELDS):\n",
    "            print(f'Starting {dimension_name}')\n",
    "            total_curve_raw, total_curve_mean, total_curve_std, total_curve_sem = \\\n",
    "                analysis.process_multiple_runs_total_task_training_curves(run_set)\n",
    "\n",
    "            analyses_per_dimension[dimension_name] = analysis.TotalCurveResults(raw=total_curve_raw,\n",
    "                                                                                mean=total_curve_mean, \n",
    "                                                                                std=total_curve_std, \n",
    "                                                                                sem=total_curve_sem)\n",
    "\n",
    "        query_mod_total_curve_analyses[mod_level] = analysis.ConditionAnalysesSet(**analyses_per_dimension)\n",
    "            \n",
    "    cache = analysis.refresh_cache(dict(query_mod_total_curve_analyses=query_mod_total_curve_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_mod_total_curve_analyses.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'maml_analyses' in cache:\n",
    "    maml_analyses = cache['maml_analyses']\n",
    "\n",
    "else:\n",
    "    maml_runs = analysis.load_runs(30, 'meta-learning-scaling/maml-sequential-benchmark')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "    #                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    maml_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(maml_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        maml_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    maml_analyses = analysis.ConditionAnalysesSet(**maml_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(maml_analyses=maml_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'six_replications_updated_analyses' in cache:\n",
    "#     six_replications_updated_analyses = cache['six_replications_updated_analyses']\n",
    "\n",
    "# else:\n",
    "maml_alpha_0_runs = analysis.load_runs(20, 'meta-learning-scaling/maml-alpha-0')\n",
    "print('Loaded runs')\n",
    "\n",
    "raise ValueError('This will not work yeet')\n",
    "\n",
    "# ignore_runs = set(['ac82mceh', '7kau3ypy', 'g9ujw7gg', 'avmcbnot'])\n",
    "ignore_runs = set()\n",
    "\n",
    "maml_alpha_0_analyses_dict = {}\n",
    "start_index = 0\n",
    "for run_set, dimension_name in zip(maml_alpha_0_runs[start_index:], \n",
    "                                   analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "    \n",
    "    maml_alpha_0_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "        run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "        ignore_runs=ignore_runs) \n",
    "\n",
    "maml_alpha_0_analyses = analysis.ConditionAnalysesSet(**maml_alpha_0_analyses_dict)\n",
    "cache = analysis.refresh_cache(dict(maml_alpha_0_analyses=maml_alpha_0_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'maml_meta_test_analyses' in cache:\n",
    "    maml_meta_test_analyses = cache['maml_meta_test_analyses']\n",
    "\n",
    "else:\n",
    "    maml_meta_test_runs = analysis.load_runs(30, 'meta-learning-scaling/maml-meta-test')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    maml_meta_test_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(maml_meta_test_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        maml_meta_test_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    maml_meta_test_analyses = analysis.ConditionAnalysesSet(**maml_meta_test_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(maml_meta_test_analyses=maml_meta_test_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'balanced_batches_analyses' in cache:\n",
    "    balanced_batches_analyses = cache['balanced_batches_analyses']\n",
    "\n",
    "else:\n",
    "    balanced_batches_runs = analysis.load_runs(30, 'meta-learning-scaling/balanced-batches-sequential-benchmark')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    balanced_batches_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(balanced_batches_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        balanced_batches_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    balanced_batches_analyses = analysis.ConditionAnalysesSet(**balanced_batches_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(balanced_batches_analyses=balanced_batches_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maml_comparison_run_ids = [1000, 1001, 2000, 2001, 2002, \n",
    "                           2003, 2004, 2005, 2006, 2007, \n",
    "                           2008, 2009, 3000, 3001, 3002, \n",
    "                           3003, 3004, 3005, 3006, 3007, 3008]\n",
    "\n",
    "maml_comparison_runs = analysis.load_runs(10, split_runs_by_dimension=False, valid_run_ids=set(maml_comparison_run_ids))\n",
    "print('Loaded runs')\n",
    "\n",
    "baseline_maml_comparison_analyses = analysis.ConditionAnalysesSet(\n",
    "    combined=analysis.process_multiple_runs(maml_comparison_runs.combined, \n",
    "                                            parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,))\n",
    "\n",
    "\n",
    "cache = analysis.refresh_cache(dict(baseline_maml_comparison_analyses=baseline_maml_comparison_analyses))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Simultaneous training in a dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded runs\n",
      "evmded6t\n",
      "4zmevm81\n",
      "rn1100g6\n",
      "valxzdh6\n",
      "vhar7vl0\n",
      "btpqc0l4\n",
      "y2okllq0\n",
      "p6jkyns3\n",
      "sdvikem3\n",
      "8gbqcsiy\n",
      "i1s48hvs 10\n",
      "yjkmbyfn\n",
      "hcj92b4t\n",
      "6g91pfuh\n",
      "2wzm82cl\n",
      "9gh81adt\n",
      "n27ud1lv\n",
      "4a0fluhk\n",
      "il27z68d\n",
      "rdy5b4h7\n",
      "93bl61z8\n",
      "h0mlpimb\n",
      "mskrvezv\n",
      "k6lcgekj\n",
      "bmlx3672\n",
      "y8roqwko\n",
      "0glw98lj\n",
      "7ewq32s7\n",
      "g6wppirc\n",
      "4rwxdt1j\n",
      "6jw8g94w 10\n",
      "cll4tmi4\n",
      "oi8y8zp9\n",
      "iwbib2kn\n",
      "8171o5q6\n",
      "v9nzvwna\n",
      "86693lo0\n",
      "nluhx1qs\n",
      "v3ezft64\n",
      "v2y1k1zu\n",
      "cwgjjrp5\n",
      "0wgkxa8e\n",
      "zh6j4sut\n",
      "8j2j6f1u\n",
      "p4fbaygn\n",
      "yfohbvi8\n",
      "vqavcfh8\n",
      "rd60a1ls\n",
      "qc4wk4kf\n",
      "00fm3ms0\n",
      "8misib06 10\n",
      "kzwulr2p\n",
      "1oe3hex8\n",
      "naiw1ed2\n",
      "7l3blxaz\n",
      "bv2v2yeg\n",
      "mgrxdvan\n",
      "tkb27tai\n",
      "qxtm0ddo\n",
      "em7rzcfv\n",
      "evmded6t\n",
      "cwgjjrp5\n",
      "4zmevm81\n",
      "0wgkxa8e\n",
      "rn1100g6\n",
      "zh6j4sut\n",
      "valxzdh6\n",
      "8j2j6f1u\n",
      "p4fbaygn\n",
      "vhar7vl0\n",
      "btpqc0l4 10\n",
      "yfohbvi8\n",
      "vqavcfh8\n",
      "y2okllq0\n",
      "rd60a1ls\n",
      "p6jkyns3\n",
      "93bl61z8\n",
      "h0mlpimb\n",
      "mskrvezv\n",
      "k6lcgekj\n",
      "bmlx3672 20\n",
      "y8roqwko\n",
      "0glw98lj\n",
      "7ewq32s7\n",
      "qc4wk4kf\n",
      "00fm3ms0\n",
      "g6wppirc\n",
      "4rwxdt1j\n",
      "sdvikem3\n",
      "8gbqcsiy\n",
      "8misib06 30\n",
      "i1s48hvs\n",
      "yjkmbyfn\n",
      "kzwulr2p\n",
      "hcj92b4t\n",
      "6g91pfuh\n",
      "2wzm82cl\n",
      "9gh81adt\n",
      "1oe3hex8\n",
      "naiw1ed2\n",
      "7l3blxaz 40\n",
      "n27ud1lv\n",
      "4a0fluhk\n",
      "bv2v2yeg\n",
      "mgrxdvan\n",
      "tkb27tai\n",
      "6jw8g94w\n",
      "cll4tmi4\n",
      "oi8y8zp9\n",
      "iwbib2kn\n",
      "8171o5q6 50\n",
      "v9nzvwna\n",
      "86693lo0\n",
      "nluhx1qs\n",
      "qxtm0ddo\n",
      "em7rzcfv\n",
      "v3ezft64\n",
      "v2y1k1zu\n",
      "il27z68d\n",
      "rdy5b4h7\n"
     ]
    }
   ],
   "source": [
    "# if 'simultaneous_training_analyses' in cache:\n",
    "#     simultaneous_training_analyses = cache['simultaneous_training_analyses']\n",
    "\n",
    "# else:\n",
    "simultaneous_training_runs = analysis.load_runs(20, 'meta-learning-scaling/simultaneous-training')\n",
    "print('Loaded runs')\n",
    "\n",
    "# ignore_runs = set(['u3gk9oio'])\n",
    "ignore_runs = set()\n",
    "\n",
    "simultaneous_training_analyses_dict = {}\n",
    "per_task_simultaneous_training_analyses_dict = {}\n",
    "start_index = 0\n",
    "for run_set, dimension_name in zip(simultaneous_training_runs[start_index:], \n",
    "                                   analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "    stacked_results, all_results_mean, all_results_std, all_results_sem, per_task_results_mean, per_task_results_std, per_task_results_sem = analysis.process_multiple_runs_simultaneous_training(run_set, ignore_runs=ignore_runs)\n",
    "\n",
    "    simultaneous_training_analyses_dict[dimension_name] = analysis.TotalCurveResults(raw=stacked_results,\n",
    "                                                                                     mean=all_results_mean, \n",
    "                                                                                     std=all_results_std, \n",
    "                                                                                     sem=all_results_sem)\n",
    "\n",
    "    per_task_simultaneous_training_analyses_dict[dimension_name] = analysis.TotalCurveResults(raw=stacked_results,\n",
    "                                                                                              mean=per_task_results_mean, \n",
    "                                                                                              std=per_task_results_std, \n",
    "                                                                                              sem=per_task_results_sem)\n",
    "\n",
    "simultaneous_training_analyses = analysis.ConditionAnalysesSet(**simultaneous_training_analyses_dict)\n",
    "per_task_simultaneous_training_analyses = analysis.ConditionAnalysesSet(**per_task_simultaneous_training_analyses_dict)\n",
    "cache = analysis.refresh_cache(dict(simultaneous_training_analyses=simultaneous_training_analyses,\n",
    "                                    per_task_simultaneous_training_analyses=per_task_simultaneous_training_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(np.isnan(simultaneous_training_analyses.combined.raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(simultaneous_training_analyses.combined.raw, axis=(1, 2)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "* Figure out how to deal with stacking -- can't properly stack since each execution has a different length\n",
    "* Create both the average over all tasks (with SD), and the average for each task (with SD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the number of times trained on for every query modulation level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = (7.25, 13)\n",
    "fonts = dict(fontsize=20)\n",
    "plots.plot_per_model_per_dimension(six_replications_analyses, query_mod_replications, \n",
    "                                   plots.examples_by_times_trained_on, \n",
    "                                   'Log examples to criterion by number of times trained',\n",
    "                                   fonts, ylim=ylim, sem_n=(20, 20, 20, 60), baseline_sem_n=(60, 60, 60, 180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.plot_per_model_per_dimension(six_replications_analyses, query_mod_replications, \n",
    "                                   plots.examples_by_num_tasks_trained, \n",
    "                                   'Log examples to criterion by number of tasks trained',\n",
    "                                   fonts, ylim=ylim, sem_n=(20, 20, 20, 60), baseline_sem_n=(60, 60, 60, 180))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison plots - compare all to second replication level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fonts = dict(fontsize=20)\n",
    "\n",
    "comparison_level = 4\n",
    "\n",
    "plots.comparison_plot_per_model(six_replications_analyses, query_mod_replications, \n",
    "                                plots.examples_by_times_trained_on, \n",
    "                                f'Examples to criterion compared to modulation level {comparison_level}',\n",
    "                                comparison_level, conditions=None, comparison_func=np.subtract, \n",
    "                                font_dict=fonts, ylim=None, shade_error=False, data_index=1, log_y=False,\n",
    "                                sem_n=(20, 20, 20, 60), baseline_sem_n=(60, 60, 60, 180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fonts = dict(fontsize=20)\n",
    "\n",
    "comparison_level = 0\n",
    "\n",
    "plots.comparison_plot_per_model(six_replications_analyses, query_mod_replications, \n",
    "                                plots.examples_by_times_trained_on, \n",
    "                                f'Examples to criterion compared to modulation level {comparison_level}',\n",
    "                                comparison_level, conditions=(3,), comparison_func=np.divide, \n",
    "                                font_dict=fonts, ylim=None, shade_error=False, \n",
    "                                data_index=0, log_x=False, log_y=False, comparison_first=True,\n",
    "                                sem_n=(20, 20, 20, 60), baseline_sem_n=(60, 60, 60, 180),\n",
    "                                colormap='cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fonts = dict(fontsize=20)\n",
    "\n",
    "comparison_level = 4\n",
    "\n",
    "plots.comparison_plot_per_model(six_replications_analyses, query_mod_replications, \n",
    "                                plots.examples_by_times_trained_on, \n",
    "                                f'Examples to criterion compared to modulation level {comparison_level}',\n",
    "                                comparison_level, conditions=(3,), comparison_func=np.divide, \n",
    "                                font_dict=fonts, ylim=None, shade_error=False, \n",
    "                                data_index=0, log_x=False, log_y=False, comparison_first=False,\n",
    "                                sem_n=(20, 20, 20, 60), baseline_sem_n=(60, 60, 60, 180),\n",
    "                                colormap='cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fonts = dict(fontsize=20)\n",
    "\n",
    "comparison_level = 0\n",
    "\n",
    "plots.comparison_plot_per_model(six_replications_analyses, query_mod_replications, \n",
    "                                plots.examples_by_num_tasks_trained, \n",
    "                                f'Examples to criterion compared to modulation level {comparison_level}',\n",
    "                                comparison_level, conditions=(3,), comparison_func=np.divide, \n",
    "                                font_dict=fonts, ylim=None, shade_error=False, \n",
    "                                data_index=0, log_y=False, comparison_first=True,\n",
    "                                sem_n=(20, 20, 20, 60), baseline_sem_n=(60, 60, 60, 180),\n",
    "                                colormap='cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fonts = dict(fontsize=20)\n",
    "\n",
    "comparison_level = 4\n",
    "\n",
    "plots.comparison_plot_per_model(six_replications_analyses, query_mod_replications, \n",
    "                                plots.examples_by_num_tasks_trained, \n",
    "                                'Log examples to criterion by number of tasks trained',\n",
    "                                comparison_level, conditions=None, comparison_func=np.subtract, \n",
    "                                font_dict=fonts, ylim=None, shade_error=False, data_index=1, log_y=False,\n",
    "                                sem_n=(20, 20, 20, 60), baseline_sem_n=(60, 60, 60, 180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fonts = dict(fontsize=20)\n",
    "\n",
    "comparison_level = 4\n",
    "\n",
    "plots.comparison_plot_per_model(six_replications_analyses, query_mod_replications, \n",
    "                                plots.examples_by_num_tasks_trained, \n",
    "                                'Log examples to criterion by number of tasks trained',\n",
    "                                comparison_level, conditions=None, comparison_func=np.divide, \n",
    "                                font_dict=fonts, ylim=None, shade_error=False, data_index=0, log_y=False,\n",
    "                                sem_n=(20, 20, 20, 60), baseline_sem_n=(60, 60, 60, 180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fonts = dict(fontsize=20)\n",
    "\n",
    "comparison_level = 0\n",
    "\n",
    "plots.comparison_plot_per_model(six_replications_analyses, query_mod_replications, \n",
    "                                plots.examples_by_times_trained_on, \n",
    "                                f'Accuracy after introducing new task compared to modulation level {comparison_level}',\n",
    "                                comparison_level, conditions=(3,), \n",
    "                                comparison_func=np.subtract, comparison_first=False,\n",
    "                                font_dict=fonts, ylim=None, shade_error=False, data_index=2, \n",
    "                                log_x=False, log_y=False,\n",
    "                                sem_n=(20, 20, 20, 60), baseline_sem_n=(60, 60, 60, 180),\n",
    "                                colormap='cool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fonts = dict(fontsize=20)\n",
    "\n",
    "comparison_level = 4\n",
    "\n",
    "plots.comparison_plot_per_model(six_replications_analyses, query_mod_replications, \n",
    "                                plots.examples_by_times_trained_on, \n",
    "                                f'Accuracy drop after introducing new task compared to modulation level {comparison_level}',\n",
    "                                comparison_level, conditions=(3,), comparison_func=np.subtract,\n",
    "                                font_dict=fonts, ylim=None, shade_error=False, data_index=2, log_y=False,\n",
    "                                sem_n=(20, 20, 20, 60), baseline_sem_n=(60, 60, 60, 180))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = '1231'\n",
    "s.count('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for outlines and analyzing the skewness of these distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, skewtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, examples = process_multiple_runs(six_replications_by_dimension_runs[1], debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = np.array(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ex[:,5,9], bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_results(skew(ex),  floatfmt=\".3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = skewtest(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.pvalue < 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the average number of example for each actual query/task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_difficulty = sum([r[1] for r in six_replications_by_dimension])\n",
    "query_names = ['blue', 'brown', 'cyan', 'gray', 'green', 'orange', 'pink',\n",
    "       'purple', 'red', 'yellow', 'cone', 'cube', 'cylinder',\n",
    "       'dodecahedron', 'ellipsoid', 'octahedron', 'pyramid', 'rectangle',\n",
    "       'sphere', 'torus', 'chain_mail', 'marble', 'maze', 'metal',\n",
    "       'metal_weave', 'polka', 'rubber', 'rug', 'tiles', 'wood_plank']\n",
    "dimension_names = ['color', 'shape', 'texture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 18))\n",
    "\n",
    "plt.suptitle('Average difficulty by query and number of times trained')\n",
    "\n",
    "for i in range(3):\n",
    "    ax = plt.subplot(3, 1, i + 1)\n",
    "    x_values = range(1, 11)\n",
    "    \n",
    "    for query_id in range(i * 10, (i + 1) * 10):\n",
    "        ax.plot(x_values, query_difficulty[query_id,:], label=f'{query_names[query_id]} ({query_id})')\n",
    "    \n",
    "    ax.set_title(dimension_names[i])\n",
    "    ax.set_xlabel('Number of times trained')\n",
    "    ax.set_ylabel('Average number of examples required')\n",
    "    ax.legend(loc='best')\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (1, 2, 3)\n",
    "b = (4, 5, 6)\n",
    "\n",
    "{c: d for (c, d) in zip(a, b)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log((4500, 22500, 45000, 90000, 225000, 450000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = six_replications_by_dimension_runs.combined[0].history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 100\n",
    "first_task_accuracy_per_epoch = np.zeros((10, samples))\n",
    "first_task_accuracy_per_epoch_counts = np.zeros((10, samples))\n",
    "new_task_accuracy_per_epoch = np.zeros((10, samples))\n",
    "new_task_accuracy_per_epoch_counts = np.zeros((10, samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_task_finished = df['Test Accuracy, Query #2'].first_valid_index() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_task_finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[analysis.TASK_ACC_COLS][:first_task_finished + 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Test Accuracy, Query #1'][first_task_finished + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_task_accuracy_per_epoch[0, 0:first_task_finished] += df['Test Accuracy, Query #1'][1:first_task_finished + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_task_accuracy_per_epoch[0, 0:first_task_finished]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_task = 2\n",
    "current_task_start = df[f'Test Accuracy, Query #{current_task}'].first_valid_index()\n",
    "current_task_end = df[f'Test Accuracy, Query #{current_task + 1}'].first_valid_index()\n",
    "\n",
    "df[analysis.TASK_ACC_COLS][current_task_start:current_task_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Test Accuracy, Query #1'][current_task_start:current_task_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_task_accuracy_per_epoch[1, 0:current_task_end - current_task_start] = df['Test Accuracy, Query #1'][current_task_start:current_task_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(((1, 1, 1, np.nan), (1, 1, 1, np.nan)))\n",
    "b = np.array(((2, 2, np.nan, np.nan), (2, 2, np.nan, np.nan)))\n",
    "c = np.array(((3, np.nan, np.nan, np.nan), (3, np.nan, np.nan, np.nan)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean([a, b, c], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanstd((a, b, c), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.argmax(np.isnan(k), axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan([b, c, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.array([a, b, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(~np.isnan(k), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.divide(a, np.array([3, 2, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasattr(k, 'shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
