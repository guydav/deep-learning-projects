{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/guydavidson/.netrc\n",
      "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!wandb login 9676e3cc95066e4865586082971f2653245f09b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.special import factorial\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from matplotlib import path as mpath\n",
    "\n",
    "import pickle\n",
    "import tabulate\n",
    "import wandb\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "from ipypb import ipb\n",
    "\n",
    "import meta_learning_data_analysis as analysis\n",
    "import meta_learning_analysis_plots as plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['six_replications_analyses', 'control_analyses', 'query_mod_replications', 'six_replications_updated_analyses', 'updated_control_analyses', 'query_mod_updated_analyses', 'forgetting_curves_raw_data', 'preliminary_maml_analyses', 'baseline_maml_comparison_analyses', 'maml_analyses', 'maml_alpha_0_analyses', 'maml_meta_test_analyses', 'balanced_batches_analyses', 'baseline_total_curve_analyses', 'control_total_curve_analyses', 'query_mod_total_curve_analyses', 'simultaneous_training_analyses', 'per_task_simultaneous_training_analyses', 'task_conditional_analyses', 'task_conditional_multiplicative_only_analyses', 'task_conditional_additive_only_analyses', 'task_conditional_weights', 'task_conditional_multiplicative_only_weights', 'task_conditional_additive_only_weights', 'forgetting_exp_decay_params', 'baseline_ratio_curriculum_analyses', 'baseline_power_curriculum_analyses', 'epochs_to_completion', 'ratio_curriculum_1_5_analyses', 'power_curriculum_2_analyses', 'length_by_task_order'])\n"
     ]
    }
   ],
   "source": [
    "cache = analysis.refresh_cache()\n",
    "print(cache.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# Baseline analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded runs\n",
      "iazzikhz Baseline-1059\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (22) into shape (23)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7f45f01dadff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                    analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n\u001b[1;32m     13\u001b[0m     updated_six_reps_dict[dimension_name] = analysis.process_multiple_runs(\n\u001b[0;32m---> 14\u001b[0;31m         run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size) \n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# combined_analysis = analysis.process_multiple_runs(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/deep-learning-projects/notebooks/meta_learning_data_analysis.py\u001b[0m in \u001b[0;36mprocess_multiple_runs\u001b[0;34m(runs, debug, ignore_runs, samples, parse_func)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0mexamples_to_criterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabsolute_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_drop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_task_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_task_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_run\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m         \u001b[0;31m# print(examples_to_criterion)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# print(np.log(examples_to_criterion))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/deep-learning-projects/notebooks/meta_learning_data_analysis.py\u001b[0m in \u001b[0;36mparse_run_results_with_new_task_accuracy_and_equal_size\u001b[0;34m(current_run_id, current_run, samples)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mabsolute_accuracy_equal_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Test Accuracy, Query #1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_row_blank\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0mfirst_task_accuracy_by_epoch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfirst_task_finished\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Test Accuracy, Query #1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_row_blank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfirst_task_finished\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \u001b[0mnew_task_accuracy_by_epoch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfirst_task_finished\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Test Accuracy, Query #1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_row_blank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfirst_task_finished\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (22) into shape (23)"
     ]
    }
   ],
   "source": [
    "if 'six_replications_updated_analyses' in cache:\n",
    "    six_replications_updated_analyses = cache['six_replications_updated_analyses']\n",
    "\n",
    "else:\n",
    "    six_replications_by_dimension_runs = analysis.load_runs(60)\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # note: the equal accuracy field will come in as accuracy_drops\n",
    "    updated_six_reps_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(six_replications_by_dimension_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "        updated_six_reps_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size) \n",
    "\n",
    "    # combined_analysis = analysis.process_multiple_runs(\n",
    "    #     six_replications_by_dimension_runs[3], \n",
    "    #     parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size)\n",
    "\n",
    "    six_replications_updated_analyses = analysis.ConditionAnalysesSet(**updated_six_reps_dict)\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(six_replications_updated_analyses=six_replications_updated_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'baseline_total_curve_analyses' in cache:\n",
    "#     baseline_total_curve_analyses = cache['baseline_total_curve_analyses']\n",
    "\n",
    "# else:\n",
    "six_replications_by_dimension_runs = analysis.load_runs(60)\n",
    "print('Loaded runs')\n",
    "\n",
    "analyses_per_dimension = {}\n",
    "\n",
    "for run_set, dimension_name in zip(six_replications_by_dimension_runs, analysis.CONDITION_ANALYSES_FIELDS):\n",
    "    print(f'Starting {dimension_name}')\n",
    "    total_curve_raw, total_curve_mean, total_curve_std, total_curve_sem = \\\n",
    "        analysis.process_multiple_runs_total_task_training_curves(run_set)\n",
    "\n",
    "    analyses_per_dimension[dimension_name] = analysis.TotalCurveResults(raw=total_curve_raw,\n",
    "                                                                        mean=total_curve_mean, \n",
    "                                                                        std=total_curve_std, \n",
    "                                                                        sem=total_curve_sem)\n",
    "\n",
    "total_curve_analyses = analysis.ConditionAnalysesSet(**analyses_per_dimension)\n",
    "cache = analysis.refresh_cache(dict(baseline_total_curve_analyses=total_curve_analyses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# Control analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'control_analyses' in cache:\n",
    "    control_analyses = cache['control_analyses']\n",
    "\n",
    "else:\n",
    "    control_runs = analysis.load_runs(150, 'meta-learning-scaling/sequential-benchmark-control', False)\n",
    "    print(f'Loaded runs')\n",
    "    control_analyses = analysis.ConditionAnalysesSet(combined=analysis.process_multiple_runs(control_runs.combined))\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(control_analyses=control_analyses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'six_replications_updated_analyses' in cache:\n",
    "#     six_replications_updated_analyses = cache['six_replications_updated_analyses']\n",
    "\n",
    "# else:\n",
    "control_runs = analysis.load_runs(150, 'meta-learning-scaling/sequential-benchmark-control', False)\n",
    "print('Loaded runs')\n",
    "\n",
    "updated_control_analyses = analysis.ConditionAnalysesSet(\n",
    "    combined=analysis.process_multiple_runs(control_runs.combined, \n",
    "                                            parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size))\n",
    "\n",
    "\n",
    "cache = analysis.refresh_cache(dict(updated_control_analyses=updated_control_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'control_total_curve_analyses' in cache:\n",
    "    control_total_curve_analyses = cache['control_total_curve_analyses']\n",
    "\n",
    "else:\n",
    "    control_runs = analysis.load_runs(150, 'meta-learning-scaling/sequential-benchmark-control', False)\n",
    "    print('Loaded runs')\n",
    "    \n",
    "    total_curve_raw, total_curve_mean, total_curve_std, total_curve_sem = \\\n",
    "            analysis.process_multiple_runs_total_task_training_curves(control_runs.combined)\n",
    "    \n",
    "    control_total_curve_analyses = analysis.ConditionAnalysesSet(\n",
    "        combined=analysis.TotalCurveResults(raw=total_curve_raw,\n",
    "                                            mean=total_curve_mean,\n",
    "                                            std=total_curve_std,\n",
    "                                            sem=total_curve_sem))\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(control_total_curve_analyses=control_total_curve_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = analysis.refresh_cache(dict(control_analyses=control_analyses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the number of examples by dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = (1000, 520000)\n",
    "\n",
    "plots.plot_processed_results(first_replication_analyses.color.examples, 'Color 10-run average', ylim)\n",
    "plots.plot_processed_results(first_replication_analyses.shape.examples, 'Shape 10-run average', ylim)\n",
    "plots.plot_processed_results(first_replication_analyses.texture.examples, 'Material 10-run average', ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = (1000, 700000)\n",
    "\n",
    "plots.plot_processed_results(six_replications_analyses.color.examples, 'Color 60-run average', ylim)\n",
    "plots.plot_processed_results(six_replications_analyses.shape.examples, 'Shape 60-run average', ylim)\n",
    "plots.plot_processed_results(six_replications_analyses.texture.examples, 'Material 60-run average', ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the log of the number of examples to criterion, in each dimension, with error bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the combined results over all 180 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = (7.75, 13.25)\n",
    "\n",
    "plots.plot_processed_results(six_replications_analyses.combined.log_examples, 'Combined 180-run average', \n",
    "                       ylim, log_x=(True, True), log_y=True, sem_n=180, shade_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the absolute accuracy after introducing a new task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = None\n",
    "\n",
    "plots.plot_processed_results(six_replications_analyses.color.accuracies, 'Color 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.shape.accuracies, 'Shape 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.texture.accuracies, 'Material 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.combined.accuracies, 'Combined 180-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=180, shade_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the accuracy drop after introducing a new task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = None\n",
    "\n",
    "plots.plot_processed_results(six_replications_analyses.color.accuracy_drops, 'Color 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.shape.accuracy_drops, 'Shape 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.texture.accuracy_drops, 'Material 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# Query-modulated analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'query_mod_replications' in cache:\n",
    "    query_mod_replications = cache['query_mod_replications']\n",
    "\n",
    "else:\n",
    "    query_mod_runs = analysis.query_modulated_runs_by_dimension(30)\n",
    "    query_mod_replications = {}\n",
    "\n",
    "    ignore_runs = [] # ('at6pkicv', )\n",
    "    for mod_level in query_mod_runs:\n",
    "        mod_level_runs = query_mod_runs[mod_level]\n",
    "\n",
    "        mod_level_dict = {dimension_name: analysis.process_multiple_runs(mod_level_runs[i], ignore_runs=ignore_runs) \n",
    "                          for i, dimension_name \n",
    "                          in enumerate(analysis.CONDITION_ANALYSES_FIELDS)}\n",
    "\n",
    "        query_mod_replications[mod_level] = analysis.ConditionAnalysesSet(**mod_level_dict)\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(query_mod_replications=query_mod_replications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = analysis.refresh_cache(dict(query_mod_updated_analyses=query_mod_replications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'six_replications_updated_analyses' in cache:\n",
    "#     six_replications_updated_analyses = cache['six_replications_updated_analyses']\n",
    "\n",
    "# else:\n",
    "query_mod_runs = analysis.query_modulated_runs_by_dimension(30)\n",
    "print('Loaded runs')\n",
    "\n",
    "# note: the equal accuracy field will come in as accuracy_drops\n",
    "query_mod_replications = {}\n",
    "mod_levels = list(query_mod_runs.keys())\n",
    "start_index = 0\n",
    "\n",
    "for mod_level in mod_levels[start_index:]:\n",
    "    print(f'Starting mod level {mod_level}')\n",
    "    mod_level_runs = query_mod_runs[mod_level]\n",
    "\n",
    "    mod_level_dict = {dimension_name: analysis.process_multiple_runs(mod_level_runs[i], \n",
    "                                                                     parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size) \n",
    "                      for i, dimension_name \n",
    "                      in enumerate(analysis.CONDITION_ANALYSES_FIELDS)}\n",
    "\n",
    "    query_mod_replications[mod_level] = analysis.ConditionAnalysesSet(**mod_level_dict)\n",
    "\n",
    "\n",
    "cache = analysis.refresh_cache(dict(query_mod_updated_analyses=query_mod_replications))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = analysis.refresh_cache(dict(query_mod_replications=query_mod_replications,\n",
    "                                    control_analyses=control_analyses,\n",
    "                                    six_replications_analyses=six_replications_analyses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'query_mod_total_curve_analyses' in cache:\n",
    "    query_mod_total_curve_analyses = cache['query_mod_total_curve_analyses']\n",
    "\n",
    "else:\n",
    "    query_mod_runs = analysis.query_modulated_runs_by_dimension(30)\n",
    "    print('Loaded runs')\n",
    "    \n",
    "    query_mod_total_curve_analyses = {}\n",
    "    mod_levels = list(query_mod_runs.keys())\n",
    "    start_index = 0\n",
    "    \n",
    "    for mod_level in mod_levels[start_index:]:\n",
    "        print(f'Starting mod level {mod_level}')\n",
    "        mod_level_runs = query_mod_runs[mod_level]\n",
    "\n",
    "        analyses_per_dimension = {}\n",
    "        \n",
    "        for run_set, dimension_name in zip(mod_level_runs, analysis.CONDITION_ANALYSES_FIELDS):\n",
    "            print(f'Starting {dimension_name}')\n",
    "            total_curve_raw, total_curve_mean, total_curve_std, total_curve_sem = \\\n",
    "                analysis.process_multiple_runs_total_task_training_curves(run_set)\n",
    "\n",
    "            analyses_per_dimension[dimension_name] = analysis.TotalCurveResults(raw=total_curve_raw,\n",
    "                                                                                mean=total_curve_mean, \n",
    "                                                                                std=total_curve_std, \n",
    "                                                                                sem=total_curve_sem)\n",
    "\n",
    "        query_mod_total_curve_analyses[mod_level] = analysis.ConditionAnalysesSet(**analyses_per_dimension)\n",
    "            \n",
    "    cache = analysis.refresh_cache(dict(query_mod_total_curve_analyses=query_mod_total_curve_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_mod_total_curve_analyses.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'maml_analyses' in cache:\n",
    "    maml_analyses = cache['maml_analyses']\n",
    "\n",
    "else:\n",
    "    maml_runs = analysis.load_runs(30, 'meta-learning-scaling/maml-sequential-benchmark')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "    #                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    maml_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(maml_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        maml_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    maml_analyses = analysis.ConditionAnalysesSet(**maml_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(maml_analyses=maml_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'six_replications_updated_analyses' in cache:\n",
    "#     six_replications_updated_analyses = cache['six_replications_updated_analyses']\n",
    "\n",
    "# else:\n",
    "maml_alpha_0_runs = analysis.load_runs(20, 'meta-learning-scaling/maml-alpha-0')\n",
    "print('Loaded runs')\n",
    "\n",
    "raise ValueError('This will not work yeet')\n",
    "\n",
    "# ignore_runs = set(['ac82mceh', '7kau3ypy', 'g9ujw7gg', 'avmcbnot'])\n",
    "ignore_runs = set()\n",
    "\n",
    "maml_alpha_0_analyses_dict = {}\n",
    "start_index = 0\n",
    "for run_set, dimension_name in zip(maml_alpha_0_runs[start_index:], \n",
    "                                   analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "    \n",
    "    maml_alpha_0_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "        run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "        ignore_runs=ignore_runs) \n",
    "\n",
    "maml_alpha_0_analyses = analysis.ConditionAnalysesSet(**maml_alpha_0_analyses_dict)\n",
    "cache = analysis.refresh_cache(dict(maml_alpha_0_analyses=maml_alpha_0_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'maml_meta_test_analyses' in cache:\n",
    "    maml_meta_test_analyses = cache['maml_meta_test_analyses']\n",
    "\n",
    "else:\n",
    "    maml_meta_test_runs = analysis.load_runs(30, 'meta-learning-scaling/maml-meta-test')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    maml_meta_test_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(maml_meta_test_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        maml_meta_test_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    maml_meta_test_analyses = analysis.ConditionAnalysesSet(**maml_meta_test_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(maml_meta_test_analyses=maml_meta_test_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'balanced_batches_analyses' in cache:\n",
    "    balanced_batches_analyses = cache['balanced_batches_analyses']\n",
    "\n",
    "else:\n",
    "    balanced_batches_runs = analysis.load_runs(30, 'meta-learning-scaling/balanced-batches-sequential-benchmark')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    balanced_batches_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(balanced_batches_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        balanced_batches_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    balanced_batches_analyses = analysis.ConditionAnalysesSet(**balanced_batches_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(balanced_batches_analyses=balanced_batches_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maml_comparison_run_ids = [1000, 1001, 2000, 2001, 2002, \n",
    "                           2003, 2004, 2005, 2006, 2007, \n",
    "                           2008, 2009, 3000, 3001, 3002, \n",
    "                           3003, 3004, 3005, 3006, 3007, 3008]\n",
    "\n",
    "maml_comparison_runs = analysis.load_runs(10, split_runs_by_dimension=False, valid_run_ids=set(maml_comparison_run_ids))\n",
    "print('Loaded runs')\n",
    "\n",
    "baseline_maml_comparison_analyses = analysis.ConditionAnalysesSet(\n",
    "    combined=analysis.process_multiple_runs(maml_comparison_runs.combined, \n",
    "                                            parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,))\n",
    "\n",
    "\n",
    "cache = analysis.refresh_cache(dict(baseline_maml_comparison_analyses=baseline_maml_comparison_analyses))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Simultaneous training in a dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'simultaneous_training_analyses' in cache:\n",
    "#     simultaneous_training_analyses = cache['simultaneous_training_analyses']\n",
    "\n",
    "# else:\n",
    "simultaneous_training_runs = analysis.load_runs(20, 'meta-learning-scaling/simultaneous-training')\n",
    "print('Loaded runs')\n",
    "\n",
    "# ignore_runs = set(['u3gk9oio'])\n",
    "ignore_runs = set()\n",
    "\n",
    "simultaneous_training_analyses_dict = {}\n",
    "per_task_simultaneous_training_analyses_dict = {}\n",
    "start_index = 0\n",
    "for run_set, dimension_name in zip(simultaneous_training_runs[start_index:], \n",
    "                                   analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "    stacked_results, all_results_mean, all_results_std, all_results_sem, per_task_results_mean, per_task_results_std, per_task_results_sem = analysis.process_multiple_runs_simultaneous_training(run_set, ignore_runs=ignore_runs)\n",
    "\n",
    "    simultaneous_training_analyses_dict[dimension_name] = analysis.TotalCurveResults(raw=stacked_results,\n",
    "                                                                                     mean=all_results_mean, \n",
    "                                                                                     std=all_results_std, \n",
    "                                                                                     sem=all_results_sem)\n",
    "\n",
    "    per_task_simultaneous_training_analyses_dict[dimension_name] = analysis.TotalCurveResults(raw=stacked_results,\n",
    "                                                                                              mean=per_task_results_mean, \n",
    "                                                                                              std=per_task_results_std, \n",
    "                                                                                              sem=per_task_results_sem)\n",
    "\n",
    "simultaneous_training_analyses = analysis.ConditionAnalysesSet(**simultaneous_training_analyses_dict)\n",
    "per_task_simultaneous_training_analyses = analysis.ConditionAnalysesSet(**per_task_simultaneous_training_analyses_dict)\n",
    "cache = analysis.refresh_cache(dict(simultaneous_training_analyses=simultaneous_training_analyses,\n",
    "                                    per_task_simultaneous_training_analyses=per_task_simultaneous_training_analyses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# New Task Modulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'task_conditional_analyses' in cache:\n",
    "    task_conditional_analyses = cache['task_conditional_analyses']\n",
    "\n",
    "else:\n",
    "    task_modulated_runs = analysis.load_runs(20, 'meta-learning-scaling/task-conditional-all-layers')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    # ignore_runs = set(['Task-conditional-[0, 1, 2, 3]-1017'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    task_modulated_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(task_modulated_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        task_modulated_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    task_conditional_analyses = analysis.ConditionAnalysesSet(**task_modulated_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(task_conditional_analyses=task_conditional_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'task_conditional_multiplicative_only_analyses' in cache:\n",
    "    task_conditional_multiplicative_only_analyses = cache['task_conditional_multiplicative_only_analyses']\n",
    "\n",
    "else:\n",
    "    task_modulated_runs = analysis.load_runs(20, 'meta-learning-scaling/task-conditional-all-layers-multiplicative-only')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['Task-conditional-multiplicative-[0, 1, 2, 3]-1006',\n",
    "    #                   'Task-conditional-multiplicative-[0, 1, 2, 3]-1005'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    task_modulated_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(task_modulated_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        task_modulated_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    task_conditional_multiplicative_only_analyses = analysis.ConditionAnalysesSet(**task_modulated_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(task_conditional_multiplicative_only_analyses=task_conditional_multiplicative_only_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'task_conditional_additive_only_analyses' in cache:\n",
    "    task_conditional_additive_only_analyses = cache['task_conditional_additive_only_analyses']\n",
    "\n",
    "else:\n",
    "    task_modulated_runs = analyses_caches/alysis.load_runs(20, 'meta-learning-scaling/task-conditional-all-layers-additive-only')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    task_modulated_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(task_modulated_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        task_modulated_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    task_conditional_additive_only_analyses = analysis.ConditionAnalysesSet(**task_modulated_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(task_conditional_additive_only_analyses=task_conditional_additive_only_analyses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the task-conditional weights en masse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'task_conditional_weights' in cache:\n",
    "    task_conditional_weights = cache['task_conditional_weights']\n",
    "\n",
    "else:\n",
    "    task_modulated_runs = analysis.load_runs(20, 'meta-learning-scaling/task-conditional-all-layers')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    ignore_runs = set(['Task-conditional-[0, 1, 2, 3]-1013',\n",
    "                      'Task-conditional-[0, 1, 2, 3]-1011',\n",
    "                      'Task-conditional-[0, 1, 2, 3]-1001'])\n",
    "#     ignore_runs = set()\n",
    "\n",
    "    task_conditional_weights = analysis.parse_task_conditional_weights(task_modulated_runs.combined,\n",
    "                                                                      additive=True,\n",
    "                                                                      multiplicative=True,\n",
    "                                                                      ignore_runs=ignore_runs)\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(task_conditional_weights=task_conditional_weights))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'task_conditional_multiplicative_only_weights' in cache:\n",
    "    task_conditional_multiplicative_only_weights = cache['task_conditional_multiplicative_only_weights']\n",
    "\n",
    "else:\n",
    "    task_modulated_runs = analysis.load_runs(20, 'meta-learning-scaling/task-conditional-all-layers-multiplicative-only')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['Task-conditional-multiplicative-[0, 1, 2, 3]-1006',\n",
    "    #                   'Task-conditional-multiplicative-[0, 1, 2, 3]-1005'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    task_conditional_multiplicative_only_weights = analysis.parse_task_conditional_weights(task_modulated_runs.combined,\n",
    "                                                                      additive=False,\n",
    "                                                                      multiplicative=True,\n",
    "                                                                      ignore_runs=ignore_runs)\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(task_conditional_multiplicative_only_weights=task_conditional_multiplicative_only_weights))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'task_conditional_additive_only_weights' in cache:\n",
    "    task_conditional_additive_only_weights = cache['task_conditional_additive_only_weights']\n",
    "\n",
    "else:\n",
    "    task_modulated_runs = analysis.load_runs(20, 'meta-learning-scaling/task-conditional-all-layers-additive-only')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    task_conditional_additive_only_weights = analysis.parse_task_conditional_weights(task_modulated_runs.combined,\n",
    "                                                                      additive=True,\n",
    "                                                                      multiplicative=False,\n",
    "                                                                      ignore_runs=ignore_runs)\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(task_conditional_additive_only_weights=task_conditional_additive_only_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "# The curriculum experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded runs\n",
      "0eogfvh3 baseline-curriculum-balanced-batches-1014\n",
      "73oixjxc baseline-curriculum-balanced-batches-1019\n",
      "tsh7vvp3 baseline-curriculum-balanced-batches-1013\n",
      "y13dp57j baseline-curriculum-balanced-batches-1018\n",
      "pz13hlhl baseline-curriculum-balanced-batches-1017\n",
      "nlffgtyt baseline-curriculum-balanced-batches-1016\n",
      "4s3capi4 baseline-curriculum-balanced-batches-1012\n",
      "pl8umlkc baseline-curriculum-balanced-batches-1011\n",
      "vcga6xtr baseline-curriculum-balanced-batches-1015\n",
      "p6uhr687 baseline-curriculum-balanced-batches-1010\n",
      "ai99wvyu baseline-curriculum-balanced-batches-1009 10\n",
      "uls4nztc baseline-curriculum-balanced-batches-1008\n",
      "j028dzy6 baseline-curriculum-balanced-batches-1007\n",
      "x8m4e500 baseline-curriculum-balanced-batches-1006\n",
      "dpftgyrt baseline-curriculum-balanced-batches-1005\n",
      "53wbl878 baseline-curriculum-balanced-batches-1004\n",
      "1t9mfnje baseline-curriculum-balanced-batches-1003\n",
      "p1bav3wk baseline-curriculum-balanced-batches-1002\n",
      "4eihb20f baseline-curriculum-balanced-batches-1001\n",
      "26ni8pzm baseline-curriculum-balanced-batches-1000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 241\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "xcktkzwd baseline-curriculum-balanced-batches-2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guydavidson/projects/deep-learning-projects/notebooks/meta_learning_data_analysis.py:559: RuntimeWarning: Mean of empty slice\n",
      "  # to avoid division by zero\n",
      "/Users/guydavidson/opt/anaconda3/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1667: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qb3s84lq baseline-curriculum-balanced-batches-2018\n",
      "znfqcwlh baseline-curriculum-balanced-batches-2017\n",
      "n1t7iufu baseline-curriculum-balanced-batches-2016\n",
      "pczljdrf baseline-curriculum-balanced-batches-2015\n",
      "k6n7c1z2 baseline-curriculum-balanced-batches-2014\n",
      "abqrc9z8 baseline-curriculum-balanced-batches-2013\n",
      "4g9m2lws baseline-curriculum-balanced-batches-2012\n",
      "6fhjcx4j baseline-curriculum-balanced-batches-2011\n",
      "hsefm9m4 baseline-curriculum-balanced-batches-2010\n",
      "7sti2qwq baseline-curriculum-balanced-batches-2009 10\n",
      "u4mncf9o baseline-curriculum-balanced-batches-2008\n",
      "9ue05xpt baseline-curriculum-balanced-batches-2007\n",
      "t4n1coyp baseline-curriculum-balanced-batches-2006\n",
      "73gl7yho baseline-curriculum-balanced-batches-2005\n",
      "3lrhiout baseline-curriculum-balanced-batches-2004\n",
      "k93b6kb5 baseline-curriculum-balanced-batches-2003\n",
      "000mn5uc baseline-curriculum-balanced-batches-2002\n",
      "0kbhx05v baseline-curriculum-balanced-batches-2001\n",
      "k7v63p2m baseline-curriculum-balanced-batches-2000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 35\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "pw82m3w1 baseline-curriculum-balanced-batches-3019\n",
      "5s6rjjjs baseline-curriculum-balanced-batches-3018\n",
      "2kl9levn baseline-curriculum-balanced-batches-3017\n",
      "b8gcrpgv baseline-curriculum-balanced-batches-3016\n",
      "2tk5rgp9 baseline-curriculum-balanced-batches-3015\n",
      "wbpf904r baseline-curriculum-balanced-batches-3014\n",
      "6i7dk292 baseline-curriculum-balanced-batches-3013\n",
      "svxp98eq baseline-curriculum-balanced-batches-3012\n",
      "q3i9mcp1 baseline-curriculum-balanced-batches-3011\n",
      "rl62d3s1 baseline-curriculum-balanced-batches-3010\n",
      "sct3wok3 baseline-curriculum-balanced-batches-3009 10\n",
      "av09le2a baseline-curriculum-balanced-batches-3008\n",
      "h98cilrk baseline-curriculum-balanced-batches-3007\n",
      "ph8zkpko baseline-curriculum-balanced-batches-3006\n",
      "pd8slg7z baseline-curriculum-balanced-batches-3005\n",
      "zfnfys55 baseline-curriculum-balanced-batches-3004\n",
      "qtdmjxjl baseline-curriculum-balanced-batches-3003\n",
      "2g2lgg1u baseline-curriculum-balanced-batches-3002\n",
      "8ddbef10 baseline-curriculum-balanced-batches-3001\n",
      "4l2du3oc baseline-curriculum-balanced-batches-3000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 29\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "0eogfvh3 baseline-curriculum-balanced-batches-1014\n",
      "73oixjxc baseline-curriculum-balanced-batches-1019\n",
      "tsh7vvp3 baseline-curriculum-balanced-batches-1013\n",
      "y13dp57j baseline-curriculum-balanced-batches-1018\n",
      "pz13hlhl baseline-curriculum-balanced-batches-1017\n",
      "nlffgtyt baseline-curriculum-balanced-batches-1016\n",
      "4s3capi4 baseline-curriculum-balanced-batches-1012\n",
      "pl8umlkc baseline-curriculum-balanced-batches-1011\n",
      "vcga6xtr baseline-curriculum-balanced-batches-1015\n",
      "p6uhr687 baseline-curriculum-balanced-batches-1010\n",
      "pw82m3w1 baseline-curriculum-balanced-batches-3019 10\n",
      "5s6rjjjs baseline-curriculum-balanced-batches-3018\n",
      "2kl9levn baseline-curriculum-balanced-batches-3017\n",
      "xcktkzwd baseline-curriculum-balanced-batches-2019\n",
      "b8gcrpgv baseline-curriculum-balanced-batches-3016\n",
      "qb3s84lq baseline-curriculum-balanced-batches-2018\n",
      "znfqcwlh baseline-curriculum-balanced-batches-2017\n",
      "2tk5rgp9 baseline-curriculum-balanced-batches-3015\n",
      "n1t7iufu baseline-curriculum-balanced-batches-2016\n",
      "pczljdrf baseline-curriculum-balanced-batches-2015\n",
      "wbpf904r baseline-curriculum-balanced-batches-3014 20\n",
      "k6n7c1z2 baseline-curriculum-balanced-batches-2014\n",
      "6i7dk292 baseline-curriculum-balanced-batches-3013\n",
      "abqrc9z8 baseline-curriculum-balanced-batches-2013\n",
      "svxp98eq baseline-curriculum-balanced-batches-3012\n",
      "4g9m2lws baseline-curriculum-balanced-batches-2012\n",
      "6fhjcx4j baseline-curriculum-balanced-batches-2011\n",
      "q3i9mcp1 baseline-curriculum-balanced-batches-3011\n",
      "hsefm9m4 baseline-curriculum-balanced-batches-2010\n",
      "rl62d3s1 baseline-curriculum-balanced-batches-3010\n",
      "ai99wvyu baseline-curriculum-balanced-batches-1009 30\n",
      "uls4nztc baseline-curriculum-balanced-batches-1008\n",
      "j028dzy6 baseline-curriculum-balanced-batches-1007\n",
      "x8m4e500 baseline-curriculum-balanced-batches-1006\n",
      "dpftgyrt baseline-curriculum-balanced-batches-1005\n",
      "53wbl878 baseline-curriculum-balanced-batches-1004\n",
      "1t9mfnje baseline-curriculum-balanced-batches-1003\n",
      "p1bav3wk baseline-curriculum-balanced-batches-1002\n",
      "sct3wok3 baseline-curriculum-balanced-batches-3009\n",
      "av09le2a baseline-curriculum-balanced-batches-3008\n",
      "h98cilrk baseline-curriculum-balanced-batches-3007 40\n",
      "7sti2qwq baseline-curriculum-balanced-batches-2009\n",
      "ph8zkpko baseline-curriculum-balanced-batches-3006\n",
      "u4mncf9o baseline-curriculum-balanced-batches-2008\n",
      "9ue05xpt baseline-curriculum-balanced-batches-2007\n",
      "pd8slg7z baseline-curriculum-balanced-batches-3005\n",
      "t4n1coyp baseline-curriculum-balanced-batches-2006\n",
      "zfnfys55 baseline-curriculum-balanced-batches-3004\n",
      "4eihb20f baseline-curriculum-balanced-batches-1001\n",
      "73gl7yho baseline-curriculum-balanced-batches-2005\n",
      "qtdmjxjl baseline-curriculum-balanced-batches-3003 50\n",
      "3lrhiout baseline-curriculum-balanced-batches-2004\n",
      "2g2lgg1u baseline-curriculum-balanced-batches-3002\n",
      "k93b6kb5 baseline-curriculum-balanced-batches-2003\n",
      "000mn5uc baseline-curriculum-balanced-batches-2002\n",
      "8ddbef10 baseline-curriculum-balanced-batches-3001\n",
      "0kbhx05v baseline-curriculum-balanced-batches-2001\n",
      "4l2du3oc baseline-curriculum-balanced-batches-3000\n",
      "k7v63p2m baseline-curriculum-balanced-batches-2000\n",
      "26ni8pzm baseline-curriculum-balanced-batches-1000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 241\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n"
     ]
    }
   ],
   "source": [
    "if 'baseline_ratio_curriculum_analyses' in cache:\n",
    "    baseline_ratio_curriculum_analyses = cache['baseline_ratio_curriculum_analyses']\n",
    "\n",
    "else:\n",
    "    runs = analysis.load_runs(20, 'meta-learning-scaling/baseline-curriculum-balanced-batches')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "    #                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    baseline_ratio_curriculum_analyses = analysis.ConditionAnalysesSet(**analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(baseline_ratio_curriculum_analyses=baseline_ratio_curriculum_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded runs\n",
      "nwsyc1b0 curriculum-ratio-1-5-balanced-batches-1013\n",
      "2wcd0n9e curriculum-ratio-1-5-balanced-batches-1019\n",
      "as0626cy curriculum-ratio-1-5-balanced-batches-1014\n",
      "q15aj1dr curriculum-ratio-1-5-balanced-batches-1018\n",
      "p1rpvmy5 curriculum-ratio-1-5-balanced-batches-1017\n",
      "50nsmuru curriculum-ratio-1-5-balanced-batches-1012\n",
      "v2rwsev0 curriculum-ratio-1-5-balanced-batches-1011\n",
      "iiqjpr55 curriculum-ratio-1-5-balanced-batches-1016\n",
      "1361kipc curriculum-ratio-1-5-balanced-batches-1015\n",
      "pwdohs4v curriculum-ratio-1-5-balanced-batches-1010\n",
      "8144n3sw curriculum-ratio-1-5-balanced-batches-1002 10\n",
      "710tk50k curriculum-ratio-1-5-balanced-batches-1004\n",
      "6132vg77 curriculum-ratio-1-5-balanced-batches-1003\n",
      "ev329iry curriculum-ratio-1-5-balanced-batches-1009\n",
      "u1hejur5 curriculum-ratio-1-5-balanced-batches-1001\n",
      "b3kyhxje curriculum-ratio-1-5-balanced-batches-1008\n",
      "zbvy733z curriculum-ratio-1-5-balanced-batches-1007\n",
      "qatiu7gl curriculum-ratio-1-5-balanced-batches-1006\n",
      "0twjv9p0 curriculum-ratio-1-5-balanced-batches-1005\n",
      "rwobrcww curriculum-ratio-1-5-balanced-batches-1000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 576\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "kpbdo2gk curriculum-ratio-1-5-balanced-batches-2019\n",
      "25zlnfmo curriculum-ratio-1-5-balanced-batches-2018\n",
      "j1m9rhzu curriculum-ratio-1-5-balanced-batches-2017\n",
      "0927uhn5 curriculum-ratio-1-5-balanced-batches-2016\n",
      "bmiw7797 curriculum-ratio-1-5-balanced-batches-2015\n",
      "uu5y0ywk curriculum-ratio-1-5-balanced-batches-2014\n",
      "e962ol54 curriculum-ratio-1-5-balanced-batches-2013\n",
      "3s3s6b0x curriculum-ratio-1-5-balanced-batches-2012\n",
      "0tpzpv80 curriculum-ratio-1-5-balanced-batches-2011\n",
      "odmzvl2a curriculum-ratio-1-5-balanced-batches-2010\n",
      "jvtzw4pq curriculum-ratio-1-5-balanced-batches-2009 10\n",
      "fero15qw curriculum-ratio-1-5-balanced-batches-2008\n",
      "461gfadg curriculum-ratio-1-5-balanced-batches-2007\n",
      "yia9ianh curriculum-ratio-1-5-balanced-batches-2006\n",
      "zj5hus6x curriculum-ratio-1-5-balanced-batches-2005\n",
      "qht85qn7 curriculum-ratio-1-5-balanced-batches-2004\n",
      "xybg9nni curriculum-ratio-1-5-balanced-batches-2003\n",
      "m4k2wdpy curriculum-ratio-1-5-balanced-batches-2002\n",
      "nsqlft7g curriculum-ratio-1-5-balanced-batches-2001\n",
      "76rxljqs curriculum-ratio-1-5-balanced-batches-2000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 51\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "haihwruq curriculum-ratio-1-5-balanced-batches-3019\n",
      "076rzaiy curriculum-ratio-1-5-balanced-batches-3018\n",
      "tdbjessr curriculum-ratio-1-5-balanced-batches-3017\n",
      "we7ce57g curriculum-ratio-1-5-balanced-batches-3016\n",
      "cw56agwo curriculum-ratio-1-5-balanced-batches-3015\n",
      "0c9wlbky curriculum-ratio-1-5-balanced-batches-3014\n",
      "5ic8j69p curriculum-ratio-1-5-balanced-batches-3013\n",
      "eemwpsf9 curriculum-ratio-1-5-balanced-batches-3012\n",
      "ty6lfypb curriculum-ratio-1-5-balanced-batches-3011\n",
      "cynm7j7g curriculum-ratio-1-5-balanced-batches-3010\n",
      "8z2c10as curriculum-ratio-1-5-balanced-batches-3009 10\n",
      "obo0ynys curriculum-ratio-1-5-balanced-batches-3008\n",
      "rt3sgkcg curriculum-ratio-1-5-balanced-batches-3007\n",
      "1x0q9zo8 curriculum-ratio-1-5-balanced-batches-3006\n",
      "sn4pombi curriculum-ratio-1-5-balanced-batches-3005\n",
      "1lyjccrv curriculum-ratio-1-5-balanced-batches-3004\n",
      "vpwgtwg0 curriculum-ratio-1-5-balanced-batches-3003\n",
      "t00p1mr5 curriculum-ratio-1-5-balanced-batches-3002\n",
      "jvm73b2j curriculum-ratio-1-5-balanced-batches-3001\n",
      "j2u8c442 curriculum-ratio-1-5-balanced-batches-3000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 115\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "nwsyc1b0 curriculum-ratio-1-5-balanced-batches-1013\n",
      "2wcd0n9e curriculum-ratio-1-5-balanced-batches-1019\n",
      "as0626cy curriculum-ratio-1-5-balanced-batches-1014\n",
      "q15aj1dr curriculum-ratio-1-5-balanced-batches-1018\n",
      "p1rpvmy5 curriculum-ratio-1-5-balanced-batches-1017\n",
      "50nsmuru curriculum-ratio-1-5-balanced-batches-1012\n",
      "v2rwsev0 curriculum-ratio-1-5-balanced-batches-1011\n",
      "iiqjpr55 curriculum-ratio-1-5-balanced-batches-1016\n",
      "1361kipc curriculum-ratio-1-5-balanced-batches-1015\n",
      "pwdohs4v curriculum-ratio-1-5-balanced-batches-1010\n",
      "haihwruq curriculum-ratio-1-5-balanced-batches-3019 10\n",
      "076rzaiy curriculum-ratio-1-5-balanced-batches-3018\n",
      "tdbjessr curriculum-ratio-1-5-balanced-batches-3017\n",
      "we7ce57g curriculum-ratio-1-5-balanced-batches-3016\n",
      "cw56agwo curriculum-ratio-1-5-balanced-batches-3015\n",
      "0c9wlbky curriculum-ratio-1-5-balanced-batches-3014\n",
      "5ic8j69p curriculum-ratio-1-5-balanced-batches-3013\n",
      "eemwpsf9 curriculum-ratio-1-5-balanced-batches-3012\n",
      "kpbdo2gk curriculum-ratio-1-5-balanced-batches-2019\n",
      "25zlnfmo curriculum-ratio-1-5-balanced-batches-2018\n",
      "ty6lfypb curriculum-ratio-1-5-balanced-batches-3011 20\n",
      "j1m9rhzu curriculum-ratio-1-5-balanced-batches-2017\n",
      "0927uhn5 curriculum-ratio-1-5-balanced-batches-2016\n",
      "bmiw7797 curriculum-ratio-1-5-balanced-batches-2015\n",
      "uu5y0ywk curriculum-ratio-1-5-balanced-batches-2014\n",
      "e962ol54 curriculum-ratio-1-5-balanced-batches-2013\n",
      "3s3s6b0x curriculum-ratio-1-5-balanced-batches-2012\n",
      "0tpzpv80 curriculum-ratio-1-5-balanced-batches-2011\n",
      "odmzvl2a curriculum-ratio-1-5-balanced-batches-2010\n",
      "cynm7j7g curriculum-ratio-1-5-balanced-batches-3010\n",
      "8144n3sw curriculum-ratio-1-5-balanced-batches-1002 30\n",
      "710tk50k curriculum-ratio-1-5-balanced-batches-1004\n",
      "6132vg77 curriculum-ratio-1-5-balanced-batches-1003\n",
      "ev329iry curriculum-ratio-1-5-balanced-batches-1009\n",
      "u1hejur5 curriculum-ratio-1-5-balanced-batches-1001\n",
      "b3kyhxje curriculum-ratio-1-5-balanced-batches-1008\n",
      "zbvy733z curriculum-ratio-1-5-balanced-batches-1007\n",
      "qatiu7gl curriculum-ratio-1-5-balanced-batches-1006\n",
      "0twjv9p0 curriculum-ratio-1-5-balanced-batches-1005\n",
      "8z2c10as curriculum-ratio-1-5-balanced-batches-3009\n",
      "obo0ynys curriculum-ratio-1-5-balanced-batches-3008 40\n",
      "rwobrcww curriculum-ratio-1-5-balanced-batches-1000\n",
      "rt3sgkcg curriculum-ratio-1-5-balanced-batches-3007\n",
      "jvtzw4pq curriculum-ratio-1-5-balanced-batches-2009\n",
      "1x0q9zo8 curriculum-ratio-1-5-balanced-batches-3006\n",
      "sn4pombi curriculum-ratio-1-5-balanced-batches-3005\n",
      "fero15qw curriculum-ratio-1-5-balanced-batches-2008\n",
      "461gfadg curriculum-ratio-1-5-balanced-batches-2007\n",
      "yia9ianh curriculum-ratio-1-5-balanced-batches-2006\n",
      "1lyjccrv curriculum-ratio-1-5-balanced-batches-3004\n",
      "vpwgtwg0 curriculum-ratio-1-5-balanced-batches-3003 50\n",
      "zj5hus6x curriculum-ratio-1-5-balanced-batches-2005\n",
      "t00p1mr5 curriculum-ratio-1-5-balanced-batches-3002\n",
      "qht85qn7 curriculum-ratio-1-5-balanced-batches-2004\n",
      "xybg9nni curriculum-ratio-1-5-balanced-batches-2003\n",
      "m4k2wdpy curriculum-ratio-1-5-balanced-batches-2002\n",
      "jvm73b2j curriculum-ratio-1-5-balanced-batches-3001\n",
      "nsqlft7g curriculum-ratio-1-5-balanced-batches-2001\n",
      "j2u8c442 curriculum-ratio-1-5-balanced-batches-3000\n",
      "76rxljqs curriculum-ratio-1-5-balanced-batches-2000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 576\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n"
     ]
    }
   ],
   "source": [
    "if 'ratio_curriculum_1_5_analyses' in cache:\n",
    "    ratio_curriculum_1_5_analyses = cache['ratio_curriculum_1_5_analyses']\n",
    "\n",
    "else:\n",
    "    runs = analysis.load_runs(20, 'meta-learning-scaling/curriculum-ratio-1-5-balanced-batches')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "    #                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    ratio_curriculum_1_5_analyses = analysis.ConditionAnalysesSet(**analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(ratio_curriculum_1_5_analyses=ratio_curriculum_1_5_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded runs\n",
      "ccxwwp22 power-curriculum-default-alpha-balanced-batches-1019\n",
      "4t75a279 power-curriculum-default-alpha-balanced-batches-1018\n",
      "doy9nqq0 power-curriculum-default-alpha-balanced-batches-1017\n",
      "z7al34ak power-curriculum-default-alpha-balanced-batches-1016\n",
      "haxw09sh power-curriculum-default-alpha-balanced-batches-1014\n",
      "2mp9wmnt power-curriculum-default-alpha-balanced-batches-1015\n",
      "a0wpbr3q power-curriculum-default-alpha-balanced-batches-1013\n",
      "38pkdpl9 power-curriculum-default-alpha-balanced-batches-1012\n",
      "da6u7oqf power-curriculum-default-alpha-balanced-batches-1011\n",
      "t5ddmb6k power-curriculum-default-alpha-balanced-batches-1010\n",
      "vl9emwn0 power-curriculum-default-alpha-balanced-batches-1009 10\n",
      "i6k50wtb power-curriculum-default-alpha-balanced-batches-1008\n",
      "9askkb8z power-curriculum-default-alpha-balanced-batches-1007\n",
      "k55xvrd0 power-curriculum-default-alpha-balanced-batches-1006\n",
      "rkadecu8 power-curriculum-default-alpha-balanced-batches-1005\n",
      "042nzzk4 power-curriculum-default-alpha-balanced-batches-1004\n",
      "ridfafpl power-curriculum-default-alpha-balanced-batches-1003\n",
      "misbra23 power-curriculum-default-alpha-balanced-batches-1002\n",
      "lyhl9c6z power-curriculum-default-alpha-balanced-batches-1001\n",
      "tsrnhjt6 power-curriculum-default-alpha-balanced-batches-1000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 94\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "4ob2ke95 power-curriculum-default-alpha-balanced-batches-2019\n",
      "op4czlw9 power-curriculum-default-alpha-balanced-batches-2018\n",
      "3kbwwy3p power-curriculum-default-alpha-balanced-batches-2017\n",
      "2mimaibn power-curriculum-default-alpha-balanced-batches-2016\n",
      "og3wlgeq power-curriculum-default-alpha-balanced-batches-2015\n",
      "ir7llz4s power-curriculum-default-alpha-balanced-batches-2014\n",
      "lbyoj0qv power-curriculum-default-alpha-balanced-batches-2013\n",
      "evcd9ko1 power-curriculum-default-alpha-balanced-batches-2012\n",
      "lhfv9zqx power-curriculum-default-alpha-balanced-batches-2011\n",
      "upqfphnz power-curriculum-default-alpha-balanced-batches-2010\n",
      "md2v3lhi power-curriculum-default-alpha-balanced-batches-2009 10\n",
      "zkoe5ztu power-curriculum-default-alpha-balanced-batches-2008\n",
      "hfm9mke0 power-curriculum-default-alpha-balanced-batches-2007\n",
      "k99hllt4 power-curriculum-default-alpha-balanced-batches-2006\n",
      "ypxrxqh2 power-curriculum-default-alpha-balanced-batches-2005\n",
      "52md37sc power-curriculum-default-alpha-balanced-batches-2004\n",
      "y41tusxf power-curriculum-default-alpha-balanced-batches-2003\n",
      "bb7am8ky power-curriculum-default-alpha-balanced-batches-2002\n",
      "i34jpezo power-curriculum-default-alpha-balanced-batches-2001\n",
      "gff335mu power-curriculum-default-alpha-balanced-batches-2000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 36\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "djyog5q5 power-curriculum-default-alpha-balanced-batches-3019\n",
      "maimf6lv power-curriculum-default-alpha-balanced-batches-3018\n",
      "nvtw0myu power-curriculum-default-alpha-balanced-batches-3017\n",
      "evqmquqy power-curriculum-default-alpha-balanced-batches-3016\n",
      "rnlnscqi power-curriculum-default-alpha-balanced-batches-3015\n",
      "jmhykh8q power-curriculum-default-alpha-balanced-batches-3014\n",
      "bnfp5tww power-curriculum-default-alpha-balanced-batches-3013\n",
      "ht077vx9 power-curriculum-default-alpha-balanced-batches-3012\n",
      "9th5pgiv power-curriculum-default-alpha-balanced-batches-3011\n",
      "ob9ixr6q power-curriculum-default-alpha-balanced-batches-3010\n",
      "rgi19yzs power-curriculum-default-alpha-balanced-batches-3009 10\n",
      "x4vm8vif power-curriculum-default-alpha-balanced-batches-3008\n",
      "m2ussvfi power-curriculum-default-alpha-balanced-batches-3007\n",
      "w7jgqrp7 power-curriculum-default-alpha-balanced-batches-3006\n",
      "fu8rp33i power-curriculum-default-alpha-balanced-batches-3005\n",
      "t10e81c6 power-curriculum-default-alpha-balanced-batches-3004\n",
      "y6r6binq power-curriculum-default-alpha-balanced-batches-3003\n",
      "u6cy4c63 power-curriculum-default-alpha-balanced-batches-3002\n",
      "6vshoxel power-curriculum-default-alpha-balanced-batches-3001\n",
      "z9l0zlxc power-curriculum-default-alpha-balanced-batches-3000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 46\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "ccxwwp22 power-curriculum-default-alpha-balanced-batches-1019\n",
      "4t75a279 power-curriculum-default-alpha-balanced-batches-1018\n",
      "doy9nqq0 power-curriculum-default-alpha-balanced-batches-1017\n",
      "z7al34ak power-curriculum-default-alpha-balanced-batches-1016\n",
      "haxw09sh power-curriculum-default-alpha-balanced-batches-1014\n",
      "2mp9wmnt power-curriculum-default-alpha-balanced-batches-1015\n",
      "a0wpbr3q power-curriculum-default-alpha-balanced-batches-1013\n",
      "38pkdpl9 power-curriculum-default-alpha-balanced-batches-1012\n",
      "da6u7oqf power-curriculum-default-alpha-balanced-batches-1011\n",
      "t5ddmb6k power-curriculum-default-alpha-balanced-batches-1010\n",
      "djyog5q5 power-curriculum-default-alpha-balanced-batches-3019 10\n",
      "maimf6lv power-curriculum-default-alpha-balanced-batches-3018\n",
      "4ob2ke95 power-curriculum-default-alpha-balanced-batches-2019\n",
      "nvtw0myu power-curriculum-default-alpha-balanced-batches-3017\n",
      "op4czlw9 power-curriculum-default-alpha-balanced-batches-2018\n",
      "evqmquqy power-curriculum-default-alpha-balanced-batches-3016\n",
      "3kbwwy3p power-curriculum-default-alpha-balanced-batches-2017\n",
      "2mimaibn power-curriculum-default-alpha-balanced-batches-2016\n",
      "rnlnscqi power-curriculum-default-alpha-balanced-batches-3015\n",
      "og3wlgeq power-curriculum-default-alpha-balanced-batches-2015\n",
      "jmhykh8q power-curriculum-default-alpha-balanced-batches-3014 20\n",
      "ir7llz4s power-curriculum-default-alpha-balanced-batches-2014\n",
      "bnfp5tww power-curriculum-default-alpha-balanced-batches-3013\n",
      "lbyoj0qv power-curriculum-default-alpha-balanced-batches-2013\n",
      "ht077vx9 power-curriculum-default-alpha-balanced-batches-3012\n",
      "evcd9ko1 power-curriculum-default-alpha-balanced-batches-2012\n",
      "lhfv9zqx power-curriculum-default-alpha-balanced-batches-2011\n",
      "9th5pgiv power-curriculum-default-alpha-balanced-batches-3011\n",
      "upqfphnz power-curriculum-default-alpha-balanced-batches-2010\n",
      "ob9ixr6q power-curriculum-default-alpha-balanced-batches-3010\n",
      "vl9emwn0 power-curriculum-default-alpha-balanced-batches-1009 30\n",
      "i6k50wtb power-curriculum-default-alpha-balanced-batches-1008\n",
      "9askkb8z power-curriculum-default-alpha-balanced-batches-1007\n",
      "k55xvrd0 power-curriculum-default-alpha-balanced-batches-1006\n",
      "rkadecu8 power-curriculum-default-alpha-balanced-batches-1005\n",
      "042nzzk4 power-curriculum-default-alpha-balanced-batches-1004\n",
      "rgi19yzs power-curriculum-default-alpha-balanced-batches-3009\n",
      "md2v3lhi power-curriculum-default-alpha-balanced-batches-2009\n",
      "x4vm8vif power-curriculum-default-alpha-balanced-batches-3008\n",
      "ridfafpl power-curriculum-default-alpha-balanced-batches-1003\n",
      "zkoe5ztu power-curriculum-default-alpha-balanced-batches-2008 40\n",
      "m2ussvfi power-curriculum-default-alpha-balanced-batches-3007\n",
      "hfm9mke0 power-curriculum-default-alpha-balanced-batches-2007\n",
      "w7jgqrp7 power-curriculum-default-alpha-balanced-batches-3006\n",
      "k99hllt4 power-curriculum-default-alpha-balanced-batches-2006\n",
      "misbra23 power-curriculum-default-alpha-balanced-batches-1002\n",
      "fu8rp33i power-curriculum-default-alpha-balanced-batches-3005\n",
      "ypxrxqh2 power-curriculum-default-alpha-balanced-batches-2005\n",
      "t10e81c6 power-curriculum-default-alpha-balanced-batches-3004\n",
      "52md37sc power-curriculum-default-alpha-balanced-batches-2004\n",
      "y6r6binq power-curriculum-default-alpha-balanced-batches-3003 50\n",
      "y41tusxf power-curriculum-default-alpha-balanced-batches-2003\n",
      "u6cy4c63 power-curriculum-default-alpha-balanced-batches-3002\n",
      "lyhl9c6z power-curriculum-default-alpha-balanced-batches-1001\n",
      "bb7am8ky power-curriculum-default-alpha-balanced-batches-2002\n",
      "6vshoxel power-curriculum-default-alpha-balanced-batches-3001\n",
      "i34jpezo power-curriculum-default-alpha-balanced-batches-2001\n",
      "z9l0zlxc power-curriculum-default-alpha-balanced-batches-3000\n",
      "gff335mu power-curriculum-default-alpha-balanced-batches-2000\n",
      "tsrnhjt6 power-curriculum-default-alpha-balanced-batches-1000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 94\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n"
     ]
    }
   ],
   "source": [
    "if 'baseline_power_curriculum_analyses' in cache:\n",
    "    baseline_power_curriculum_analyses = cache['baseline_power_curriculum_analyses']\n",
    "\n",
    "else:\n",
    "    runs = analysis.load_runs(20, 'meta-learning-scaling/power-curriculum-default-alpha-balanced-batches')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "    #                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    baseline_power_curriculum_analyses = analysis.ConditionAnalysesSet(**analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(baseline_power_curriculum_analyses=baseline_power_curriculum_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded runs\n",
      "zthzz67w power-curriculum-alpha-2-balanced-batches-1012\n",
      "0w0pex28 power-curriculum-alpha-2-balanced-batches-1017\n",
      "ctwrkyy6 power-curriculum-alpha-2-balanced-batches-1014\n",
      "bdpoo2d1 power-curriculum-alpha-2-balanced-batches-1016\n",
      "kiuc2pdv power-curriculum-alpha-2-balanced-batches-1013\n",
      "96lcdbwd power-curriculum-alpha-2-balanced-batches-1011\n",
      "dylccd2p power-curriculum-alpha-2-balanced-batches-1019\n",
      "g88qx9fl power-curriculum-alpha-2-balanced-batches-1018\n",
      "wktqpnd7 power-curriculum-alpha-2-balanced-batches-1015\n",
      "wm7s3dwa power-curriculum-alpha-2-balanced-batches-1010\n",
      "a4hj9y7k power-curriculum-alpha-2-balanced-batches-1002 10\n",
      "0a1iifo9 power-curriculum-alpha-2-balanced-batches-1004\n",
      "t071il8w power-curriculum-alpha-2-balanced-batches-1009\n",
      "8dtqbr1j power-curriculum-alpha-2-balanced-batches-1008\n",
      "mdjzx1jh power-curriculum-alpha-2-balanced-batches-1003\n",
      "p88x7rzh power-curriculum-alpha-2-balanced-batches-1001\n",
      "81so866q power-curriculum-alpha-2-balanced-batches-1007\n",
      "itt7kotx power-curriculum-alpha-2-balanced-batches-1006\n",
      "j94dgyhy power-curriculum-alpha-2-balanced-batches-1005\n",
      "4s2sznro power-curriculum-alpha-2-balanced-batches-1000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 914\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "wfqqfxw1 power-curriculum-alpha-2-balanced-batches-2019\n",
      "pvjfmk8m power-curriculum-alpha-2-balanced-batches-2018\n",
      "ymbu87sl power-curriculum-alpha-2-balanced-batches-2017\n",
      "ahwn8jt9 power-curriculum-alpha-2-balanced-batches-2016\n",
      "1mwtxiqm power-curriculum-alpha-2-balanced-batches-2015\n",
      "562f6vqo power-curriculum-alpha-2-balanced-batches-2014\n",
      "jokq9ifo power-curriculum-alpha-2-balanced-batches-2013\n",
      "urlulgxj power-curriculum-alpha-2-balanced-batches-2012\n",
      "7599gku5 power-curriculum-alpha-2-balanced-batches-2011\n",
      "5l3s9b0n power-curriculum-alpha-2-balanced-batches-2010\n",
      "1j7x7cx0 power-curriculum-alpha-2-balanced-batches-2009 10\n",
      "tkbqabcy power-curriculum-alpha-2-balanced-batches-2008\n",
      "e2yilpsf power-curriculum-alpha-2-balanced-batches-2007\n",
      "oze9neb3 power-curriculum-alpha-2-balanced-batches-2006\n",
      "iozr2j1y power-curriculum-alpha-2-balanced-batches-2005\n",
      "hgbn92hj power-curriculum-alpha-2-balanced-batches-2004\n",
      "1uyhapjj power-curriculum-alpha-2-balanced-batches-2003\n",
      "90qs1ggr power-curriculum-alpha-2-balanced-batches-2002\n",
      "hzviecjv power-curriculum-alpha-2-balanced-batches-2001\n",
      "auhn18le power-curriculum-alpha-2-balanced-batches-2000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 91\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "u60z72ym power-curriculum-alpha-2-balanced-batches-3019\n",
      "0wc0ak1n power-curriculum-alpha-2-balanced-batches-3018\n",
      "lq0cbjuu power-curriculum-alpha-2-balanced-batches-3017\n",
      "q5ucveij power-curriculum-alpha-2-balanced-batches-3016\n",
      "yezggf4g power-curriculum-alpha-2-balanced-batches-3015\n",
      "6m1sp4o0 power-curriculum-alpha-2-balanced-batches-3014\n",
      "j9d7aq68 power-curriculum-alpha-2-balanced-batches-3013\n",
      "cpg2ndv0 power-curriculum-alpha-2-balanced-batches-3012\n",
      "997mnkwi power-curriculum-alpha-2-balanced-batches-3011\n",
      "nyafqc4r power-curriculum-alpha-2-balanced-batches-3010\n",
      "s801akir power-curriculum-alpha-2-balanced-batches-3009 10\n",
      "mnma5wha power-curriculum-alpha-2-balanced-batches-3008\n",
      "9hcoqk36 power-curriculum-alpha-2-balanced-batches-3007\n",
      "wq7l6495 power-curriculum-alpha-2-balanced-batches-3006\n",
      "3fh9lrwx power-curriculum-alpha-2-balanced-batches-3005\n",
      "qgv2q9eb power-curriculum-alpha-2-balanced-batches-3004\n",
      "8qnc7fgz power-curriculum-alpha-2-balanced-batches-3003\n",
      "ch9qru5o power-curriculum-alpha-2-balanced-batches-3002\n",
      "82mv7f8y power-curriculum-alpha-2-balanced-batches-3001\n",
      "5eetqy8y power-curriculum-alpha-2-balanced-batches-3000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 138\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "zthzz67w power-curriculum-alpha-2-balanced-batches-1012\n",
      "0w0pex28 power-curriculum-alpha-2-balanced-batches-1017\n",
      "ctwrkyy6 power-curriculum-alpha-2-balanced-batches-1014\n",
      "bdpoo2d1 power-curriculum-alpha-2-balanced-batches-1016\n",
      "kiuc2pdv power-curriculum-alpha-2-balanced-batches-1013\n",
      "96lcdbwd power-curriculum-alpha-2-balanced-batches-1011\n",
      "dylccd2p power-curriculum-alpha-2-balanced-batches-1019\n",
      "u60z72ym power-curriculum-alpha-2-balanced-batches-3019\n",
      "g88qx9fl power-curriculum-alpha-2-balanced-batches-1018\n",
      "wktqpnd7 power-curriculum-alpha-2-balanced-batches-1015\n",
      "wm7s3dwa power-curriculum-alpha-2-balanced-batches-1010 10\n",
      "0wc0ak1n power-curriculum-alpha-2-balanced-batches-3018\n",
      "lq0cbjuu power-curriculum-alpha-2-balanced-batches-3017\n",
      "q5ucveij power-curriculum-alpha-2-balanced-batches-3016\n",
      "yezggf4g power-curriculum-alpha-2-balanced-batches-3015\n",
      "wfqqfxw1 power-curriculum-alpha-2-balanced-batches-2019\n",
      "6m1sp4o0 power-curriculum-alpha-2-balanced-batches-3014\n",
      "pvjfmk8m power-curriculum-alpha-2-balanced-batches-2018\n",
      "j9d7aq68 power-curriculum-alpha-2-balanced-batches-3013\n",
      "ymbu87sl power-curriculum-alpha-2-balanced-batches-2017\n",
      "ahwn8jt9 power-curriculum-alpha-2-balanced-batches-2016 20\n",
      "cpg2ndv0 power-curriculum-alpha-2-balanced-batches-3012\n",
      "1mwtxiqm power-curriculum-alpha-2-balanced-batches-2015\n",
      "997mnkwi power-curriculum-alpha-2-balanced-batches-3011\n",
      "562f6vqo power-curriculum-alpha-2-balanced-batches-2014\n",
      "jokq9ifo power-curriculum-alpha-2-balanced-batches-2013\n",
      "nyafqc4r power-curriculum-alpha-2-balanced-batches-3010\n",
      "urlulgxj power-curriculum-alpha-2-balanced-batches-2012\n",
      "7599gku5 power-curriculum-alpha-2-balanced-batches-2011\n",
      "5l3s9b0n power-curriculum-alpha-2-balanced-batches-2010\n",
      "a4hj9y7k power-curriculum-alpha-2-balanced-batches-1002 30\n",
      "0a1iifo9 power-curriculum-alpha-2-balanced-batches-1004\n",
      "t071il8w power-curriculum-alpha-2-balanced-batches-1009\n",
      "8dtqbr1j power-curriculum-alpha-2-balanced-batches-1008\n",
      "s801akir power-curriculum-alpha-2-balanced-batches-3009\n",
      "mdjzx1jh power-curriculum-alpha-2-balanced-batches-1003\n",
      "mnma5wha power-curriculum-alpha-2-balanced-batches-3008\n",
      "p88x7rzh power-curriculum-alpha-2-balanced-batches-1001\n",
      "1j7x7cx0 power-curriculum-alpha-2-balanced-batches-2009\n",
      "9hcoqk36 power-curriculum-alpha-2-balanced-batches-3007\n",
      "81so866q power-curriculum-alpha-2-balanced-batches-1007 40\n",
      "tkbqabcy power-curriculum-alpha-2-balanced-batches-2008\n",
      "wq7l6495 power-curriculum-alpha-2-balanced-batches-3006\n",
      "itt7kotx power-curriculum-alpha-2-balanced-batches-1006\n",
      "e2yilpsf power-curriculum-alpha-2-balanced-batches-2007\n",
      "3fh9lrwx power-curriculum-alpha-2-balanced-batches-3005\n",
      "j94dgyhy power-curriculum-alpha-2-balanced-batches-1005\n",
      "oze9neb3 power-curriculum-alpha-2-balanced-batches-2006\n",
      "qgv2q9eb power-curriculum-alpha-2-balanced-batches-3004\n",
      "iozr2j1y power-curriculum-alpha-2-balanced-batches-2005\n",
      "4s2sznro power-curriculum-alpha-2-balanced-batches-1000 50\n",
      "8qnc7fgz power-curriculum-alpha-2-balanced-batches-3003\n",
      "hgbn92hj power-curriculum-alpha-2-balanced-batches-2004\n",
      "1uyhapjj power-curriculum-alpha-2-balanced-batches-2003\n",
      "ch9qru5o power-curriculum-alpha-2-balanced-batches-3002\n",
      "90qs1ggr power-curriculum-alpha-2-balanced-batches-2002\n",
      "82mv7f8y power-curriculum-alpha-2-balanced-batches-3001\n",
      "hzviecjv power-curriculum-alpha-2-balanced-batches-2001\n",
      "5eetqy8y power-curriculum-alpha-2-balanced-batches-3000\n",
      "auhn18le power-curriculum-alpha-2-balanced-batches-2000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 914\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n"
     ]
    }
   ],
   "source": [
    "if 'power_curriculum_2_analyses' in cache:\n",
    "    power_curriculum_2_analyses = cache['power_curriculum_2_analyses']\n",
    "\n",
    "else:\n",
    "    runs = analysis.load_runs(20, 'meta-learning-scaling/power-curriculum-alpha-2-balanced-batches')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "    #                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    power_curriculum_2_analyses = analysis.ConditionAnalysesSet(**analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(power_curriculum_2_analyses=power_curriculum_2_analyses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the epochs to completion in each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_TO_COMPLETION_SETS = (\n",
    "    # name, URI, num_runs, ignore_runs, split_by_condition\n",
    "    ('baseline', 'meta-learning-scaling/sequential-benchmark-baseline', 60, None, True),\n",
    "    # ('heterogeneous', 'meta-learning-scaling/sequential-benchmark-control', 150, None, False),\n",
    "    # (),  # TODO: query-modulated, too?\n",
    "    ('ratio_curriculum', 'meta-learning-scaling/baseline-curriculum-balanced-batches', 20, None, True),\n",
    "    ('ratio_curriculum_1_5', 'meta-learning-scaling/curriculum-ratio-1-5-balanced-batches', 20, None, True),\n",
    "    ('power_curriculum', 'meta-learning-scaling/power-curriculum-default-alpha-balanced-batches', 20, None, True),\n",
    "    ('power_curriculum_2', 'meta-learning-scaling/power-curriculum-alpha-2-balanced-batches', 20, None, True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">baseline/color</span>\n",
       "<progress style=\"width:45ex\" max=\"60\" value=\"60\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">60/60</span>\n",
       "<span class=\"Time-label\">[04:21<00:04, 4.36s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " baseline/color [█████████████████████████████████████████████] 60/60 [04:21<00:04, 4.36s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">baseline/shape</span>\n",
       "<progress style=\"width:45ex\" max=\"60\" value=\"60\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">60/60</span>\n",
       "<span class=\"Time-label\">[01:56<00:02, 1.93s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " baseline/shape [█████████████████████████████████████████████] 60/60 [01:56<00:02, 1.93s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">baseline/texture</span>\n",
       "<progress style=\"width:45ex\" max=\"60\" value=\"60\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">60/60</span>\n",
       "<span class=\"Time-label\">[02:47<00:02, 2.79s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "baseline/textur [█████████████████████████████████████████████] 60/60 [02:47<00:02, 2.79s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">baseline/combined</span>\n",
       "<progress style=\"width:45ex\" max=\"180\" value=\"180\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">180/180</span>\n",
       "<span class=\"Time-label\">[09:47<00:03, 3.26s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "baseline/combin [█████████████████████████████████████████████] 180/180 [09:47<00:03, 3.26s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error resolved after 0:00:19.720891, resuming normal operation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">ratio_curriculum/color</span>\n",
       "<progress style=\"width:45ex\" max=\"20\" value=\"20\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">20/20</span>\n",
       "<span class=\"Time-label\">[00:32<00:03, 1.60s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "ratio_curriculu [█████████████████████████████████████████████] 20/20 [00:32<00:03, 1.60s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">ratio_curriculum/shape</span>\n",
       "<progress style=\"width:45ex\" max=\"20\" value=\"20\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">20/20</span>\n",
       "<span class=\"Time-label\">[00:17<00:01, 0.86s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "ratio_curriculu [█████████████████████████████████████████████] 20/20 [00:17<00:01, 0.86s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">ratio_curriculum/texture</span>\n",
       "<progress style=\"width:45ex\" max=\"20\" value=\"20\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">20/20</span>\n",
       "<span class=\"Time-label\">[00:20<00:01, 0.98s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "ratio_curriculu [█████████████████████████████████████████████] 20/20 [00:20<00:01, 0.98s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">ratio_curriculum/combined</span>\n",
       "<progress style=\"width:45ex\" max=\"60\" value=\"60\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">60/60</span>\n",
       "<span class=\"Time-label\">[01:12<00:02, 1.20s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "ratio_curriculu [█████████████████████████████████████████████] 60/60 [01:12<00:02, 1.20s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">ratio_curriculum_1_5/color</span>\n",
       "<progress style=\"width:45ex\" max=\"20\" value=\"20\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">20/20</span>\n",
       "<span class=\"Time-label\">[00:37<00:03, 1.87s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "ratio_curriculu [█████████████████████████████████████████████] 20/20 [00:37<00:03, 1.87s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guydavidson/projects/deep-learning-projects/notebooks/meta_learning_data_analysis.py:498: RuntimeWarning: divide by zero encountered in log\n",
      "  ResultSet(name=name, mean=np.nanmean(np.log(results_per_run), axis=0), std=np.nanstd(np.log(results_per_run), axis=0)),\\\n",
      "/Users/guydavidson/opt/anaconda3/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1541: RuntimeWarning: invalid value encountered in subtract\n",
      "  np.subtract(arr, avg, out=arr, casting='unsafe')\n",
      "/Users/guydavidson/projects/deep-learning-projects/notebooks/meta_learning_data_analysis.py:500: RuntimeWarning: divide by zero encountered in log\n",
      "  ResultSet(name=name, mean=np.nanmean(np.log(trials_per_run), axis=0), std=np.nanstd(np.log(trials_per_run), axis=0)),\\\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">ratio_curriculum_1_5/shape</span>\n",
       "<progress style=\"width:45ex\" max=\"20\" value=\"20\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">20/20</span>\n",
       "<span class=\"Time-label\">[00:21<00:01, 1.03s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "ratio_curriculu [█████████████████████████████████████████████] 20/20 [00:21<00:01, 1.03s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">ratio_curriculum_1_5/texture</span>\n",
       "<progress style=\"width:45ex\" max=\"20\" value=\"20\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">20/20</span>\n",
       "<span class=\"Time-label\">[00:31<00:01, 1.53s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "ratio_curriculu [█████████████████████████████████████████████] 20/20 [00:31<00:01, 1.53s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">ratio_curriculum_1_5/combined</span>\n",
       "<progress style=\"width:45ex\" max=\"60\" value=\"60\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">60/60</span>\n",
       "<span class=\"Time-label\">[01:28<00:01, 1.47s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "ratio_curriculu [█████████████████████████████████████████████] 60/60 [01:28<00:01, 1.47s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">power_curriculum/color</span>\n",
       "<progress style=\"width:45ex\" max=\"20\" value=\"20\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">20/20</span>\n",
       "<span class=\"Time-label\">[00:31<00:02, 1.57s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "power_curriculu [█████████████████████████████████████████████] 20/20 [00:31<00:02, 1.57s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">power_curriculum/shape</span>\n",
       "<progress style=\"width:45ex\" max=\"20\" value=\"20\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">20/20</span>\n",
       "<span class=\"Time-label\">[00:17<00:01, 0.84s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "power_curriculu [█████████████████████████████████████████████] 20/20 [00:17<00:01, 0.84s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">power_curriculum/texture</span>\n",
       "<progress style=\"width:45ex\" max=\"20\" value=\"20\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">20/20</span>\n",
       "<span class=\"Time-label\">[00:19<00:01, 0.96s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "power_curriculu [█████████████████████████████████████████████] 20/20 [00:19<00:01, 0.96s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">power_curriculum/combined</span>\n",
       "<progress style=\"width:45ex\" max=\"60\" value=\"60\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">60/60</span>\n",
       "<span class=\"Time-label\">[01:13<00:01, 1.21s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "power_curriculu [█████████████████████████████████████████████] 60/60 [01:13<00:01, 1.21s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">power_curriculum_2/color</span>\n",
       "<progress style=\"width:45ex\" max=\"20\" value=\"20\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">20/20</span>\n",
       "<span class=\"Time-label\">[00:48<00:03, 2.41s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "power_curriculu [█████████████████████████████████████████████] 20/20 [00:48<00:03, 2.41s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">power_curriculum_2/shape</span>\n",
       "<progress style=\"width:45ex\" max=\"20\" value=\"20\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">20/20</span>\n",
       "<span class=\"Time-label\">[00:24<00:01, 1.20s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "power_curriculu [█████████████████████████████████████████████] 20/20 [00:24<00:01, 1.20s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">power_curriculum_2/texture</span>\n",
       "<progress style=\"width:45ex\" max=\"20\" value=\"20\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">20/20</span>\n",
       "<span class=\"Time-label\">[00:34<00:01, 1.68s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "power_curriculu [█████████████████████████████████████████████] 20/20 [00:34<00:01, 1.68s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">power_curriculum_2/combined</span>\n",
       "<progress style=\"width:45ex\" max=\"60\" value=\"60\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">60/60</span>\n",
       "<span class=\"Time-label\">[01:27<00:01, 1.45s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "power_curriculu [█████████████████████████████████████████████] 60/60 [01:27<00:01, 1.45s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'epochs_to_completion' in cache:\n",
    "    epochs_to_completion = cache['epochs_to_completion']\n",
    "    \n",
    "else:\n",
    "    epochs_to_completion = dict()\n",
    "    \n",
    "    for name, path, num_runs, ignore_runs, split_runs_by_dimension in EPOCHS_TO_COMPLETION_SETS:\n",
    "        runs = analysis.load_runs(num_runs, path, split_runs_by_dimension=split_runs_by_dimension)\n",
    "        raw_analyses_dict = {}\n",
    "        log_analyses_dict = {}\n",
    "        trial_analyses_dict = {}\n",
    "        log_trial_analyses_dict = {}\n",
    "        last_tasks_dict = {}\n",
    "        \n",
    "        if split_runs_by_dimension:\n",
    "            for run_set, dimension_name in zip(runs, analysis.CONDITION_ANALYSES_FIELDS):\n",
    "                results, log_results, trial_results, log_trial_results, last_tasks = analysis.epochs_to_task_completions(run_set, name=name, ignore_runs=ignore_runs, ipb_desc=f'{name}/{dimension_name}') \n",
    "                raw_analyses_dict[dimension_name] = results\n",
    "                log_analyses_dict[dimension_name] = log_results\n",
    "                trial_analyses_dict[dimension_name] = trial_results\n",
    "                log_trial_analyses_dict[dimension_name] = log_trial_results\n",
    "                last_tasks_dict[dimension_name] = last_tasks\n",
    "                \n",
    "        else:\n",
    "            results, log_results, trial_results, log_trial_results, last_tasks = analysis.epochs_to_task_completions(runs.combined, name=name, ignore_runs=ignore_runs, ipb_desc=f'{name}') \n",
    "            raw_analyses_dict[analysis.COMBINED] = results\n",
    "            log_analyses_dict[analysis.COMBINED] = log_results\n",
    "            trial_analyses_dict[dimension_name] = trial_results\n",
    "            log_trial_analyses_dict[dimension_name] = log_trial_results\n",
    "            last_tasks_dict[dimension_name] = last_tasks\n",
    "            \n",
    "        raw_analyses_set = analysis.ConditionAnalysesSet(**raw_analyses_dict)\n",
    "        log_analyses_set = analysis.ConditionAnalysesSet(**log_analyses_dict)\n",
    "        trial_analyses_set = analysis.ConditionAnalysesSet(**trial_analyses_dict)\n",
    "        trial_log_analyses_set = analysis.ConditionAnalysesSet(**log_trial_analyses_dict)\n",
    "        last_tasks_analyses_set = analysis.ConditionAnalysesSet(**last_tasks_dict)\n",
    "        \n",
    "        epochs_to_completion[name] = dict(raw=raw_analyses_set, log=log_analyses_set, \n",
    "                                          raw_trial=trial_analyses_set, log_trial=trial_log_analyses_set,\n",
    "                                          last_tasks=last_tasks_analyses_set)\n",
    "        \n",
    "    cache = analysis.refresh_cache(dict(epochs_to_completion=epochs_to_completion)) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded runs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:0; max-width:15ex; vertical-align:middle; text-align:right\"></span>\n",
       "<progress style=\"width:60ex\" max=\"30\" value=\"30\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">30/30</span>\n",
       "<span class=\"Time-label\">[00:38<00:05, 1.27s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " [████████████████████████████████████████████████████████████] 30/30 [00:38<00:05, 1.27s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 10) (10,)\n"
     ]
    }
   ],
   "source": [
    "runs = analysis.load_runs(20, 'meta-learning-scaling/baseline-curriculum-balanced-batches')\n",
    "print('Loaded runs')\n",
    "\n",
    "# ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "#                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "ignore_runs = set()\n",
    "\n",
    "raw_analyses_dict = {}\n",
    "log_analyses_dict = {}\n",
    "start_index = 3\n",
    "for run_set, dimension_name in zip(runs[start_index:], \n",
    "                                   analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "    results, log_results = analysis.epochs_to_taks_completions(run_set, name='ratio', ignore_runs=ignore_runs) \n",
    "    raw_analyses_dict[dimension_name] = results\n",
    "    log_analyses_dict[dimension_name] = log_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_finished_succesfully(run, samples=5000):\n",
    "    df = run.history(pandas=True, samples=samples)\n",
    "    test_acc_column_names = [f'Test Accuracy, Query #{i + 1}' for i in range(10)]\n",
    "    if not all([col in df.columns for col in test_acc_column_names]):\n",
    "        return False\n",
    "    \n",
    "    last_epoch_accuracies = [df[col].iloc[-1] for col in test_acc_column_names]\n",
    "    return np.all(np.array(last_epoch_accuracies) >= 0.95)\n",
    "\n",
    "\n",
    "def condition_finished_succesfully(runs, num_runs=20, split_runs_by_dimension=True):\n",
    "    if isinstance(runs, analysis.ConditionAnalysesSet):\n",
    "        runs = runs.combined\n",
    "        \n",
    "    if isinstance(runs, str):\n",
    "        runs = analysis.load_runs(num_runs, runs, split_runs_by_dimension=split_runs_by_dimension).combined\n",
    "        \n",
    "    failed_runs = []\n",
    "    running = []\n",
    "    for run in ipb(runs, desc='Runs'):\n",
    "        run_id = run.config['dataset_random_seed']\n",
    "        if run.state == 'running':\n",
    "            print(f'Run {run.name} is still running')\n",
    "            running.append(run_id)\n",
    "            continue\n",
    "            \n",
    "        if not run_finished_succesfully(run):\n",
    "            print(f'Run {run.name} failed')\n",
    "            failed_runs.append(run_id)\n",
    "            \n",
    "    if len(running) > 0:\n",
    "        print(f'{len(running)} runs are still running: {running}')\n",
    "            \n",
    "    if len(failed_runs) == 0:\n",
    "        print('All finished runs passed')\n",
    "    else:\n",
    "        print(f'{len(failed_runs)} runs failed: {failed_runs}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-learning-scaling/baseline-curriculum-balanced-batches\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">Runs</span>\n",
       "<progress style=\"width:45ex\" max=\"60\" value=\"60\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">60/60</span>\n",
       "<span class=\"Time-label\">[00:46<00:02, 0.77s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "           Runs [█████████████████████████████████████████████] 60/60 [00:46<00:02, 0.77s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All finished runs passed\n",
      "meta-learning-scaling/curriculum-ratio-1-5-balanced-batches\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">Runs</span>\n",
       "<progress style=\"width:45ex\" max=\"60\" value=\"60\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">60/60</span>\n",
       "<span class=\"Time-label\">[00:55<00:00, 0.91s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "           Runs [█████████████████████████████████████████████] 60/60 [00:55<00:00, 0.91s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All finished runs passed\n",
      "meta-learning-scaling/power-curriculum-default-alpha-balanced-batches\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">Runs</span>\n",
       "<progress style=\"width:45ex\" max=\"60\" value=\"60\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">60/60</span>\n",
       "<span class=\"Time-label\">[00:48<00:01, 0.81s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "           Runs [█████████████████████████████████████████████] 60/60 [00:48<00:01, 0.81s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All finished runs passed\n",
      "meta-learning-scaling/power-curriculum-alpha-2-balanced-batches\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">Runs</span>\n",
       "<progress style=\"width:45ex\" max=\"60\" value=\"60\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">60/60</span>\n",
       "<span class=\"Time-label\">[01:39<00:01, 1.65s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "           Runs [█████████████████████████████████████████████] 60/60 [01:39<00:01, 1.65s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All finished runs passed\n"
     ]
    }
   ],
   "source": [
    "curriculum_urls = (\n",
    "    'meta-learning-scaling/baseline-curriculum-balanced-batches', \n",
    "    'meta-learning-scaling/curriculum-ratio-1-5-balanced-batches',\n",
    "    'meta-learning-scaling/power-curriculum-default-alpha-balanced-batches',\n",
    "    'meta-learning-scaling/power-curriculum-alpha-2-balanced-batches',\n",
    ")\n",
    "\n",
    "for url in curriculum_urls:\n",
    "    print(url)\n",
    "    condition_finished_succesfully(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_lengths_for_runs(runs, split_runs_by_dimension=True):\n",
    "    if isinstance(runs, analysis.ConditionAnalysesSet):\n",
    "        runs = runs.combined\n",
    "        \n",
    "    if isinstance(runs, str):\n",
    "        runs = analysis.load_runs(num_runs, runs, split_runs_by_dimension=split_runs_by_dimension).combined\n",
    "        \n",
    "    lengths = {r.config['latin_square_index']: r.lastHistoryStep for r in runs}\n",
    "    values = np.array(list(lengths.values()))\n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values)\n",
    "    \n",
    "    normalized_lengths = {run_id: (lengths[run_id] - mean) / std for run_id in lengths}\n",
    "    \n",
    "    return lengths, normalized_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline\n",
      "ratio_curriculum\n",
      "ratio_curriculum_1_5\n",
      "power_curriculum\n",
      "power_curriculum_2\n"
     ]
    }
   ],
   "source": [
    "LENGTH_BY_TASK_ORDER_SETS = (\n",
    "    # name, URI, num_runs, ignore_runs, split_by_condition\n",
    "    ('baseline', 'meta-learning-scaling/sequential-benchmark-baseline', 60, None, True),\n",
    "    ('ratio_curriculum', 'meta-learning-scaling/baseline-curriculum-balanced-batches', 20, None, True),\n",
    "    ('ratio_curriculum_1_5', 'meta-learning-scaling/curriculum-ratio-1-5-balanced-batches', 20, None, True),\n",
    "    ('power_curriculum', 'meta-learning-scaling/power-curriculum-default-alpha-balanced-batches', 20, None, True),\n",
    "    ('power_curriculum_2', 'meta-learning-scaling/power-curriculum-alpha-2-balanced-batches', 20, None, True),\n",
    ")\n",
    "\n",
    "\n",
    "if 'length_by_task_order' in cache:\n",
    "    length_by_task_order = cache['length_by_task_order']\n",
    "    \n",
    "else:\n",
    "    length_by_task_order = dict()\n",
    "    \n",
    "    for name, path, num_runs, ignore_runs, split_runs_by_dimension in LENGTH_BY_TASK_ORDER_SETS:\n",
    "        print(name)\n",
    "        runs = analysis.load_runs(num_runs, path, split_runs_by_dimension=split_runs_by_dimension)\n",
    "        raw_lengths_dict = {}\n",
    "        normalized_lengths_dict = {}\n",
    "        \n",
    "        if split_runs_by_dimension:\n",
    "            for run_set, dimension_name in zip(runs, analysis.CONDITION_ANALYSES_FIELDS):\n",
    "                raw_lengths, normalized_lengths = normalized_lengths_for_runs(run_set)\n",
    "                raw_lengths_dict[dimension_name] = raw_lengths\n",
    "                normalized_lengths_dict[dimension_name] = normalized_lengths\n",
    "                \n",
    "        else:\n",
    "            raw_lengths, normalized_lengths = normalized_lengths_for_runs(runs.combined)\n",
    "            raw_lengths_dict[dimension_name] = raw_lengths\n",
    "            normalized_lengths_dict[dimension_name] = normalized_lengths\n",
    "           \n",
    "        raw_lengths_set = analysis.ConditionAnalysesSet(**raw_lengths_dict)\n",
    "        normalized_lengths_set = analysis.ConditionAnalysesSet(**normalized_lengths_dict)\n",
    "        \n",
    "        length_by_task_order[name] = dict(raw=raw_lengths_set, normalized=normalized_lengths_set)\n",
    "        \n",
    "    cache = analysis.refresh_cache(dict(length_by_task_order=length_by_task_order)) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = analysis.load_runs(20, 'meta-learning-scaling/baseline-curriculum-balanced-batches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = runs.combined[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.0005,\n",
       " 'loss': 'CE',\n",
       " 'decay': 0.0001,\n",
       " 'epochs': 10000,\n",
       " 'batch_size': 1500,\n",
       " 'query_order': [29, 24, 23, 28, 21, 25, 26, 22, 27, 20],\n",
       " 'balanced_batches': True,\n",
       " 'test_coreset_size': 5000,\n",
       " 'accuracy_threshold': 0.95,\n",
       " 'latin_square_index': 3007,\n",
       " 'benchmark_dimension': 2,\n",
       " 'dataset_random_seed': 3007,\n",
       " 'latest_task_num_examples': 22500,\n",
       " 'latin_square_random_seed': 3000,\n",
       " 'previous_tasks_example_ratio': 1.25}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kth_diag_indices(a, k):\n",
    "    rows, cols = np.diag_indices_from(a)\n",
    "    if k < 0:\n",
    "        return rows[-k:], cols[:k]\n",
    "    elif k > 0:\n",
    "        return rows[:-k], cols[k:]\n",
    "    else:\n",
    "        return rows, cols\n",
    "\n",
    "    \n",
    "a = np.arange(16).reshape(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  6, 11])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.zeros((3, 4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = kth_diag_indices(z, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "z[1, r, c] += np.diag(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  2.,  0.,  0.],\n",
       "        [ 0.,  0., 12.,  0.],\n",
       "        [ 0.,  0.,  0., 22.],\n",
       "        [ 0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = r.history(pandas=True, samples=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([20, 70, 129, 147, 176, 209, 275, 343, 375, 409],\n",
       " [1, 2, 2, 1, 1, 1, 2, 2, 1, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = test_df\n",
    "first_row_blank = int(np.isnan(df['Test Accuracy'][0]))\n",
    "first_task_finished = df['Test Accuracy, Query #2'].first_valid_index() - first_row_blank \n",
    "\n",
    "task_finishes = [first_task_finished]\n",
    "last_tasks = [1]\n",
    "\n",
    "for current_task in range(2, 11):\n",
    "    current_task_start = df[f'Test Accuracy, Query #{current_task}'].first_valid_index()\n",
    "\n",
    "    if current_task == 10:\n",
    "        current_task_end = df.shape[0]\n",
    "    else:\n",
    "        current_task_end = df[f'Test Accuracy, Query #{current_task + 1}'].first_valid_index()\n",
    "\n",
    "    task_finishes.append(current_task_end)\n",
    "    \n",
    "    sub_df = test_df[[f'Test Accuracy, Query #{i + 1}' for i in range(current_task)]]\n",
    "    min_column_name = sub_df.iloc[current_task_end - 2].idxmin()\n",
    "    last_task_to_finish = int(min_column_name[min_column_name.find('#') + 1:])\n",
    "    last_tasks.append(last_task_to_finish)\n",
    "    \n",
    "task_finishes, last_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Test Accuracy, Query #2'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[[f'Test Accuracy, Query #{i}' for i in range(1, 5)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[i] * 7 for i in range(9)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1],\n",
       "       [2, 2, 2, 2, 2, 2, 2],\n",
       "       [3, 3, 3, 3, 3, 3, 3],\n",
       "       [4, 4, 4, 4, 4, 4, 4],\n",
       "       [5, 5, 5, 5, 5, 5, 5],\n",
       "       [6, 6, 6, 6, 6, 6, 6],\n",
       "       [7, 7, 7, 7, 7, 7, 7],\n",
       "       [8, 8, 8, 8, 8, 8, 8]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [np.random.randint(1, i + 1, (20,)) for i in range(1, 11)]\n",
    "r = np.stack(x).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([20]),\n",
       " array([15,  5]),\n",
       " array([6, 7, 7]),\n",
       " array([5, 5, 5, 5]),\n",
       " array([4, 3, 2, 7, 4]),\n",
       " array([3, 5, 2, 4, 3, 3]),\n",
       " array([5, 5, 2, 2, 2, 2, 2]),\n",
       " array([3, 2, 4, 3, 2, 1, 4, 1]),\n",
       " array([1, 3, 6, 2, 2, 0, 2, 1, 3]),\n",
       " array([2, 1, 3, 3, 4, 3, 1, 0, 1, 2])]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.bincount(r[:, i])[1:] for i in range(r.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing around with reading from the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../projects/')\n",
    "\n",
    "from metalearning import cnnmlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_LEARNING_RATE = 5e-4\n",
    "DEFAULT_WEIGHT_DECAY = 1e-4\n",
    "\n",
    "def create_task_conditional_model(multiplicative=True, additive=True, checkpoint_path=None, name=None):\n",
    "    mod_level = list(range(4))\n",
    "\n",
    "    model = cnnmlp.TaskConditionalCNNMLP(\n",
    "        mod_level=mod_level,\n",
    "        multiplicative_mod=multiplicative,\n",
    "        additive_mod=additive,\n",
    "        query_length=30,\n",
    "        conv_filter_sizes=(16, 32, 48, 64),\n",
    "        conv_output_size=4480,\n",
    "        mlp_layer_sizes=(512, 512, 512, 512),\n",
    "        lr=DEFAULT_LEARNING_RATE,\n",
    "        weight_decay=DEFAULT_WEIGHT_DECAY,\n",
    "        use_lr_scheduler=False,\n",
    "        conv_dropout=False,\n",
    "        mlp_dropout=False,\n",
    "        name=name)\n",
    "\n",
    "    if checkpoint_path is not None:\n",
    "        model.load_state(checkpoint_path)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = api.run('meta-learning-scaling/task-conditional-all-layers-additive-only/runs/49dq79wf')\n",
    "last_checkpoint_file = run.file(f'{run.name.replace(\"[0, 1, 2, 3]-\", \"\")}-query-9.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_checkpoint = last_checkpoint_file.download(replace=True, root='/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_task_conditional_model(multiplicative=False, \n",
    "                                      checkpoint_path='/tmp/' + last_checkpoint_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = range(4)\n",
    "additive_weights = [model.conv.additive_mod_layers[f'additive-{i}'].weight.detach().cpu().numpy() \n",
    "                                for i in layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = run.config['benchmark_dimension']\n",
    "dimension = 0\n",
    "[w[:, dimension * 10:(dimension + 1) * 10].sum() for w in additive_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv.additive_mod_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv.additive_mod_layers['additive-0'].weight.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[model.conv.additive_mod_layers['additive-0'].weight.detach().cpu().numpy()[:, i * 10:(i + 1) * 10].sum() for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ravel(model.conv.additive_mod_layers['additive-0'].weight.detach().cpu().numpy()[:, 20:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Out[44], bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
