{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/guydavidson/.netrc\n",
      "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!wandb login 9676e3cc95066e4865586082971f2653245f09b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.special import factorial\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from matplotlib import path as mpath\n",
    "\n",
    "import pickle\n",
    "import tabulate\n",
    "import wandb\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "from ipypb import ipb\n",
    "\n",
    "import meta_learning_data_analysis as analysis\n",
    "import meta_learning_analysis_plots as plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['six_replications_analyses', 'control_analyses', 'query_mod_replications', 'six_replications_updated_analyses', 'updated_control_analyses', 'query_mod_updated_analyses', 'forgetting_curves_raw_data', 'preliminary_maml_analyses', 'baseline_maml_comparison_analyses', 'maml_analyses', 'maml_alpha_0_analyses', 'maml_meta_test_analyses', 'balanced_batches_analyses', 'baseline_total_curve_analyses', 'control_total_curve_analyses', 'query_mod_total_curve_analyses', 'simultaneous_training_analyses', 'per_task_simultaneous_training_analyses', 'task_conditional_analyses', 'task_conditional_multiplicative_only_analyses', 'task_conditional_additive_only_analyses', 'task_conditional_weights', 'task_conditional_multiplicative_only_weights', 'task_conditional_additive_only_weights', 'forgetting_exp_decay_params', 'baseline_ratio_curriculum_analyses', 'baseline_power_curriculum_analyses', 'epochs_to_completion'])\n"
     ]
    }
   ],
   "source": [
    "cache = analysis.refresh_cache()\n",
    "print(cache.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# Baseline analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'six_replications_analyses' in cache:\n",
    "    six_replications_analyses = cache['six_replications_analyses']\n",
    "\n",
    "else:\n",
    "    six_replications_by_dimension_runs = analysis.load_runs(60)\n",
    "    print('Loaded runs')\n",
    "\n",
    "    six_reps_dict = {dimension_name:analysis.process_multiple_runs(run_set) \n",
    "                     for run_set, dimension_name \n",
    "                     in zip(six_replications_by_dimension_runs, analysis.CONDITION_ANALYSES_FIELDS)}\n",
    "    six_replications_analyses = analysis.ConditionAnalysesSet(**six_reps_dict)\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(six_replications_analyses=six_replications_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'six_replications_updated_analyses' in cache:\n",
    "#     six_replications_updated_analyses = cache['six_replications_updated_analyses']\n",
    "\n",
    "# else:\n",
    "six_replications_by_dimension_runs = analysis.load_runs(60)\n",
    "print('Loaded runs')\n",
    "\n",
    "# note: the equal accuracy field will come in as accuracy_drops\n",
    "updated_six_reps_dict = {}\n",
    "start_index = 0\n",
    "for run_set, dimension_name in zip(six_replications_by_dimension_runs[start_index:], \n",
    "                                   analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "    updated_six_reps_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "        run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size) \n",
    "\n",
    "# combined_analysis = analysis.process_multiple_runs(\n",
    "#     six_replications_by_dimension_runs[3], \n",
    "#     parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size)\n",
    "\n",
    "six_replications_updated_analyses = analysis.ConditionAnalysesSet(**updated_six_reps_dict)\n",
    "\n",
    "cache = analysis.refresh_cache(dict(six_replications_updated_analyses=six_replications_updated_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'baseline_total_curve_analyses' in cache:\n",
    "#     baseline_total_curve_analyses = cache['baseline_total_curve_analyses']\n",
    "\n",
    "# else:\n",
    "six_replications_by_dimension_runs = analysis.load_runs(60)\n",
    "print('Loaded runs')\n",
    "\n",
    "analyses_per_dimension = {}\n",
    "\n",
    "for run_set, dimension_name in zip(six_replications_by_dimension_runs, analysis.CONDITION_ANALYSES_FIELDS):\n",
    "    print(f'Starting {dimension_name}')\n",
    "    total_curve_raw, total_curve_mean, total_curve_std, total_curve_sem = \\\n",
    "        analysis.process_multiple_runs_total_task_training_curves(run_set)\n",
    "\n",
    "    analyses_per_dimension[dimension_name] = analysis.TotalCurveResults(raw=total_curve_raw,\n",
    "                                                                        mean=total_curve_mean, \n",
    "                                                                        std=total_curve_std, \n",
    "                                                                        sem=total_curve_sem)\n",
    "\n",
    "total_curve_analyses = analysis.ConditionAnalysesSet(**analyses_per_dimension)\n",
    "cache = analysis.refresh_cache(dict(baseline_total_curve_analyses=total_curve_analyses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# Control analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'control_analyses' in cache:\n",
    "    control_analyses = cache['control_analyses']\n",
    "\n",
    "else:\n",
    "    control_runs = analysis.load_runs(150, 'meta-learning-scaling/sequential-benchmark-control', False)\n",
    "    print(f'Loaded runs')\n",
    "    control_analyses = analysis.ConditionAnalysesSet(combined=analysis.process_multiple_runs(control_runs.combined))\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(control_analyses=control_analyses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'six_replications_updated_analyses' in cache:\n",
    "#     six_replications_updated_analyses = cache['six_replications_updated_analyses']\n",
    "\n",
    "# else:\n",
    "control_runs = analysis.load_runs(150, 'meta-learning-scaling/sequential-benchmark-control', False)\n",
    "print('Loaded runs')\n",
    "\n",
    "updated_control_analyses = analysis.ConditionAnalysesSet(\n",
    "    combined=analysis.process_multiple_runs(control_runs.combined, \n",
    "                                            parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size))\n",
    "\n",
    "\n",
    "cache = analysis.refresh_cache(dict(updated_control_analyses=updated_control_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'control_total_curve_analyses' in cache:\n",
    "    control_total_curve_analyses = cache['control_total_curve_analyses']\n",
    "\n",
    "else:\n",
    "    control_runs = analysis.load_runs(150, 'meta-learning-scaling/sequential-benchmark-control', False)\n",
    "    print('Loaded runs')\n",
    "    \n",
    "    total_curve_raw, total_curve_mean, total_curve_std, total_curve_sem = \\\n",
    "            analysis.process_multiple_runs_total_task_training_curves(control_runs.combined)\n",
    "    \n",
    "    control_total_curve_analyses = analysis.ConditionAnalysesSet(\n",
    "        combined=analysis.TotalCurveResults(raw=total_curve_raw,\n",
    "                                            mean=total_curve_mean,\n",
    "                                            std=total_curve_std,\n",
    "                                            sem=total_curve_sem))\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(control_total_curve_analyses=control_total_curve_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = analysis.refresh_cache(dict(control_analyses=control_analyses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the number of examples by dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = (1000, 520000)\n",
    "\n",
    "plots.plot_processed_results(first_replication_analyses.color.examples, 'Color 10-run average', ylim)\n",
    "plots.plot_processed_results(first_replication_analyses.shape.examples, 'Shape 10-run average', ylim)\n",
    "plots.plot_processed_results(first_replication_analyses.texture.examples, 'Material 10-run average', ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = (1000, 700000)\n",
    "\n",
    "plots.plot_processed_results(six_replications_analyses.color.examples, 'Color 60-run average', ylim)\n",
    "plots.plot_processed_results(six_replications_analyses.shape.examples, 'Shape 60-run average', ylim)\n",
    "plots.plot_processed_results(six_replications_analyses.texture.examples, 'Material 60-run average', ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the log of the number of examples to criterion, in each dimension, with error bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the combined results over all 180 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = (7.75, 13.25)\n",
    "\n",
    "plots.plot_processed_results(six_replications_analyses.combined.log_examples, 'Combined 180-run average', \n",
    "                       ylim, log_x=(True, True), log_y=True, sem_n=180, shade_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the absolute accuracy after introducing a new task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = None\n",
    "\n",
    "plots.plot_processed_results(six_replications_analyses.color.accuracies, 'Color 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.shape.accuracies, 'Shape 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.texture.accuracies, 'Material 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.combined.accuracies, 'Combined 180-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=180, shade_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the accuracy drop after introducing a new task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = None\n",
    "\n",
    "plots.plot_processed_results(six_replications_analyses.color.accuracy_drops, 'Color 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.shape.accuracy_drops, 'Shape 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.texture.accuracy_drops, 'Material 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# Query-modulated analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'query_mod_replications' in cache:\n",
    "    query_mod_replications = cache['query_mod_replications']\n",
    "\n",
    "else:\n",
    "    query_mod_runs = analysis.query_modulated_runs_by_dimension(30)\n",
    "    query_mod_replications = {}\n",
    "\n",
    "    ignore_runs = [] # ('at6pkicv', )\n",
    "    for mod_level in query_mod_runs:\n",
    "        mod_level_runs = query_mod_runs[mod_level]\n",
    "\n",
    "        mod_level_dict = {dimension_name: analysis.process_multiple_runs(mod_level_runs[i], ignore_runs=ignore_runs) \n",
    "                          for i, dimension_name \n",
    "                          in enumerate(analysis.CONDITION_ANALYSES_FIELDS)}\n",
    "\n",
    "        query_mod_replications[mod_level] = analysis.ConditionAnalysesSet(**mod_level_dict)\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(query_mod_replications=query_mod_replications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = analysis.refresh_cache(dict(query_mod_updated_analyses=query_mod_replications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'six_replications_updated_analyses' in cache:\n",
    "#     six_replications_updated_analyses = cache['six_replications_updated_analyses']\n",
    "\n",
    "# else:\n",
    "query_mod_runs = analysis.query_modulated_runs_by_dimension(30)\n",
    "print('Loaded runs')\n",
    "\n",
    "# note: the equal accuracy field will come in as accuracy_drops\n",
    "query_mod_replications = {}\n",
    "mod_levels = list(query_mod_runs.keys())\n",
    "start_index = 0\n",
    "\n",
    "for mod_level in mod_levels[start_index:]:\n",
    "    print(f'Starting mod level {mod_level}')\n",
    "    mod_level_runs = query_mod_runs[mod_level]\n",
    "\n",
    "    mod_level_dict = {dimension_name: analysis.process_multiple_runs(mod_level_runs[i], \n",
    "                                                                     parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size) \n",
    "                      for i, dimension_name \n",
    "                      in enumerate(analysis.CONDITION_ANALYSES_FIELDS)}\n",
    "\n",
    "    query_mod_replications[mod_level] = analysis.ConditionAnalysesSet(**mod_level_dict)\n",
    "\n",
    "\n",
    "cache = analysis.refresh_cache(dict(query_mod_updated_analyses=query_mod_replications))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = analysis.refresh_cache(dict(query_mod_replications=query_mod_replications,\n",
    "                                    control_analyses=control_analyses,\n",
    "                                    six_replications_analyses=six_replications_analyses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'query_mod_total_curve_analyses' in cache:\n",
    "    query_mod_total_curve_analyses = cache['query_mod_total_curve_analyses']\n",
    "\n",
    "else:\n",
    "    query_mod_runs = analysis.query_modulated_runs_by_dimension(30)\n",
    "    print('Loaded runs')\n",
    "    \n",
    "    query_mod_total_curve_analyses = {}\n",
    "    mod_levels = list(query_mod_runs.keys())\n",
    "    start_index = 0\n",
    "    \n",
    "    for mod_level in mod_levels[start_index:]:\n",
    "        print(f'Starting mod level {mod_level}')\n",
    "        mod_level_runs = query_mod_runs[mod_level]\n",
    "\n",
    "        analyses_per_dimension = {}\n",
    "        \n",
    "        for run_set, dimension_name in zip(mod_level_runs, analysis.CONDITION_ANALYSES_FIELDS):\n",
    "            print(f'Starting {dimension_name}')\n",
    "            total_curve_raw, total_curve_mean, total_curve_std, total_curve_sem = \\\n",
    "                analysis.process_multiple_runs_total_task_training_curves(run_set)\n",
    "\n",
    "            analyses_per_dimension[dimension_name] = analysis.TotalCurveResults(raw=total_curve_raw,\n",
    "                                                                                mean=total_curve_mean, \n",
    "                                                                                std=total_curve_std, \n",
    "                                                                                sem=total_curve_sem)\n",
    "\n",
    "        query_mod_total_curve_analyses[mod_level] = analysis.ConditionAnalysesSet(**analyses_per_dimension)\n",
    "            \n",
    "    cache = analysis.refresh_cache(dict(query_mod_total_curve_analyses=query_mod_total_curve_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_mod_total_curve_analyses.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'maml_analyses' in cache:\n",
    "    maml_analyses = cache['maml_analyses']\n",
    "\n",
    "else:\n",
    "    maml_runs = analysis.load_runs(30, 'meta-learning-scaling/maml-sequential-benchmark')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "    #                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    maml_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(maml_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        maml_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    maml_analyses = analysis.ConditionAnalysesSet(**maml_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(maml_analyses=maml_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'six_replications_updated_analyses' in cache:\n",
    "#     six_replications_updated_analyses = cache['six_replications_updated_analyses']\n",
    "\n",
    "# else:\n",
    "maml_alpha_0_runs = analysis.load_runs(20, 'meta-learning-scaling/maml-alpha-0')\n",
    "print('Loaded runs')\n",
    "\n",
    "raise ValueError('This will not work yeet')\n",
    "\n",
    "# ignore_runs = set(['ac82mceh', '7kau3ypy', 'g9ujw7gg', 'avmcbnot'])\n",
    "ignore_runs = set()\n",
    "\n",
    "maml_alpha_0_analyses_dict = {}\n",
    "start_index = 0\n",
    "for run_set, dimension_name in zip(maml_alpha_0_runs[start_index:], \n",
    "                                   analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "    \n",
    "    maml_alpha_0_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "        run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "        ignore_runs=ignore_runs) \n",
    "\n",
    "maml_alpha_0_analyses = analysis.ConditionAnalysesSet(**maml_alpha_0_analyses_dict)\n",
    "cache = analysis.refresh_cache(dict(maml_alpha_0_analyses=maml_alpha_0_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'maml_meta_test_analyses' in cache:\n",
    "    maml_meta_test_analyses = cache['maml_meta_test_analyses']\n",
    "\n",
    "else:\n",
    "    maml_meta_test_runs = analysis.load_runs(30, 'meta-learning-scaling/maml-meta-test')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    maml_meta_test_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(maml_meta_test_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        maml_meta_test_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    maml_meta_test_analyses = analysis.ConditionAnalysesSet(**maml_meta_test_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(maml_meta_test_analyses=maml_meta_test_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'balanced_batches_analyses' in cache:\n",
    "    balanced_batches_analyses = cache['balanced_batches_analyses']\n",
    "\n",
    "else:\n",
    "    balanced_batches_runs = analysis.load_runs(30, 'meta-learning-scaling/balanced-batches-sequential-benchmark')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    balanced_batches_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(balanced_batches_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        balanced_batches_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    balanced_batches_analyses = analysis.ConditionAnalysesSet(**balanced_batches_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(balanced_batches_analyses=balanced_batches_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maml_comparison_run_ids = [1000, 1001, 2000, 2001, 2002, \n",
    "                           2003, 2004, 2005, 2006, 2007, \n",
    "                           2008, 2009, 3000, 3001, 3002, \n",
    "                           3003, 3004, 3005, 3006, 3007, 3008]\n",
    "\n",
    "maml_comparison_runs = analysis.load_runs(10, split_runs_by_dimension=False, valid_run_ids=set(maml_comparison_run_ids))\n",
    "print('Loaded runs')\n",
    "\n",
    "baseline_maml_comparison_analyses = analysis.ConditionAnalysesSet(\n",
    "    combined=analysis.process_multiple_runs(maml_comparison_runs.combined, \n",
    "                                            parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,))\n",
    "\n",
    "\n",
    "cache = analysis.refresh_cache(dict(baseline_maml_comparison_analyses=baseline_maml_comparison_analyses))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Simultaneous training in a dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'simultaneous_training_analyses' in cache:\n",
    "#     simultaneous_training_analyses = cache['simultaneous_training_analyses']\n",
    "\n",
    "# else:\n",
    "simultaneous_training_runs = analysis.load_runs(20, 'meta-learning-scaling/simultaneous-training')\n",
    "print('Loaded runs')\n",
    "\n",
    "# ignore_runs = set(['u3gk9oio'])\n",
    "ignore_runs = set()\n",
    "\n",
    "simultaneous_training_analyses_dict = {}\n",
    "per_task_simultaneous_training_analyses_dict = {}\n",
    "start_index = 0\n",
    "for run_set, dimension_name in zip(simultaneous_training_runs[start_index:], \n",
    "                                   analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "    stacked_results, all_results_mean, all_results_std, all_results_sem, per_task_results_mean, per_task_results_std, per_task_results_sem = analysis.process_multiple_runs_simultaneous_training(run_set, ignore_runs=ignore_runs)\n",
    "\n",
    "    simultaneous_training_analyses_dict[dimension_name] = analysis.TotalCurveResults(raw=stacked_results,\n",
    "                                                                                     mean=all_results_mean, \n",
    "                                                                                     std=all_results_std, \n",
    "                                                                                     sem=all_results_sem)\n",
    "\n",
    "    per_task_simultaneous_training_analyses_dict[dimension_name] = analysis.TotalCurveResults(raw=stacked_results,\n",
    "                                                                                              mean=per_task_results_mean, \n",
    "                                                                                              std=per_task_results_std, \n",
    "                                                                                              sem=per_task_results_sem)\n",
    "\n",
    "simultaneous_training_analyses = analysis.ConditionAnalysesSet(**simultaneous_training_analyses_dict)\n",
    "per_task_simultaneous_training_analyses = analysis.ConditionAnalysesSet(**per_task_simultaneous_training_analyses_dict)\n",
    "cache = analysis.refresh_cache(dict(simultaneous_training_analyses=simultaneous_training_analyses,\n",
    "                                    per_task_simultaneous_training_analyses=per_task_simultaneous_training_analyses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# New Task Modulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'task_conditional_analyses' in cache:\n",
    "    task_conditional_analyses = cache['task_conditional_analyses']\n",
    "\n",
    "else:\n",
    "    task_modulated_runs = analysis.load_runs(20, 'meta-learning-scaling/task-conditional-all-layers')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    # ignore_runs = set(['Task-conditional-[0, 1, 2, 3]-1017'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    task_modulated_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(task_modulated_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        task_modulated_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    task_conditional_analyses = analysis.ConditionAnalysesSet(**task_modulated_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(task_conditional_analyses=task_conditional_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'task_conditional_multiplicative_only_analyses' in cache:\n",
    "    task_conditional_multiplicative_only_analyses = cache['task_conditional_multiplicative_only_analyses']\n",
    "\n",
    "else:\n",
    "    task_modulated_runs = analysis.load_runs(20, 'meta-learning-scaling/task-conditional-all-layers-multiplicative-only')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['Task-conditional-multiplicative-[0, 1, 2, 3]-1006',\n",
    "    #                   'Task-conditional-multiplicative-[0, 1, 2, 3]-1005'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    task_modulated_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(task_modulated_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        task_modulated_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    task_conditional_multiplicative_only_analyses = analysis.ConditionAnalysesSet(**task_modulated_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(task_conditional_multiplicative_only_analyses=task_conditional_multiplicative_only_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'task_conditional_additive_only_analyses' in cache:\n",
    "    task_conditional_additive_only_analyses = cache['task_conditional_additive_only_analyses']\n",
    "\n",
    "else:\n",
    "    task_modulated_runs = analyses_caches/alysis.load_runs(20, 'meta-learning-scaling/task-conditional-all-layers-additive-only')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    task_modulated_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(task_modulated_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        task_modulated_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    task_conditional_additive_only_analyses = analysis.ConditionAnalysesSet(**task_modulated_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(task_conditional_additive_only_analyses=task_conditional_additive_only_analyses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the task-conditional weights en masse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'task_conditional_weights' in cache:\n",
    "    task_conditional_weights = cache['task_conditional_weights']\n",
    "\n",
    "else:\n",
    "    task_modulated_runs = analysis.load_runs(20, 'meta-learning-scaling/task-conditional-all-layers')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    ignore_runs = set(['Task-conditional-[0, 1, 2, 3]-1013',\n",
    "                      'Task-conditional-[0, 1, 2, 3]-1011',\n",
    "                      'Task-conditional-[0, 1, 2, 3]-1001'])\n",
    "#     ignore_runs = set()\n",
    "\n",
    "    task_conditional_weights = analysis.parse_task_conditional_weights(task_modulated_runs.combined,\n",
    "                                                                      additive=True,\n",
    "                                                                      multiplicative=True,\n",
    "                                                                      ignore_runs=ignore_runs)\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(task_conditional_weights=task_conditional_weights))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'task_conditional_multiplicative_only_weights' in cache:\n",
    "    task_conditional_multiplicative_only_weights = cache['task_conditional_multiplicative_only_weights']\n",
    "\n",
    "else:\n",
    "    task_modulated_runs = analysis.load_runs(20, 'meta-learning-scaling/task-conditional-all-layers-multiplicative-only')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['Task-conditional-multiplicative-[0, 1, 2, 3]-1006',\n",
    "    #                   'Task-conditional-multiplicative-[0, 1, 2, 3]-1005'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    task_conditional_multiplicative_only_weights = analysis.parse_task_conditional_weights(task_modulated_runs.combined,\n",
    "                                                                      additive=False,\n",
    "                                                                      multiplicative=True,\n",
    "                                                                      ignore_runs=ignore_runs)\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(task_conditional_multiplicative_only_weights=task_conditional_multiplicative_only_weights))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'task_conditional_additive_only_weights' in cache:\n",
    "    task_conditional_additive_only_weights = cache['task_conditional_additive_only_weights']\n",
    "\n",
    "else:\n",
    "    task_modulated_runs = analysis.load_runs(20, 'meta-learning-scaling/task-conditional-all-layers-additive-only')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    task_conditional_additive_only_weights = analysis.parse_task_conditional_weights(task_modulated_runs.combined,\n",
    "                                                                      additive=True,\n",
    "                                                                      multiplicative=False,\n",
    "                                                                      ignore_runs=ignore_runs)\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(task_conditional_additive_only_weights=task_conditional_additive_only_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "# The curriculum experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'baseline_ratio_curriculum_analyses' in cache:\n",
    "    baseline_ratio_curriculum_analyses = cache['baseline_ratio_curriculum_analyses']\n",
    "\n",
    "else:\n",
    "    runs = analysis.load_runs(20, 'meta-learning-scaling/baseline-curriculum-balanced-batches')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "    #                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    baseline_ratio_curriculum_analyses = analysis.ConditionAnalysesSet(**analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(baseline_ratio_curriculum_analyses=baseline_ratio_curriculum_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ratio_curriculum_1_5_analyses' in cache:\n",
    "    ratio_curriculum_1_5_analyses = cache['ratio_curriculum_1_5_analyses']\n",
    "\n",
    "else:\n",
    "    runs = analysis.load_runs(20, 'meta-learning-scaling/curriculum-ratio-1-5-balanced-batches')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "    #                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    ratio_curriculum_1_5_analyses = analysis.ConditionAnalysesSet(**analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(ratio_curriculum_1_5_analyses=ratio_curriculum_1_5_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'baseline_power_curriculum_analyses' in cache:\n",
    "    baseline_power_curriculum_analyses = cache['baseline_power_curriculum_analyses']\n",
    "\n",
    "else:\n",
    "    runs = analysis.load_runs(20, 'meta-learning-scaling/power-curriculum-default-alpha-balanced-batches')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "    #                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    baseline_power_curriculum_analyses = analysis.ConditionAnalysesSet(**analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(baseline_power_curriculum_analyses=baseline_power_curriculum_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'power_curriculum_2_analyses' in cache:\n",
    "    power_curriculum_2_analyses = cache['power_curriculum_2_analyses']\n",
    "\n",
    "else:\n",
    "    runs = analysis.load_runs(20, 'meta-learning-scaling/power-curriculum-alpha-2-balanced-batches')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "    #                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    power_curriculum_2_analyses = analysis.ConditionAnalysesSet(**analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(power_curriculum_2_analyses=power_curriculum_2_analyses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the epochs to completion in each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_TO_COMPLETION_SETS = (\n",
    "    # name, URI, num_runs, ignore_runs, split_by_condition\n",
    "    ('baseline', 'meta-learning-scaling/sequential-benchmark-baseline', 60, None, True),\n",
    "    ('heterogeneous', 'meta-learning-scaling/sequential-benchmark-control', 150, None, False),\n",
    "    # (),  # TODO: query-modulated, too?\n",
    "#     ('ratio_curriculum', 'meta-learning-scaling/baseline-curriculum-balanced-batches', 20, None, True),\n",
    "#     ('ratio_curriculum_1_5', 'meta-learning-scaling/curriculum-ratio-1-5-balanced-batches', 20, None, True),\n",
    "#     ('power_curriculum', 'meta-learning-scaling/power-curriculum-default-alpha-balanced-batches', 20, None, True),\n",
    "#     ('power_curriculum_2', 'meta-learning-scaling/power-curriculum-alpha-2-balanced-batches', 20, None, True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'epochs_to_completion' in cache:\n",
    "    epochs_to_completion = cache['epochs_to_completion']\n",
    "    \n",
    "else:\n",
    "    epochs_to_completion = dict()\n",
    "    \n",
    "    for name, path, num_runs, ignore_runs, split_runs_by_dimension in EPOCHS_TO_COMPLETION_SETS:\n",
    "        runs = analysis.load_runs(num_runs, path, split_runs_by_dimension=split_runs_by_dimension)\n",
    "        raw_analyses_dict = {}\n",
    "        log_analyses_dict = {}\n",
    "        \n",
    "        if split_runs_by_dimension:\n",
    "            for run_set, dimension_name in zip(runs, analysis.CONDITION_ANALYSES_FIELDS):\n",
    "                results, log_results = analysis.epochs_to_taks_completions(run_set, ignore_runs=ignore_runs, ipb_desc=f'{name}/{dimension_name}') \n",
    "                raw_analyses_dict[dimension_name] = results\n",
    "                log_analyses_dict[dimension_name] = log_results\n",
    "                \n",
    "        else:\n",
    "            results, log_results = analysis.epochs_to_taks_completions(runs.combined, ignore_runs=ignore_runs, ipb_desc=f'{name}') \n",
    "            raw_analyses_dict[analysis.COMBINED] = results\n",
    "            log_analyses_dict[analysis.COMBINED] = log_results\n",
    "            \n",
    "        raw_analyses_set = analysis.ConditionAnalysesSet(**raw_analyses_dict)\n",
    "        log_analyses_set = analysis.ConditionAnalysesSet(**log_analyses_dict)\n",
    "        \n",
    "        epochs_to_completion[name] = dict(raw=raw_analyses_set, log=log_analyses_set)\n",
    "        \n",
    "    cache = analysis.refresh_cache(dict(epochs_to_completion=epochs_to_completion)) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_to_completion.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = analysis.load_runs(20, 'meta-learning-scaling/baseline-curriculum-balanced-batches')\n",
    "print('Loaded runs')\n",
    "\n",
    "# ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "#                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "ignore_runs = set()\n",
    "\n",
    "raw_analyses_dict = {}\n",
    "log_analyses_dict = {}\n",
    "start_index = 0\n",
    "for run_set, dimension_name in zip(runs[start_index:], \n",
    "                                   analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "    results, log_results = analysis.epochs_to_taks_completions(run_set, ignore_runs=ignore_runs) \n",
    "    raw_analyses_dict[dimension_name] = results\n",
    "    log_analyses_dict[dimension_name] = log_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_finished_succesfully(run, samples=5000):\n",
    "    df = run.history(pandas=True, samples=samples)\n",
    "    test_acc_column_names = [f'Test Accuracy, Query #{i + 1}' for i in range(10)]\n",
    "    if not all([col in df.columns for col in test_acc_column_names]):\n",
    "        return False\n",
    "    \n",
    "    last_epoch_accuracies = [df[col].iloc[-1] for col in test_acc_column_names]\n",
    "    return np.all(np.array(last_epoch_accuracies) >= 0.95)\n",
    "\n",
    "\n",
    "def condition_finished_succesfully(runs, num_runs=20, split_runs_by_dimension=True):\n",
    "    if isinstance(runs, analysis.ConditionAnalysesSet):\n",
    "        runs = runs.combined\n",
    "        \n",
    "    if isinstance(runs, str):\n",
    "        runs = analysis.load_runs(num_runs, runs, split_runs_by_dimension=split_runs_by_dimension).combined\n",
    "        \n",
    "    failed_runs = []\n",
    "    running = []\n",
    "    for run in ipb(runs, desc='Runs'):\n",
    "        run_id = run.config['dataset_random_seed']\n",
    "        if run.state == 'running':\n",
    "            print(f'Run {run.name} is still running')\n",
    "            running.append(run_id)\n",
    "            continue\n",
    "            \n",
    "        if not run_finished_succesfully(run):\n",
    "            print(f'Run {run.name} failed')\n",
    "            failed_runs.append(run_id)\n",
    "            \n",
    "    if len(running) > 0:\n",
    "        print(f'{len(running)} runs are still running: {running}')\n",
    "            \n",
    "    if len(failed_runs) == 0:\n",
    "        print('All finished runs passed')\n",
    "    else:\n",
    "        print(f'{len(failed_runs)} runs failed: {failed_runs}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-learning-scaling/baseline-curriculum-balanced-batches\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">Runs</span>\n",
       "<progress style=\"width:45ex\" max=\"60\" value=\"60\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">60/60</span>\n",
       "<span class=\"Time-label\">[00:51<00:01, 0.86s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "           Runs [█████████████████████████████████████████████] 60/60 [00:51<00:01, 0.86s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All finished runs passed\n",
      "meta-learning-scaling/curriculum-ratio-1-5-balanced-batches\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">Runs</span>\n",
       "<progress style=\"width:45ex\" max=\"61\" value=\"61\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">61/61</span>\n",
       "<span class=\"Time-label\">[01:19<00:09, 1.30s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "           Runs [█████████████████████████████████████████████] 61/61 [01:19<00:09, 1.30s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run curriculum-ratio-1-5-balanced-batches-1001 is still running\n",
      "Run curriculum-ratio-1-5-balanced-batches-1001 is still running\n",
      "Run curriculum-ratio-1-5-balanced-batches-1014 failed\n",
      "Run curriculum-ratio-1-5-balanced-batches-1000 failed\n",
      "2 runs are still running: [1001, 1001]\n",
      "2 runs failed: [1014, 1000]\n",
      "meta-learning-scaling/power-curriculum-default-alpha-balanced-batches\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">Runs</span>\n",
       "<progress style=\"width:45ex\" max=\"60\" value=\"60\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">60/60</span>\n",
       "<span class=\"Time-label\">[00:47<00:03, 0.78s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "           Runs [█████████████████████████████████████████████] 60/60 [00:47<00:03, 0.78s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All finished runs passed\n",
      "meta-learning-scaling/power-curriculum-alpha-2-balanced-batches\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">Runs</span>\n",
       "<progress style=\"width:45ex\" max=\"62\" value=\"62\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">62/62</span>\n",
       "<span class=\"Time-label\">[02:10<00:08, 2.09s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "           Runs [█████████████████████████████████████████████] 62/62 [02:10<00:08, 2.09s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run power-curriculum-alpha-2-balanced-batches-1012 failed\n",
      "Run power-curriculum-alpha-2-balanced-batches-1012 failed\n",
      "Run power-curriculum-alpha-2-balanced-batches-1014 failed\n",
      "Run power-curriculum-alpha-2-balanced-batches-1001 failed\n",
      "Run power-curriculum-alpha-2-balanced-batches-1000 failed\n",
      "5 runs failed: [1012, 1012, 1014, 1001, 1000]\n"
     ]
    }
   ],
   "source": [
    "curriculum_urls = (\n",
    "    'meta-learning-scaling/baseline-curriculum-balanced-batches', \n",
    "    'meta-learning-scaling/curriculum-ratio-1-5-balanced-batches',\n",
    "    'meta-learning-scaling/power-curriculum-default-alpha-balanced-batches',\n",
    "    'meta-learning-scaling/power-curriculum-alpha-2-balanced-batches',\n",
    ")\n",
    "\n",
    "for url in curriculum_urls:\n",
    "    print(url)\n",
    "    condition_finished_succesfully(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(curriculum_urls[0], str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = runs.combined[0].history(pandas=True, samples=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Test Accuracy, Query #7'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Test Accuracy, Query #17' in test_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing around with reading from the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../projects/')\n",
    "\n",
    "from metalearning import cnnmlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_LEARNING_RATE = 5e-4\n",
    "DEFAULT_WEIGHT_DECAY = 1e-4\n",
    "\n",
    "def create_task_conditional_model(multiplicative=True, additive=True, checkpoint_path=None, name=None):\n",
    "    mod_level = list(range(4))\n",
    "\n",
    "    model = cnnmlp.TaskConditionalCNNMLP(\n",
    "        mod_level=mod_level,\n",
    "        multiplicative_mod=multiplicative,\n",
    "        additive_mod=additive,\n",
    "        query_length=30,\n",
    "        conv_filter_sizes=(16, 32, 48, 64),\n",
    "        conv_output_size=4480,\n",
    "        mlp_layer_sizes=(512, 512, 512, 512),\n",
    "        lr=DEFAULT_LEARNING_RATE,\n",
    "        weight_decay=DEFAULT_WEIGHT_DECAY,\n",
    "        use_lr_scheduler=False,\n",
    "        conv_dropout=False,\n",
    "        mlp_dropout=False,\n",
    "        name=name)\n",
    "\n",
    "    if checkpoint_path is not None:\n",
    "        model.load_state(checkpoint_path)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = api.run('meta-learning-scaling/task-conditional-all-layers-additive-only/runs/49dq79wf')\n",
    "last_checkpoint_file = run.file(f'{run.name.replace(\"[0, 1, 2, 3]-\", \"\")}-query-9.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_checkpoint = last_checkpoint_file.download(replace=True, root='/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_task_conditional_model(multiplicative=False, \n",
    "                                      checkpoint_path='/tmp/' + last_checkpoint_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = range(4)\n",
    "additive_weights = [model.conv.additive_mod_layers[f'additive-{i}'].weight.detach().cpu().numpy() \n",
    "                                for i in layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = run.config['benchmark_dimension']\n",
    "dimension = 0\n",
    "[w[:, dimension * 10:(dimension + 1) * 10].sum() for w in additive_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv.additive_mod_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv.additive_mod_layers['additive-0'].weight.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[model.conv.additive_mod_layers['additive-0'].weight.detach().cpu().numpy()[:, i * 10:(i + 1) * 10].sum() for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ravel(model.conv.additive_mod_layers['additive-0'].weight.detach().cpu().numpy()[:, 20:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Out[44], bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
