{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/guydavidson/.netrc\n",
      "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!wandb login 9676e3cc95066e4865586082971f2653245f09b4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.special import factorial\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import patches\n",
    "from matplotlib import path as mpath\n",
    "\n",
    "import pickle\n",
    "import tabulate\n",
    "import wandb\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "from ipypb import ipb\n",
    "\n",
    "import meta_learning_data_analysis as analysis\n",
    "import meta_learning_analysis_plots as plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['six_replications_analyses', 'control_analyses', 'query_mod_replications', 'six_replications_updated_analyses', 'updated_control_analyses', 'query_mod_updated_analyses', 'forgetting_curves_raw_data', 'preliminary_maml_analyses', 'baseline_maml_comparison_analyses', 'maml_analyses', 'maml_alpha_0_analyses', 'maml_meta_test_analyses', 'balanced_batches_analyses', 'baseline_total_curve_analyses', 'control_total_curve_analyses', 'query_mod_total_curve_analyses', 'simultaneous_training_analyses', 'per_task_simultaneous_training_analyses', 'task_conditional_analyses', 'task_conditional_multiplicative_only_analyses', 'task_conditional_additive_only_analyses', 'task_conditional_weights', 'task_conditional_multiplicative_only_weights', 'task_conditional_additive_only_weights', 'forgetting_exp_decay_params', 'baseline_ratio_curriculum_analyses', 'baseline_power_curriculum_analyses', 'epochs_to_completion'])\n"
     ]
    }
   ],
   "source": [
    "cache = analysis.refresh_cache()\n",
    "print(cache.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# Baseline analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'six_replications_analyses' in cache:\n",
    "    six_replications_analyses = cache['six_replications_analyses']\n",
    "\n",
    "else:\n",
    "    six_replications_by_dimension_runs = analysis.load_runs(60)\n",
    "    print('Loaded runs')\n",
    "\n",
    "    six_reps_dict = {dimension_name:analysis.process_multiple_runs(run_set) \n",
    "                     for run_set, dimension_name \n",
    "                     in zip(six_replications_by_dimension_runs, analysis.CONDITION_ANALYSES_FIELDS)}\n",
    "    six_replications_analyses = analysis.ConditionAnalysesSet(**six_reps_dict)\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(six_replications_analyses=six_replications_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'six_replications_updated_analyses' in cache:\n",
    "#     six_replications_updated_analyses = cache['six_replications_updated_analyses']\n",
    "\n",
    "# else:\n",
    "six_replications_by_dimension_runs = analysis.load_runs(60)\n",
    "print('Loaded runs')\n",
    "\n",
    "# note: the equal accuracy field will come in as accuracy_drops\n",
    "updated_six_reps_dict = {}\n",
    "start_index = 0\n",
    "for run_set, dimension_name in zip(six_replications_by_dimension_runs[start_index:], \n",
    "                                   analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "    updated_six_reps_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "        run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size) \n",
    "\n",
    "# combined_analysis = analysis.process_multiple_runs(\n",
    "#     six_replications_by_dimension_runs[3], \n",
    "#     parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size)\n",
    "\n",
    "six_replications_updated_analyses = analysis.ConditionAnalysesSet(**updated_six_reps_dict)\n",
    "\n",
    "cache = analysis.refresh_cache(dict(six_replications_updated_analyses=six_replications_updated_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'baseline_total_curve_analyses' in cache:\n",
    "#     baseline_total_curve_analyses = cache['baseline_total_curve_analyses']\n",
    "\n",
    "# else:\n",
    "six_replications_by_dimension_runs = analysis.load_runs(60)\n",
    "print('Loaded runs')\n",
    "\n",
    "analyses_per_dimension = {}\n",
    "\n",
    "for run_set, dimension_name in zip(six_replications_by_dimension_runs, analysis.CONDITION_ANALYSES_FIELDS):\n",
    "    print(f'Starting {dimension_name}')\n",
    "    total_curve_raw, total_curve_mean, total_curve_std, total_curve_sem = \\\n",
    "        analysis.process_multiple_runs_total_task_training_curves(run_set)\n",
    "\n",
    "    analyses_per_dimension[dimension_name] = analysis.TotalCurveResults(raw=total_curve_raw,\n",
    "                                                                        mean=total_curve_mean, \n",
    "                                                                        std=total_curve_std, \n",
    "                                                                        sem=total_curve_sem)\n",
    "\n",
    "total_curve_analyses = analysis.ConditionAnalysesSet(**analyses_per_dimension)\n",
    "cache = analysis.refresh_cache(dict(baseline_total_curve_analyses=total_curve_analyses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# Control analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'control_analyses' in cache:\n",
    "    control_analyses = cache['control_analyses']\n",
    "\n",
    "else:\n",
    "    control_runs = analysis.load_runs(150, 'meta-learning-scaling/sequential-benchmark-control', False)\n",
    "    print(f'Loaded runs')\n",
    "    control_analyses = analysis.ConditionAnalysesSet(combined=analysis.process_multiple_runs(control_runs.combined))\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(control_analyses=control_analyses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'six_replications_updated_analyses' in cache:\n",
    "#     six_replications_updated_analyses = cache['six_replications_updated_analyses']\n",
    "\n",
    "# else:\n",
    "control_runs = analysis.load_runs(150, 'meta-learning-scaling/sequential-benchmark-control', False)\n",
    "print('Loaded runs')\n",
    "\n",
    "updated_control_analyses = analysis.ConditionAnalysesSet(\n",
    "    combined=analysis.process_multiple_runs(control_runs.combined, \n",
    "                                            parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size))\n",
    "\n",
    "\n",
    "cache = analysis.refresh_cache(dict(updated_control_analyses=updated_control_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'control_total_curve_analyses' in cache:\n",
    "    control_total_curve_analyses = cache['control_total_curve_analyses']\n",
    "\n",
    "else:\n",
    "    control_runs = analysis.load_runs(150, 'meta-learning-scaling/sequential-benchmark-control', False)\n",
    "    print('Loaded runs')\n",
    "    \n",
    "    total_curve_raw, total_curve_mean, total_curve_std, total_curve_sem = \\\n",
    "            analysis.process_multiple_runs_total_task_training_curves(control_runs.combined)\n",
    "    \n",
    "    control_total_curve_analyses = analysis.ConditionAnalysesSet(\n",
    "        combined=analysis.TotalCurveResults(raw=total_curve_raw,\n",
    "                                            mean=total_curve_mean,\n",
    "                                            std=total_curve_std,\n",
    "                                            sem=total_curve_sem))\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(control_total_curve_analyses=control_total_curve_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = analysis.refresh_cache(dict(control_analyses=control_analyses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the number of examples by dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = (1000, 520000)\n",
    "\n",
    "plots.plot_processed_results(first_replication_analyses.color.examples, 'Color 10-run average', ylim)\n",
    "plots.plot_processed_results(first_replication_analyses.shape.examples, 'Shape 10-run average', ylim)\n",
    "plots.plot_processed_results(first_replication_analyses.texture.examples, 'Material 10-run average', ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = (1000, 700000)\n",
    "\n",
    "plots.plot_processed_results(six_replications_analyses.color.examples, 'Color 60-run average', ylim)\n",
    "plots.plot_processed_results(six_replications_analyses.shape.examples, 'Shape 60-run average', ylim)\n",
    "plots.plot_processed_results(six_replications_analyses.texture.examples, 'Material 60-run average', ylim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the log of the number of examples to criterion, in each dimension, with error bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the combined results over all 180 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = (7.75, 13.25)\n",
    "\n",
    "plots.plot_processed_results(six_replications_analyses.combined.log_examples, 'Combined 180-run average', \n",
    "                       ylim, log_x=(True, True), log_y=True, sem_n=180, shade_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the absolute accuracy after introducing a new task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = None\n",
    "\n",
    "plots.plot_processed_results(six_replications_analyses.color.accuracies, 'Color 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.shape.accuracies, 'Shape 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.texture.accuracies, 'Material 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.combined.accuracies, 'Combined 180-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=180, shade_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the accuracy drop after introducing a new task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ylim = None\n",
    "\n",
    "plots.plot_processed_results(six_replications_analyses.color.accuracy_drops, 'Color 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.shape.accuracy_drops, 'Shape 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)\n",
    "plots.plot_processed_results(six_replications_analyses.texture.accuracy_drops, 'Material 60-run average', \n",
    "                       ylim, log_x=False, log_y=False, sem_n=60, shade_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# Query-modulated analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'query_mod_replications' in cache:\n",
    "    query_mod_replications = cache['query_mod_replications']\n",
    "\n",
    "else:\n",
    "    query_mod_runs = analysis.query_modulated_runs_by_dimension(30)\n",
    "    query_mod_replications = {}\n",
    "\n",
    "    ignore_runs = [] # ('at6pkicv', )\n",
    "    for mod_level in query_mod_runs:\n",
    "        mod_level_runs = query_mod_runs[mod_level]\n",
    "\n",
    "        mod_level_dict = {dimension_name: analysis.process_multiple_runs(mod_level_runs[i], ignore_runs=ignore_runs) \n",
    "                          for i, dimension_name \n",
    "                          in enumerate(analysis.CONDITION_ANALYSES_FIELDS)}\n",
    "\n",
    "        query_mod_replications[mod_level] = analysis.ConditionAnalysesSet(**mod_level_dict)\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(query_mod_replications=query_mod_replications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = analysis.refresh_cache(dict(query_mod_updated_analyses=query_mod_replications))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'six_replications_updated_analyses' in cache:\n",
    "#     six_replications_updated_analyses = cache['six_replications_updated_analyses']\n",
    "\n",
    "# else:\n",
    "query_mod_runs = analysis.query_modulated_runs_by_dimension(30)\n",
    "print('Loaded runs')\n",
    "\n",
    "# note: the equal accuracy field will come in as accuracy_drops\n",
    "query_mod_replications = {}\n",
    "mod_levels = list(query_mod_runs.keys())\n",
    "start_index = 0\n",
    "\n",
    "for mod_level in mod_levels[start_index:]:\n",
    "    print(f'Starting mod level {mod_level}')\n",
    "    mod_level_runs = query_mod_runs[mod_level]\n",
    "\n",
    "    mod_level_dict = {dimension_name: analysis.process_multiple_runs(mod_level_runs[i], \n",
    "                                                                     parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size) \n",
    "                      for i, dimension_name \n",
    "                      in enumerate(analysis.CONDITION_ANALYSES_FIELDS)}\n",
    "\n",
    "    query_mod_replications[mod_level] = analysis.ConditionAnalysesSet(**mod_level_dict)\n",
    "\n",
    "\n",
    "cache = analysis.refresh_cache(dict(query_mod_updated_analyses=query_mod_replications))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = analysis.refresh_cache(dict(query_mod_replications=query_mod_replications,\n",
    "                                    control_analyses=control_analyses,\n",
    "                                    six_replications_analyses=six_replications_analyses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'query_mod_total_curve_analyses' in cache:\n",
    "    query_mod_total_curve_analyses = cache['query_mod_total_curve_analyses']\n",
    "\n",
    "else:\n",
    "    query_mod_runs = analysis.query_modulated_runs_by_dimension(30)\n",
    "    print('Loaded runs')\n",
    "    \n",
    "    query_mod_total_curve_analyses = {}\n",
    "    mod_levels = list(query_mod_runs.keys())\n",
    "    start_index = 0\n",
    "    \n",
    "    for mod_level in mod_levels[start_index:]:\n",
    "        print(f'Starting mod level {mod_level}')\n",
    "        mod_level_runs = query_mod_runs[mod_level]\n",
    "\n",
    "        analyses_per_dimension = {}\n",
    "        \n",
    "        for run_set, dimension_name in zip(mod_level_runs, analysis.CONDITION_ANALYSES_FIELDS):\n",
    "            print(f'Starting {dimension_name}')\n",
    "            total_curve_raw, total_curve_mean, total_curve_std, total_curve_sem = \\\n",
    "                analysis.process_multiple_runs_total_task_training_curves(run_set)\n",
    "\n",
    "            analyses_per_dimension[dimension_name] = analysis.TotalCurveResults(raw=total_curve_raw,\n",
    "                                                                                mean=total_curve_mean, \n",
    "                                                                                std=total_curve_std, \n",
    "                                                                                sem=total_curve_sem)\n",
    "\n",
    "        query_mod_total_curve_analyses[mod_level] = analysis.ConditionAnalysesSet(**analyses_per_dimension)\n",
    "            \n",
    "    cache = analysis.refresh_cache(dict(query_mod_total_curve_analyses=query_mod_total_curve_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_mod_total_curve_analyses.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "# MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'maml_analyses' in cache:\n",
    "    maml_analyses = cache['maml_analyses']\n",
    "\n",
    "else:\n",
    "    maml_runs = analysis.load_runs(30, 'meta-learning-scaling/maml-sequential-benchmark')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "    #                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    maml_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(maml_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        maml_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    maml_analyses = analysis.ConditionAnalysesSet(**maml_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(maml_analyses=maml_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'six_replications_updated_analyses' in cache:\n",
    "#     six_replications_updated_analyses = cache['six_replications_updated_analyses']\n",
    "\n",
    "# else:\n",
    "maml_alpha_0_runs = analysis.load_runs(20, 'meta-learning-scaling/maml-alpha-0')\n",
    "print('Loaded runs')\n",
    "\n",
    "raise ValueError('This will not work yeet')\n",
    "\n",
    "# ignore_runs = set(['ac82mceh', '7kau3ypy', 'g9ujw7gg', 'avmcbnot'])\n",
    "ignore_runs = set()\n",
    "\n",
    "maml_alpha_0_analyses_dict = {}\n",
    "start_index = 0\n",
    "for run_set, dimension_name in zip(maml_alpha_0_runs[start_index:], \n",
    "                                   analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "    \n",
    "    maml_alpha_0_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "        run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "        ignore_runs=ignore_runs) \n",
    "\n",
    "maml_alpha_0_analyses = analysis.ConditionAnalysesSet(**maml_alpha_0_analyses_dict)\n",
    "cache = analysis.refresh_cache(dict(maml_alpha_0_analyses=maml_alpha_0_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'maml_meta_test_analyses' in cache:\n",
    "    maml_meta_test_analyses = cache['maml_meta_test_analyses']\n",
    "\n",
    "else:\n",
    "    maml_meta_test_runs = analysis.load_runs(30, 'meta-learning-scaling/maml-meta-test')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    maml_meta_test_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(maml_meta_test_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        maml_meta_test_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    maml_meta_test_analyses = analysis.ConditionAnalysesSet(**maml_meta_test_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(maml_meta_test_analyses=maml_meta_test_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'balanced_batches_analyses' in cache:\n",
    "    balanced_batches_analyses = cache['balanced_batches_analyses']\n",
    "\n",
    "else:\n",
    "    balanced_batches_runs = analysis.load_runs(30, 'meta-learning-scaling/balanced-batches-sequential-benchmark')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    balanced_batches_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(balanced_batches_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        balanced_batches_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    balanced_batches_analyses = analysis.ConditionAnalysesSet(**balanced_batches_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(balanced_batches_analyses=balanced_batches_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maml_comparison_run_ids = [1000, 1001, 2000, 2001, 2002, \n",
    "                           2003, 2004, 2005, 2006, 2007, \n",
    "                           2008, 2009, 3000, 3001, 3002, \n",
    "                           3003, 3004, 3005, 3006, 3007, 3008]\n",
    "\n",
    "maml_comparison_runs = analysis.load_runs(10, split_runs_by_dimension=False, valid_run_ids=set(maml_comparison_run_ids))\n",
    "print('Loaded runs')\n",
    "\n",
    "baseline_maml_comparison_analyses = analysis.ConditionAnalysesSet(\n",
    "    combined=analysis.process_multiple_runs(maml_comparison_runs.combined, \n",
    "                                            parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,))\n",
    "\n",
    "\n",
    "cache = analysis.refresh_cache(dict(baseline_maml_comparison_analyses=baseline_maml_comparison_analyses))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Simultaneous training in a dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'simultaneous_training_analyses' in cache:\n",
    "#     simultaneous_training_analyses = cache['simultaneous_training_analyses']\n",
    "\n",
    "# else:\n",
    "simultaneous_training_runs = analysis.load_runs(20, 'meta-learning-scaling/simultaneous-training')\n",
    "print('Loaded runs')\n",
    "\n",
    "# ignore_runs = set(['u3gk9oio'])\n",
    "ignore_runs = set()\n",
    "\n",
    "simultaneous_training_analyses_dict = {}\n",
    "per_task_simultaneous_training_analyses_dict = {}\n",
    "start_index = 0\n",
    "for run_set, dimension_name in zip(simultaneous_training_runs[start_index:], \n",
    "                                   analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "    stacked_results, all_results_mean, all_results_std, all_results_sem, per_task_results_mean, per_task_results_std, per_task_results_sem = analysis.process_multiple_runs_simultaneous_training(run_set, ignore_runs=ignore_runs)\n",
    "\n",
    "    simultaneous_training_analyses_dict[dimension_name] = analysis.TotalCurveResults(raw=stacked_results,\n",
    "                                                                                     mean=all_results_mean, \n",
    "                                                                                     std=all_results_std, \n",
    "                                                                                     sem=all_results_sem)\n",
    "\n",
    "    per_task_simultaneous_training_analyses_dict[dimension_name] = analysis.TotalCurveResults(raw=stacked_results,\n",
    "                                                                                              mean=per_task_results_mean, \n",
    "                                                                                              std=per_task_results_std, \n",
    "                                                                                              sem=per_task_results_sem)\n",
    "\n",
    "simultaneous_training_analyses = analysis.ConditionAnalysesSet(**simultaneous_training_analyses_dict)\n",
    "per_task_simultaneous_training_analyses = analysis.ConditionAnalysesSet(**per_task_simultaneous_training_analyses_dict)\n",
    "cache = analysis.refresh_cache(dict(simultaneous_training_analyses=simultaneous_training_analyses,\n",
    "                                    per_task_simultaneous_training_analyses=per_task_simultaneous_training_analyses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# New Task Modulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'task_conditional_analyses' in cache:\n",
    "    task_conditional_analyses = cache['task_conditional_analyses']\n",
    "\n",
    "else:\n",
    "    task_modulated_runs = analysis.load_runs(20, 'meta-learning-scaling/task-conditional-all-layers')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    # ignore_runs = set(['Task-conditional-[0, 1, 2, 3]-1017'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    task_modulated_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(task_modulated_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        task_modulated_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    task_conditional_analyses = analysis.ConditionAnalysesSet(**task_modulated_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(task_conditional_analyses=task_conditional_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'task_conditional_multiplicative_only_analyses' in cache:\n",
    "    task_conditional_multiplicative_only_analyses = cache['task_conditional_multiplicative_only_analyses']\n",
    "\n",
    "else:\n",
    "    task_modulated_runs = analysis.load_runs(20, 'meta-learning-scaling/task-conditional-all-layers-multiplicative-only')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['Task-conditional-multiplicative-[0, 1, 2, 3]-1006',\n",
    "    #                   'Task-conditional-multiplicative-[0, 1, 2, 3]-1005'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    task_modulated_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(task_modulated_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        task_modulated_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    task_conditional_multiplicative_only_analyses = analysis.ConditionAnalysesSet(**task_modulated_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(task_conditional_multiplicative_only_analyses=task_conditional_multiplicative_only_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'task_conditional_additive_only_analyses' in cache:\n",
    "    task_conditional_additive_only_analyses = cache['task_conditional_additive_only_analyses']\n",
    "\n",
    "else:\n",
    "    task_modulated_runs = analyses_caches/alysis.load_runs(20, 'meta-learning-scaling/task-conditional-all-layers-additive-only')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    task_modulated_analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(task_modulated_runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        task_modulated_analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    task_conditional_additive_only_analyses = analysis.ConditionAnalysesSet(**task_modulated_analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(task_conditional_additive_only_analyses=task_conditional_additive_only_analyses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the task-conditional weights en masse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'task_conditional_weights' in cache:\n",
    "    task_conditional_weights = cache['task_conditional_weights']\n",
    "\n",
    "else:\n",
    "    task_modulated_runs = analysis.load_runs(20, 'meta-learning-scaling/task-conditional-all-layers')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    ignore_runs = set(['Task-conditional-[0, 1, 2, 3]-1013',\n",
    "                      'Task-conditional-[0, 1, 2, 3]-1011',\n",
    "                      'Task-conditional-[0, 1, 2, 3]-1001'])\n",
    "#     ignore_runs = set()\n",
    "\n",
    "    task_conditional_weights = analysis.parse_task_conditional_weights(task_modulated_runs.combined,\n",
    "                                                                      additive=True,\n",
    "                                                                      multiplicative=True,\n",
    "                                                                      ignore_runs=ignore_runs)\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(task_conditional_weights=task_conditional_weights))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'task_conditional_multiplicative_only_weights' in cache:\n",
    "    task_conditional_multiplicative_only_weights = cache['task_conditional_multiplicative_only_weights']\n",
    "\n",
    "else:\n",
    "    task_modulated_runs = analysis.load_runs(20, 'meta-learning-scaling/task-conditional-all-layers-multiplicative-only')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['Task-conditional-multiplicative-[0, 1, 2, 3]-1006',\n",
    "    #                   'Task-conditional-multiplicative-[0, 1, 2, 3]-1005'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    task_conditional_multiplicative_only_weights = analysis.parse_task_conditional_weights(task_modulated_runs.combined,\n",
    "                                                                      additive=False,\n",
    "                                                                      multiplicative=True,\n",
    "                                                                      ignore_runs=ignore_runs)\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(task_conditional_multiplicative_only_weights=task_conditional_multiplicative_only_weights))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'task_conditional_additive_only_weights' in cache:\n",
    "    task_conditional_additive_only_weights = cache['task_conditional_additive_only_weights']\n",
    "\n",
    "else:\n",
    "    task_modulated_runs = analysis.load_runs(20, 'meta-learning-scaling/task-conditional-all-layers-additive-only')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['u3gk9oio'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    task_conditional_additive_only_weights = analysis.parse_task_conditional_weights(task_modulated_runs.combined,\n",
    "                                                                      additive=True,\n",
    "                                                                      multiplicative=False,\n",
    "                                                                      ignore_runs=ignore_runs)\n",
    "\n",
    "    cache = analysis.refresh_cache(dict(task_conditional_additive_only_weights=task_conditional_additive_only_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "# The curriculum experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded runs\n",
      "baseline-curriculum-balanced-batches-1009\n",
      "baseline-curriculum-balanced-batches-1008\n",
      "baseline-curriculum-balanced-batches-1007\n",
      "baseline-curriculum-balanced-batches-1006\n",
      "baseline-curriculum-balanced-batches-1005\n",
      "baseline-curriculum-balanced-batches-1004\n",
      "baseline-curriculum-balanced-batches-1003\n",
      "baseline-curriculum-balanced-batches-1002\n",
      "baseline-curriculum-balanced-batches-1001\n",
      "baseline-curriculum-balanced-batches-1000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 241\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "baseline-curriculum-balanced-batches-2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guydavidson/projects/deep-learning-projects/notebooks/meta_learning_data_analysis.py:533: RuntimeWarning: Mean of empty slice\n",
      "  mean=np.nanmean(result_set, axis=0),\n",
      "/Users/guydavidson/opt/anaconda3/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1667: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline-curriculum-balanced-batches-2008\n",
      "baseline-curriculum-balanced-batches-2007\n",
      "baseline-curriculum-balanced-batches-2006\n",
      "baseline-curriculum-balanced-batches-2005\n",
      "baseline-curriculum-balanced-batches-2004\n",
      "baseline-curriculum-balanced-batches-2003\n",
      "baseline-curriculum-balanced-batches-2002\n",
      "baseline-curriculum-balanced-batches-2001\n",
      "baseline-curriculum-balanced-batches-2000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 35\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "baseline-curriculum-balanced-batches-3009\n",
      "baseline-curriculum-balanced-batches-3008\n",
      "baseline-curriculum-balanced-batches-3007\n",
      "baseline-curriculum-balanced-batches-3006\n",
      "baseline-curriculum-balanced-batches-3005\n",
      "baseline-curriculum-balanced-batches-3004\n",
      "baseline-curriculum-balanced-batches-3003\n",
      "baseline-curriculum-balanced-batches-3002\n",
      "baseline-curriculum-balanced-batches-3001\n",
      "baseline-curriculum-balanced-batches-3000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 28\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "baseline-curriculum-balanced-batches-1009\n",
      "baseline-curriculum-balanced-batches-1008\n",
      "baseline-curriculum-balanced-batches-1007\n",
      "baseline-curriculum-balanced-batches-1006\n",
      "baseline-curriculum-balanced-batches-1005\n",
      "baseline-curriculum-balanced-batches-1004\n",
      "baseline-curriculum-balanced-batches-1003\n",
      "baseline-curriculum-balanced-batches-1002\n",
      "baseline-curriculum-balanced-batches-3009\n",
      "baseline-curriculum-balanced-batches-3008\n",
      "baseline-curriculum-balanced-batches-3007 10\n",
      "baseline-curriculum-balanced-batches-2009\n",
      "baseline-curriculum-balanced-batches-3006\n",
      "baseline-curriculum-balanced-batches-2008\n",
      "baseline-curriculum-balanced-batches-2007\n",
      "baseline-curriculum-balanced-batches-3005\n",
      "baseline-curriculum-balanced-batches-2006\n",
      "baseline-curriculum-balanced-batches-3004\n",
      "baseline-curriculum-balanced-batches-1001\n",
      "baseline-curriculum-balanced-batches-2005\n",
      "baseline-curriculum-balanced-batches-3003 20\n",
      "baseline-curriculum-balanced-batches-2004\n",
      "baseline-curriculum-balanced-batches-3002\n",
      "baseline-curriculum-balanced-batches-2003\n",
      "baseline-curriculum-balanced-batches-2002\n",
      "baseline-curriculum-balanced-batches-3001\n",
      "baseline-curriculum-balanced-batches-2001\n",
      "baseline-curriculum-balanced-batches-3000\n",
      "baseline-curriculum-balanced-batches-2000\n",
      "baseline-curriculum-balanced-batches-1000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 241\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n"
     ]
    }
   ],
   "source": [
    "if 'baseline_ratio_curriculum_analyses' in cache:\n",
    "    baseline_ratio_curriculum_analyses = cache['baseline_ratio_curriculum_analyses']\n",
    "\n",
    "else:\n",
    "    runs = analysis.load_runs(20, 'meta-learning-scaling/baseline-curriculum-balanced-batches')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "    #                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    baseline_ratio_curriculum_analyses = analysis.ConditionAnalysesSet(**analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(baseline_ratio_curriculum_analyses=baseline_ratio_curriculum_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded runs\n",
      "curriculum-ratio-1-5-balanced-batches-1002\n",
      "curriculum-ratio-1-5-balanced-batches-1004\n",
      "curriculum-ratio-1-5-balanced-batches-1003\n",
      "curriculum-ratio-1-5-balanced-batches-1009\n",
      "curriculum-ratio-1-5-balanced-batches-1001\n",
      "curriculum-ratio-1-5-balanced-batches-1008\n",
      "curriculum-ratio-1-5-balanced-batches-1007\n",
      "curriculum-ratio-1-5-balanced-batches-1006\n",
      "curriculum-ratio-1-5-balanced-batches-1005\n",
      "curriculum-ratio-1-5-balanced-batches-1000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 576\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "curriculum-ratio-1-5-balanced-batches-2009\n",
      "curriculum-ratio-1-5-balanced-batches-2008\n",
      "curriculum-ratio-1-5-balanced-batches-2007\n",
      "curriculum-ratio-1-5-balanced-batches-2006\n",
      "curriculum-ratio-1-5-balanced-batches-2005\n",
      "curriculum-ratio-1-5-balanced-batches-2004\n",
      "curriculum-ratio-1-5-balanced-batches-2003\n",
      "curriculum-ratio-1-5-balanced-batches-2002\n",
      "curriculum-ratio-1-5-balanced-batches-2001\n",
      "curriculum-ratio-1-5-balanced-batches-2000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 43\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "curriculum-ratio-1-5-balanced-batches-3009\n",
      "curriculum-ratio-1-5-balanced-batches-3008\n",
      "curriculum-ratio-1-5-balanced-batches-3007\n",
      "curriculum-ratio-1-5-balanced-batches-3006\n",
      "curriculum-ratio-1-5-balanced-batches-3005\n",
      "curriculum-ratio-1-5-balanced-batches-3004\n",
      "curriculum-ratio-1-5-balanced-batches-3003\n",
      "curriculum-ratio-1-5-balanced-batches-3002\n",
      "curriculum-ratio-1-5-balanced-batches-3001\n",
      "curriculum-ratio-1-5-balanced-batches-3000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 114\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "curriculum-ratio-1-5-balanced-batches-1002\n",
      "curriculum-ratio-1-5-balanced-batches-1004\n",
      "curriculum-ratio-1-5-balanced-batches-1003\n",
      "curriculum-ratio-1-5-balanced-batches-1009\n",
      "curriculum-ratio-1-5-balanced-batches-1001\n",
      "curriculum-ratio-1-5-balanced-batches-1008\n",
      "curriculum-ratio-1-5-balanced-batches-1007\n",
      "curriculum-ratio-1-5-balanced-batches-1006\n",
      "curriculum-ratio-1-5-balanced-batches-1005\n",
      "curriculum-ratio-1-5-balanced-batches-3009\n",
      "curriculum-ratio-1-5-balanced-batches-3008 10\n",
      "curriculum-ratio-1-5-balanced-batches-1000\n",
      "curriculum-ratio-1-5-balanced-batches-3007\n",
      "curriculum-ratio-1-5-balanced-batches-2009\n",
      "curriculum-ratio-1-5-balanced-batches-3006\n",
      "curriculum-ratio-1-5-balanced-batches-3005\n",
      "curriculum-ratio-1-5-balanced-batches-2008\n",
      "curriculum-ratio-1-5-balanced-batches-2007\n",
      "curriculum-ratio-1-5-balanced-batches-2006\n",
      "curriculum-ratio-1-5-balanced-batches-3004\n",
      "curriculum-ratio-1-5-balanced-batches-3003 20\n",
      "curriculum-ratio-1-5-balanced-batches-2005\n",
      "curriculum-ratio-1-5-balanced-batches-3002\n",
      "curriculum-ratio-1-5-balanced-batches-2004\n",
      "curriculum-ratio-1-5-balanced-batches-2003\n",
      "curriculum-ratio-1-5-balanced-batches-2002\n",
      "curriculum-ratio-1-5-balanced-batches-3001\n",
      "curriculum-ratio-1-5-balanced-batches-2001\n",
      "curriculum-ratio-1-5-balanced-batches-3000\n",
      "curriculum-ratio-1-5-balanced-batches-2000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 576\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n"
     ]
    }
   ],
   "source": [
    "if 'ratio_curriculum_1_5_analyses' in cache:\n",
    "    ratio_curriculum_1_5_analyses = cache['ratio_curriculum_1_5_analyses']\n",
    "\n",
    "else:\n",
    "    runs = analysis.load_runs(20, 'meta-learning-scaling/curriculum-ratio-1-5-balanced-batches')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "    #                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    ratio_curriculum_1_5_analyses = analysis.ConditionAnalysesSet(**analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(ratio_curriculum_1_5_analyses=ratio_curriculum_1_5_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded runs\n",
      "power-curriculum-default-alpha-balanced-batches-1009\n",
      "power-curriculum-default-alpha-balanced-batches-1008\n",
      "power-curriculum-default-alpha-balanced-batches-1007\n",
      "power-curriculum-default-alpha-balanced-batches-1006\n",
      "power-curriculum-default-alpha-balanced-batches-1005\n",
      "power-curriculum-default-alpha-balanced-batches-1004\n",
      "power-curriculum-default-alpha-balanced-batches-1003\n",
      "power-curriculum-default-alpha-balanced-batches-1002\n",
      "power-curriculum-default-alpha-balanced-batches-1001\n",
      "power-curriculum-default-alpha-balanced-batches-1000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 84\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "power-curriculum-default-alpha-balanced-batches-2009\n",
      "power-curriculum-default-alpha-balanced-batches-2008\n",
      "power-curriculum-default-alpha-balanced-batches-2007\n",
      "power-curriculum-default-alpha-balanced-batches-2006\n",
      "power-curriculum-default-alpha-balanced-batches-2005\n",
      "power-curriculum-default-alpha-balanced-batches-2004\n",
      "power-curriculum-default-alpha-balanced-batches-2003\n",
      "power-curriculum-default-alpha-balanced-batches-2002\n",
      "power-curriculum-default-alpha-balanced-batches-2001\n",
      "power-curriculum-default-alpha-balanced-batches-2000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 36\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "power-curriculum-default-alpha-balanced-batches-3009\n",
      "power-curriculum-default-alpha-balanced-batches-3008\n",
      "power-curriculum-default-alpha-balanced-batches-3007\n",
      "power-curriculum-default-alpha-balanced-batches-3006\n",
      "power-curriculum-default-alpha-balanced-batches-3005\n",
      "power-curriculum-default-alpha-balanced-batches-3004\n",
      "power-curriculum-default-alpha-balanced-batches-3003\n",
      "power-curriculum-default-alpha-balanced-batches-3002\n",
      "power-curriculum-default-alpha-balanced-batches-3001\n",
      "power-curriculum-default-alpha-balanced-batches-3000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 46\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "power-curriculum-default-alpha-balanced-batches-1009\n",
      "power-curriculum-default-alpha-balanced-batches-1008\n",
      "power-curriculum-default-alpha-balanced-batches-1007\n",
      "power-curriculum-default-alpha-balanced-batches-1006\n",
      "power-curriculum-default-alpha-balanced-batches-1005\n",
      "power-curriculum-default-alpha-balanced-batches-1004\n",
      "power-curriculum-default-alpha-balanced-batches-3009\n",
      "power-curriculum-default-alpha-balanced-batches-2009\n",
      "power-curriculum-default-alpha-balanced-batches-3008\n",
      "power-curriculum-default-alpha-balanced-batches-1003\n",
      "power-curriculum-default-alpha-balanced-batches-2008 10\n",
      "power-curriculum-default-alpha-balanced-batches-3007\n",
      "power-curriculum-default-alpha-balanced-batches-2007\n",
      "power-curriculum-default-alpha-balanced-batches-3006\n",
      "power-curriculum-default-alpha-balanced-batches-2006\n",
      "power-curriculum-default-alpha-balanced-batches-1002\n",
      "power-curriculum-default-alpha-balanced-batches-3005\n",
      "power-curriculum-default-alpha-balanced-batches-2005\n",
      "power-curriculum-default-alpha-balanced-batches-3004\n",
      "power-curriculum-default-alpha-balanced-batches-2004\n",
      "power-curriculum-default-alpha-balanced-batches-3003 20\n",
      "power-curriculum-default-alpha-balanced-batches-2003\n",
      "power-curriculum-default-alpha-balanced-batches-3002\n",
      "power-curriculum-default-alpha-balanced-batches-1001\n",
      "power-curriculum-default-alpha-balanced-batches-2002\n",
      "power-curriculum-default-alpha-balanced-batches-3001\n",
      "power-curriculum-default-alpha-balanced-batches-2001\n",
      "power-curriculum-default-alpha-balanced-batches-3000\n",
      "power-curriculum-default-alpha-balanced-batches-2000\n",
      "power-curriculum-default-alpha-balanced-batches-1000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 84\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n"
     ]
    }
   ],
   "source": [
    "if 'baseline_power_curriculum_analyses' in cache:\n",
    "    baseline_power_curriculum_analyses = cache['baseline_power_curriculum_analyses']\n",
    "\n",
    "else:\n",
    "    runs = analysis.load_runs(20, 'meta-learning-scaling/power-curriculum-default-alpha-balanced-batches')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "    #                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    baseline_power_curriculum_analyses = analysis.ConditionAnalysesSet(**analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(baseline_power_curriculum_analyses=baseline_power_curriculum_analyses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded runs\n",
      "a4hj9y7k power-curriculum-alpha-2-balanced-batches-1002\n",
      "0a1iifo9 power-curriculum-alpha-2-balanced-batches-1004\n",
      "t071il8w power-curriculum-alpha-2-balanced-batches-1009\n",
      "8dtqbr1j power-curriculum-alpha-2-balanced-batches-1008\n",
      "mdjzx1jh power-curriculum-alpha-2-balanced-batches-1003\n",
      "p88x7rzh power-curriculum-alpha-2-balanced-batches-1001\n",
      "81so866q power-curriculum-alpha-2-balanced-batches-1007\n",
      "itt7kotx power-curriculum-alpha-2-balanced-batches-1006\n",
      "j94dgyhy power-curriculum-alpha-2-balanced-batches-1005\n",
      "4s2sznro power-curriculum-alpha-2-balanced-batches-1000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 914\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "1j7x7cx0 power-curriculum-alpha-2-balanced-batches-2009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guydavidson/projects/deep-learning-projects/notebooks/meta_learning_data_analysis.py:533: RuntimeWarning: Mean of empty slice\n",
      "  mean=np.nanmean(result_set, axis=0),\n",
      "/Users/guydavidson/opt/anaconda3/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1667: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tkbqabcy power-curriculum-alpha-2-balanced-batches-2008\n",
      "e2yilpsf power-curriculum-alpha-2-balanced-batches-2007\n",
      "oze9neb3 power-curriculum-alpha-2-balanced-batches-2006\n",
      "iozr2j1y power-curriculum-alpha-2-balanced-batches-2005\n",
      "hgbn92hj power-curriculum-alpha-2-balanced-batches-2004\n",
      "1uyhapjj power-curriculum-alpha-2-balanced-batches-2003\n",
      "90qs1ggr power-curriculum-alpha-2-balanced-batches-2002\n",
      "hzviecjv power-curriculum-alpha-2-balanced-batches-2001\n",
      "auhn18le power-curriculum-alpha-2-balanced-batches-2000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 91\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "s801akir power-curriculum-alpha-2-balanced-batches-3009\n",
      "mnma5wha power-curriculum-alpha-2-balanced-batches-3008\n",
      "9hcoqk36 power-curriculum-alpha-2-balanced-batches-3007\n",
      "wq7l6495 power-curriculum-alpha-2-balanced-batches-3006\n",
      "3fh9lrwx power-curriculum-alpha-2-balanced-batches-3005\n",
      "qgv2q9eb power-curriculum-alpha-2-balanced-batches-3004\n",
      "8qnc7fgz power-curriculum-alpha-2-balanced-batches-3003\n",
      "ch9qru5o power-curriculum-alpha-2-balanced-batches-3002\n",
      "82mv7f8y power-curriculum-alpha-2-balanced-batches-3001\n",
      "5eetqy8y power-curriculum-alpha-2-balanced-batches-3000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 127\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n",
      "a4hj9y7k power-curriculum-alpha-2-balanced-batches-1002\n",
      "0a1iifo9 power-curriculum-alpha-2-balanced-batches-1004\n",
      "t071il8w power-curriculum-alpha-2-balanced-batches-1009\n",
      "8dtqbr1j power-curriculum-alpha-2-balanced-batches-1008\n",
      "s801akir power-curriculum-alpha-2-balanced-batches-3009\n",
      "mdjzx1jh power-curriculum-alpha-2-balanced-batches-1003\n",
      "mnma5wha power-curriculum-alpha-2-balanced-batches-3008\n",
      "p88x7rzh power-curriculum-alpha-2-balanced-batches-1001\n",
      "1j7x7cx0 power-curriculum-alpha-2-balanced-batches-2009\n",
      "9hcoqk36 power-curriculum-alpha-2-balanced-batches-3007\n",
      "81so866q power-curriculum-alpha-2-balanced-batches-1007 10\n",
      "tkbqabcy power-curriculum-alpha-2-balanced-batches-2008\n",
      "wq7l6495 power-curriculum-alpha-2-balanced-batches-3006\n",
      "itt7kotx power-curriculum-alpha-2-balanced-batches-1006\n",
      "e2yilpsf power-curriculum-alpha-2-balanced-batches-2007\n",
      "3fh9lrwx power-curriculum-alpha-2-balanced-batches-3005\n",
      "j94dgyhy power-curriculum-alpha-2-balanced-batches-1005\n",
      "oze9neb3 power-curriculum-alpha-2-balanced-batches-2006\n",
      "qgv2q9eb power-curriculum-alpha-2-balanced-batches-3004\n",
      "iozr2j1y power-curriculum-alpha-2-balanced-batches-2005\n",
      "4s2sznro power-curriculum-alpha-2-balanced-batches-1000 20\n",
      "8qnc7fgz power-curriculum-alpha-2-balanced-batches-3003\n",
      "hgbn92hj power-curriculum-alpha-2-balanced-batches-2004\n",
      "1uyhapjj power-curriculum-alpha-2-balanced-batches-2003\n",
      "ch9qru5o power-curriculum-alpha-2-balanced-batches-3002\n",
      "90qs1ggr power-curriculum-alpha-2-balanced-batches-2002\n",
      "82mv7f8y power-curriculum-alpha-2-balanced-batches-3001\n",
      "hzviecjv power-curriculum-alpha-2-balanced-batches-2001\n",
      "5eetqy8y power-curriculum-alpha-2-balanced-batches-3000\n",
      "auhn18le power-curriculum-alpha-2-balanced-batches-2000\n",
      "Removing extraneous nans\n",
      "Max first nan index: 914\n",
      "Examples to criterion examples\n",
      "Log examples to criterion log_examples\n",
      "New task accuracy accuracies\n",
      "New task accuracy delta accuracy_drops\n",
      "First task accuracy by epoch first_task_accuracies\n",
      "New task accuracy by epoch new_task_accuracies\n"
     ]
    }
   ],
   "source": [
    "if 'power_curriculum_2_analyses' in cache:\n",
    "    power_curriculum_2_analyses = cache['power_curriculum_2_analyses']\n",
    "\n",
    "else:\n",
    "    runs = analysis.load_runs(20, 'meta-learning-scaling/power-curriculum-alpha-2-balanced-batches')\n",
    "    print('Loaded runs')\n",
    "\n",
    "    # ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "    #                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "    ignore_runs = set()\n",
    "\n",
    "    analyses_dict = {}\n",
    "    start_index = 0\n",
    "    for run_set, dimension_name in zip(runs[start_index:], \n",
    "                                       analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "        analyses_dict[dimension_name] = analysis.process_multiple_runs(\n",
    "            run_set, parse_func=analysis.parse_run_results_with_new_task_accuracy_and_equal_size,\n",
    "            ignore_runs=ignore_runs) \n",
    "\n",
    "    power_curriculum_2_analyses = analysis.ConditionAnalysesSet(**analyses_dict)\n",
    "    cache = analysis.refresh_cache(dict(power_curriculum_2_analyses=power_curriculum_2_analyses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the epochs to completion in each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS_TO_COMPLETION_SETS = (\n",
    "    # name, URI, num_runs, ignore_runs, split_by_condition\n",
    "    ('baseline', 'meta-learning-scaling/sequential-benchmark-baseline', 60, None, True),\n",
    "    ('heterogeneous', 'meta-learning-scaling/sequential-benchmark-control', 150, None, False),\n",
    "    # (),  # TODO: query-modulated, too?\n",
    "    ('ratio_curriculum', 'meta-learning-scaling/baseline-curriculum-balanced-batches', 20, None, True),\n",
    "    ('ratio_curriculum_1_5', 'meta-learning-scaling/curriculum-ratio-1-5-balanced-batches', 20, None, True),\n",
    "    ('power_curriculum', 'meta-learning-scaling/power-curriculum-default-alpha-balanced-batches', 20, None, True),\n",
    "    ('power_curriculum_2', 'meta-learning-scaling/power-curriculum-alpha-2-balanced-batches', 20, None, True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">baseline/color</span>\n",
       "<progress style=\"width:45ex\" max=\"60\" value=\"60\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">60/60</span>\n",
       "<span class=\"Time-label\">[03:56<00:03, 3.94s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " baseline/color [█████████████████████████████████████████████] 60/60 [03:56<00:03, 3.94s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error resolved after 0:00:26.981506, resuming normal operation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">baseline/shape</span>\n",
       "<progress style=\"width:45ex\" max=\"60\" value=\"60\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">60/60</span>\n",
       "<span class=\"Time-label\">[01:55<00:08, 1.92s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       " baseline/shape [█████████████████████████████████████████████] 60/60 [01:55<00:08, 1.92s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">baseline/texture</span>\n",
       "<progress style=\"width:45ex\" max=\"60\" value=\"60\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">60/60</span>\n",
       "<span class=\"Time-label\">[02:51<00:02, 2.85s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "baseline/textur [█████████████████████████████████████████████] 60/60 [02:51<00:02, 2.85s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">baseline/combined</span>\n",
       "<progress style=\"width:45ex\" max=\"180\" value=\"180\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">180/180</span>\n",
       "<span class=\"Time-label\">[09:52<00:04, 3.29s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "baseline/combin [█████████████████████████████████████████████] 180/180 [09:52<00:04, 3.29s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error resolved after 0:00:14.102628, resuming normal operation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">heterogeneous</span>\n",
       "<progress style=\"width:45ex\" max=\"90\" value=\"90\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">90/90</span>\n",
       "<span class=\"Time-label\">[04:47<00:03, 3.19s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "  heterogeneous [█████████████████████████████████████████████] 90/90 [04:47<00:03, 3.19s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">ratio_curriculum/color</span>\n",
       "<progress style=\"width:45ex\" max=\"10\" value=\"10\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">10/10</span>\n",
       "<span class=\"Time-label\">[00:33<00:15, 3.33s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "ratio_curriculu [█████████████████████████████████████████████] 10/10 [00:33<00:15, 3.33s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">ratio_curriculum/shape</span>\n",
       "<progress style=\"width:45ex\" max=\"10\" value=\"10\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">10/10</span>\n",
       "<span class=\"Time-label\">[00:14<00:00, 1.39s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "ratio_curriculu [█████████████████████████████████████████████] 10/10 [00:14<00:00, 1.39s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">ratio_curriculum/texture</span>\n",
       "<progress style=\"width:45ex\" max=\"10\" value=\"10\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">10/10</span>\n",
       "<span class=\"Time-label\">[00:12<00:01, 1.18s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "ratio_curriculu [█████████████████████████████████████████████] 10/10 [00:12<00:01, 1.18s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">ratio_curriculum/combined</span>\n",
       "<progress style=\"width:45ex\" max=\"30\" value=\"30\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">30/30</span>\n",
       "<span class=\"Time-label\">[00:54<00:02, 1.81s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "ratio_curriculu [█████████████████████████████████████████████] 30/30 [00:54<00:02, 1.81s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error resolved after 0:00:12.721399, resuming normal operation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">ratio_curriculum_1_5/color</span>\n",
       "<progress style=\"width:45ex\" max=\"10\" value=\"10\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">10/10</span>\n",
       "<span class=\"Time-label\">[00:20<00:04, 2.00s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "ratio_curriculu [█████████████████████████████████████████████] 10/10 [00:20<00:04, 2.00s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guydavidson/projects/deep-learning-projects/notebooks/meta_learning_data_analysis.py:476: RuntimeWarning: divide by zero encountered in log\n",
      "  ResultSet(name=name, mean=np.nanmean(np.log(results_per_run), axis=0), std=np.nanstd(np.log(results_per_run), axis=0))\n",
      "/Users/guydavidson/opt/anaconda3/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1541: RuntimeWarning: invalid value encountered in subtract\n",
      "  np.subtract(arr, avg, out=arr, casting='unsafe')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">ratio_curriculum_1_5/shape</span>\n",
       "<progress style=\"width:45ex\" max=\"10\" value=\"10\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">10/10</span>\n",
       "<span class=\"Time-label\">[00:09<00:01, 0.93s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "ratio_curriculu [█████████████████████████████████████████████] 10/10 [00:09<00:01, 0.93s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">ratio_curriculum_1_5/texture</span>\n",
       "<progress style=\"width:45ex\" max=\"10\" value=\"10\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">10/10</span>\n",
       "<span class=\"Time-label\">[00:14<00:01, 1.36s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "ratio_curriculu [█████████████████████████████████████████████] 10/10 [00:14<00:01, 1.36s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">ratio_curriculum_1_5/combined</span>\n",
       "<progress style=\"width:45ex\" max=\"30\" value=\"30\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">30/30</span>\n",
       "<span class=\"Time-label\">[00:36<00:01, 1.21s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "ratio_curriculu [█████████████████████████████████████████████] 30/30 [00:36<00:01, 1.21s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">power_curriculum/color</span>\n",
       "<progress style=\"width:45ex\" max=\"10\" value=\"10\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">10/10</span>\n",
       "<span class=\"Time-label\">[00:16<00:02, 1.58s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "power_curriculu [█████████████████████████████████████████████] 10/10 [00:16<00:02, 1.58s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">power_curriculum/shape</span>\n",
       "<progress style=\"width:45ex\" max=\"10\" value=\"10\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">10/10</span>\n",
       "<span class=\"Time-label\">[00:10<00:01, 1.01s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "power_curriculu [█████████████████████████████████████████████] 10/10 [00:10<00:01, 1.01s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">power_curriculum/texture</span>\n",
       "<progress style=\"width:45ex\" max=\"10\" value=\"10\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">10/10</span>\n",
       "<span class=\"Time-label\">[00:09<00:01, 0.93s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "power_curriculu [█████████████████████████████████████████████] 10/10 [00:09<00:01, 0.93s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">power_curriculum/combined</span>\n",
       "<progress style=\"width:45ex\" max=\"30\" value=\"30\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">30/30</span>\n",
       "<span class=\"Time-label\">[00:27<00:01, 0.89s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "power_curriculu [█████████████████████████████████████████████] 30/30 [00:27<00:01, 0.89s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">power_curriculum_2/color</span>\n",
       "<progress style=\"width:45ex\" max=\"10\" value=\"10\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">10/10</span>\n",
       "<span class=\"Time-label\">[00:23<00:05, 2.32s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "power_curriculu [█████████████████████████████████████████████] 10/10 [00:23<00:05, 2.32s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">power_curriculum_2/shape</span>\n",
       "<progress style=\"width:45ex\" max=\"10\" value=\"10\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">10/10</span>\n",
       "<span class=\"Time-label\">[00:13<00:01, 1.35s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "power_curriculu [█████████████████████████████████████████████] 10/10 [00:13<00:01, 1.35s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">power_curriculum_2/texture</span>\n",
       "<progress style=\"width:45ex\" max=\"10\" value=\"10\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">10/10</span>\n",
       "<span class=\"Time-label\">[00:16<00:01, 1.64s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "power_curriculu [█████████████████████████████████████████████] 10/10 [00:16<00:01, 1.64s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">power_curriculum_2/combined</span>\n",
       "<progress style=\"width:45ex\" max=\"30\" value=\"30\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">30/30</span>\n",
       "<span class=\"Time-label\">[01:01<00:01, 2.03s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "power_curriculu [█████████████████████████████████████████████] 30/30 [01:01<00:01, 2.03s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'epochs_to_completion' in cache:\n",
    "    epochs_to_completion = cache['epochs_to_completion']\n",
    "    \n",
    "else:\n",
    "    epochs_to_completion = dict()\n",
    "    \n",
    "    for name, path, num_runs, ignore_runs, split_runs_by_dimension in EPOCHS_TO_COMPLETION_SETS:\n",
    "        runs = analysis.load_runs(num_runs, path, split_runs_by_dimension=split_runs_by_dimension)\n",
    "        raw_analyses_dict = {}\n",
    "        log_analyses_dict = {}\n",
    "        \n",
    "        if split_runs_by_dimension:\n",
    "            for run_set, dimension_name in zip(runs, analysis.CONDITION_ANALYSES_FIELDS):\n",
    "                results, log_results = analysis.epochs_to_taks_completions(run_set, ignore_runs=ignore_runs, ipb_desc=f'{name}/{dimension_name}') \n",
    "                raw_analyses_dict[dimension_name] = results\n",
    "                log_analyses_dict[dimension_name] = log_results\n",
    "                \n",
    "        else:\n",
    "            results, log_results = analysis.epochs_to_taks_completions(runs.combined, ignore_runs=ignore_runs, ipb_desc=f'{name}') \n",
    "            raw_analyses_dict[analysis.COMBINED] = results\n",
    "            log_analyses_dict[analysis.COMBINED] = log_results\n",
    "            \n",
    "        raw_analyses_set = analysis.ConditionAnalysesSet(**raw_analyses_dict)\n",
    "        log_analyses_set = analysis.ConditionAnalysesSet(**log_analyses_dict)\n",
    "        \n",
    "        epochs_to_completion[name] = dict(raw=raw_analyses_set, log=log_analyses_set)\n",
    "        \n",
    "    cache = analysis.refresh_cache(dict(epochs_to_completion=epochs_to_completion)) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ai99wvyu'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = analysis.load_runs(20, 'meta-learning-scaling/baseline-curriculum-balanced-batches')\n",
    "print('Loaded runs')\n",
    "\n",
    "# ignore_runs = set(['oz996ztv', 'oin8pqu2', 'h09i9xyg', 'umo8x16f', 'pm76ui1s', \n",
    "#                    'qnyo08x8', 'hmjtjheq', 'wki7q8cs', 'q8ku840y'])\n",
    "ignore_runs = set()\n",
    "\n",
    "raw_analyses_dict = {}\n",
    "log_analyses_dict = {}\n",
    "start_index = 0\n",
    "for run_set, dimension_name in zip(runs[start_index:], \n",
    "                                   analysis.CONDITION_ANALYSES_FIELDS[start_index:]):\n",
    "\n",
    "    results, log_results = analysis.epochs_to_taks_completions(run_set, ignore_runs=ignore_runs) \n",
    "    raw_analyses_dict[dimension_name] = results\n",
    "    log_analyses_dict[dimension_name] = log_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_finished_succesfully(run, samples=5000):\n",
    "    df = run.history(pandas=True, samples=samples)\n",
    "    test_acc_column_names = [f'Test Accuracy, Query #{i + 1}' for i in range(10)]\n",
    "    if not all([col in df.columns for col in test_acc_column_names]):\n",
    "        return False\n",
    "    \n",
    "    last_epoch_accuracies = [df[col].iloc[-1] for col in test_acc_column_names]\n",
    "    return np.all(np.array(last_epoch_accuracies) >= 0.95)\n",
    "\n",
    "\n",
    "def condition_finished_succesfully(runs, num_runs=20, split_runs_by_dimension=True):\n",
    "    if isinstance(runs, analysis.ConditionAnalysesSet):\n",
    "        runs = runs.combined\n",
    "        \n",
    "    if isinstance(runs, str):\n",
    "        runs = analysis.load_runs(num_runs, runs, split_runs_by_dimension=split_runs_by_dimension).combined\n",
    "        \n",
    "    failed_runs = []\n",
    "    running = []\n",
    "    for run in ipb(runs, desc='Runs'):\n",
    "        run_id = run.config['dataset_random_seed']\n",
    "        if run.state == 'running':\n",
    "            print(f'Run {run.name} is still running')\n",
    "            running.append(run_id)\n",
    "            continue\n",
    "            \n",
    "        if not run_finished_succesfully(run):\n",
    "            print(f'Run {run.name} failed')\n",
    "            failed_runs.append(run_id)\n",
    "            \n",
    "    if len(running) > 0:\n",
    "        print(f'{len(running)} runs are still running: {running}')\n",
    "            \n",
    "    if len(failed_runs) == 0:\n",
    "        print('All finished runs passed')\n",
    "    else:\n",
    "        print(f'{len(failed_runs)} runs failed: {failed_runs}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-learning-scaling/baseline-curriculum-balanced-batches\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">Runs</span>\n",
       "<progress style=\"width:45ex\" max=\"30\" value=\"30\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">30/30</span>\n",
       "<span class=\"Time-label\">[00:29<00:02, 0.97s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "           Runs [█████████████████████████████████████████████] 30/30 [00:29<00:02, 0.97s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All finished runs passed\n",
      "meta-learning-scaling/curriculum-ratio-1-5-balanced-batches\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">Runs</span>\n",
       "<progress style=\"width:45ex\" max=\"30\" value=\"30\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">30/30</span>\n",
       "<span class=\"Time-label\">[00:42<00:01, 1.39s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "           Runs [█████████████████████████████████████████████] 30/30 [00:42<00:01, 1.39s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All finished runs passed\n",
      "meta-learning-scaling/power-curriculum-default-alpha-balanced-batches\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">Runs</span>\n",
       "<progress style=\"width:45ex\" max=\"30\" value=\"30\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">30/30</span>\n",
       "<span class=\"Time-label\">[00:27<00:01, 0.90s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "           Runs [█████████████████████████████████████████████] 30/30 [00:27<00:01, 0.90s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All finished runs passed\n",
      "meta-learning-scaling/power-curriculum-alpha-2-balanced-batches\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><span class=\"Text-label\" style=\"display:inline-block; overflow:hidden; white-space:nowrap; text-overflow:ellipsis; min-width:15ex; max-width:15ex; vertical-align:middle; text-align:right\">Runs</span>\n",
       "<progress style=\"width:45ex\" max=\"32\" value=\"32\" class=\"Progress-main\"/></progress>\n",
       "<span class=\"Progress-label\"><strong>100%</strong></span>\n",
       "<span class=\"Iteration-label\">32/32</span>\n",
       "<span class=\"Time-label\">[00:58<00:01, 1.81s/it]</span></div>"
      ],
      "text/plain": [
       "\u001b[A\u001b[2K\r",
       "           Runs [█████████████████████████████████████████████] 32/32 [00:58<00:01, 1.81s/it]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All finished runs passed\n"
     ]
    }
   ],
   "source": [
    "curriculum_urls = (\n",
    "    'meta-learning-scaling/baseline-curriculum-balanced-batches', \n",
    "    'meta-learning-scaling/curriculum-ratio-1-5-balanced-batches',\n",
    "    'meta-learning-scaling/power-curriculum-default-alpha-balanced-batches',\n",
    "    'meta-learning-scaling/power-curriculum-alpha-2-balanced-batches',\n",
    ")\n",
    "\n",
    "for url in curriculum_urls:\n",
    "    print(url)\n",
    "    condition_finished_succesfully(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(curriculum_urls[0], str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = runs.combined[0].history(pandas=True, samples=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Test Accuracy, Query #7'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Test Accuracy, Query #17' in test_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing around with reading from the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../projects/')\n",
    "\n",
    "from metalearning import cnnmlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_LEARNING_RATE = 5e-4\n",
    "DEFAULT_WEIGHT_DECAY = 1e-4\n",
    "\n",
    "def create_task_conditional_model(multiplicative=True, additive=True, checkpoint_path=None, name=None):\n",
    "    mod_level = list(range(4))\n",
    "\n",
    "    model = cnnmlp.TaskConditionalCNNMLP(\n",
    "        mod_level=mod_level,\n",
    "        multiplicative_mod=multiplicative,\n",
    "        additive_mod=additive,\n",
    "        query_length=30,\n",
    "        conv_filter_sizes=(16, 32, 48, 64),\n",
    "        conv_output_size=4480,\n",
    "        mlp_layer_sizes=(512, 512, 512, 512),\n",
    "        lr=DEFAULT_LEARNING_RATE,\n",
    "        weight_decay=DEFAULT_WEIGHT_DECAY,\n",
    "        use_lr_scheduler=False,\n",
    "        conv_dropout=False,\n",
    "        mlp_dropout=False,\n",
    "        name=name)\n",
    "\n",
    "    if checkpoint_path is not None:\n",
    "        model.load_state(checkpoint_path)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = api.run('meta-learning-scaling/task-conditional-all-layers-additive-only/runs/49dq79wf')\n",
    "last_checkpoint_file = run.file(f'{run.name.replace(\"[0, 1, 2, 3]-\", \"\")}-query-9.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_checkpoint = last_checkpoint_file.download(replace=True, root='/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_task_conditional_model(multiplicative=False, \n",
    "                                      checkpoint_path='/tmp/' + last_checkpoint_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = range(4)\n",
    "additive_weights = [model.conv.additive_mod_layers[f'additive-{i}'].weight.detach().cpu().numpy() \n",
    "                                for i in layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = run.config['benchmark_dimension']\n",
    "dimension = 0\n",
    "[w[:, dimension * 10:(dimension + 1) * 10].sum() for w in additive_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv.additive_mod_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv.additive_mod_layers['additive-0'].weight.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[model.conv.additive_mod_layers['additive-0'].weight.detach().cpu().numpy()[:, i * 10:(i + 1) * 10].sum() for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ravel(model.conv.additive_mod_layers['additive-0'].weight.detach().cpu().numpy()[:, 20:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(Out[44], bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
